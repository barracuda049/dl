{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c47811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d43a1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = 'TOP4040/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5cbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZLoader(Dataset):\n",
    "    def __init__(self, path, transform=None, cur_iter = 7):\n",
    "        self.path = path\n",
    "        self.files = list(Path(path).glob('**/*.npz'))\n",
    "        self.transform = transform\n",
    "        self.cur_iter = cur_iter\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        numpy_array = np.load(self.files[item])['arr_0']\n",
    "        target = (numpy_array[-1] > 0.5).astype(np.float32)\n",
    "        r,c = numpy_array[0].shape\n",
    "        n_1_iter = numpy_array[self.cur_iter]\n",
    "        n_iter = numpy_array[self.cur_iter-1]\n",
    "        gradient = (n_1_iter - n_iter).reshape(1,r,c)\n",
    "        sample = np.concatenate((n_1_iter.reshape(1,r,c),gradient)), target.reshape(1,40,40)\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "559235ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        image, target = sample\n",
    "        \n",
    "        return torch.from_numpy(image.astype(np.float32)), torch.from_numpy(target.astype(np.float32))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9944884",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = NPZLoader(path_folder,cur_iter = 20, transform = ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac303e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2723d947f10>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x1100 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 20\n",
    "plt.figure(figsize = (14,11))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(f'Target')\n",
    "plt.imshow(check[num][1].view(40,40), cmap= 'binary')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(f'n_iters = {check.cur_iter}')\n",
    "plt.imshow(check[num][0][0].numpy(), cmap= 'binary')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(f'gradient')\n",
    "plt.imshow(check[num][0][1].numpy(), cmap= 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4c50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(2, 16, 3, padding = 1)\n",
    "conv11 = nn.Conv2d(16, 16, 3, padding = 1)\n",
    "conv33 = nn.Conv2d(64, 64, 3, padding = 1)\n",
    "pool = nn.MaxPool2d(2,2)\n",
    "conv2 = nn.Conv2d(16, 32, 3,padding = 1)\n",
    "conv3 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "conv4 = nn.Conv2d(64,128, 3, padding=1)\n",
    "conv5 = nn.ConvTranspose2d(128, 64, 4,stride = 2, padding = 1)\n",
    "conv6 = nn.ConvTranspose2d(64, 32, 4,stride = 2, padding = 1)\n",
    "conv7 = nn.ConvTranspose2d(32, 16, 3, padding = 1)\n",
    "conv8 = nn.ConvTranspose2d(16, 1, 3, padding = 1)\n",
    "upsample = nn.Upsample(size = 20, mode = 'nearest')\n",
    "relu = nn.ReLU()\n",
    "bn1 = nn.BatchNorm2d(16)\n",
    "print(exam.shape)\n",
    "drop = nn.Dropout()\n",
    "x = F.relu(bn1(conv1(exam)))\n",
    "print(f'conv1 - {x.shape}')\n",
    "x = conv11(x)\n",
    "print(f'conv11 - {x.shape}')\n",
    "x = pool(x)\n",
    "print(f'pool - {x.shape}')\n",
    "x = conv2(x)\n",
    "print(f'conv2 - {x.shape}')\n",
    "x = pool(x)\n",
    "print(f'pool - {x.shape}')\n",
    "x = conv3(x)\n",
    "print(f'conv3 - {x.shape}')\n",
    "x = conv33(x)\n",
    "print(f'conv33 - {x.shape}')\n",
    "x = pool(x)\n",
    "print(f'pool - {x.shape}')\n",
    "x = conv4(x)\n",
    "print(f'conv4 - {x.shape}')\n",
    "x = conv5(x)\n",
    "print(f'conv5 - {x.shape}')\n",
    "x = conv6(x)\n",
    "print(f'conv6 - {x.shape}')\n",
    "x = conv7(x)\n",
    "print(f'conv7 - {x.shape}')\n",
    "x = conv8(x)\n",
    "print(f'conv8 - {x.shape}')\n",
    "with torch.no_grad():\n",
    "    plt.imshow(x[2].numpy().reshape(40,40), cmap = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d32d7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [check[i] for i in range(8000)]\n",
    "test_set = [check[i] for i in range(8000, len(check))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66852e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_set, batch_size=100, shuffle = True)\n",
    "test_set = DataLoader(dataset=test_set, batch_size=100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614c7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "exiter = iter(test_set)\n",
    "exam, target = next(exiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "888fc327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2, 40, 40])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44029ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 40, 40])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3cd9011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2723e35a860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target[0][0], cmap = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2019e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d01230",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_check = check[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b505d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_check[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0396ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(2, 16, 3, padding = 1)\n",
    "pool = nn.MaxPool2d(2,2)\n",
    "conv2 = nn.Conv2d(16, 32, 3,padding = 1)\n",
    "conv3 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "conv4 = nn.Conv2d(64,128, 3, padding=1)\n",
    "conv5 = nn.ConvTranspose2d(128, 64, 4,stride = 2, padding = 1)\n",
    "conv6 = nn.ConvTranspose2d(64, 32, 4,stride = 2, padding = 1)\n",
    "conv7 = nn.ConvTranspose2d(32, 16, 3, padding = 1)\n",
    "conv8 = nn.ConvTranspose2d(16, 1, 3, padding = 1)\n",
    "upsample = nn.Upsample(size = 20, mode = 'nearest')\n",
    "print(exam.shape)\n",
    "drop = nn.Dropout()\n",
    "x = conv1(exam)\n",
    "print(f'conv1 - {x.shape}')\n",
    "x = pool(x)\n",
    "print(f'pool - {x.shape}')\n",
    "x = conv2(x)\n",
    "print(f'conv2 - {x.shape}')\n",
    "x = drop(x)\n",
    "print(f'drop - {x.shape}')\n",
    "x = conv3(x)\n",
    "print(f'conv4 - {x.shape}')\n",
    "x = pool(x)\n",
    "print(f'pool - {x.shape}')\n",
    "x = conv4(x)\n",
    "print(f'conv4 - {x.shape}')\n",
    "x = conv5(x)\n",
    "print(f'conv5 - {x.shape}')\n",
    "x = conv6(x)\n",
    "print(f'conv6 - {x.shape}')\n",
    "x = conv7(x)\n",
    "print(f'conv7 - {x.shape}')\n",
    "x = conv8(x)\n",
    "print(f'conv8 - {x.shape}')\n",
    "with torch.no_grad():\n",
    "    plt.imshow(x[2].numpy().reshape(40,40), cmap = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f0d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_check = image_check[0].view(1,1,40,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(up_check.numpy(), (40,40)), cmap = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20efdec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "up = nn.Upsample(size = (80,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1955e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_check_sam = up(up_check)\n",
    "up_check_sam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(up_check_sam.numpy(), (80,80)), cmap = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dad0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Upsample( size = (8,8),mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ee130",
   "metadata": {},
   "outputs": [],
   "source": [
    "m(a).view(8,8), m(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "m(b).view(1,8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e495136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=2, out_channels=1, init_features=16, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343e161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.ADAM(model.parameters(), lr = learning_rate)\n",
    "# n_epochs = 2\n",
    "\n",
    "# for epochs in range(n_epochs):\n",
    "#     for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "# #         print(images.shape, labels.shape)\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "#         # backward\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if (i+1) % 100 ==0:\n",
    "            \n",
    "#             print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9220a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "''' inplace=True means that \n",
    "                it will modify the input \n",
    "                directly, without allocating any\n",
    "                additional output. It can sometimes slightly decrease the memory usage\n",
    "'''\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,kernel_size=3,stride = 1,padding = 1, bias =False), # bias false cause we use batch norm\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels,kernel_size=3,stride = 1,padding = 1, bias =False), # bias false cause we use batch norm\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels = 3, out_channels = 1, features = [64,128,256,512] # features - the channels\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #Down part of UNET\n",
    "\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # up part of UNET\n",
    "\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature *2, feature, kernel_size=2, stride = 2)\n",
    "            )\n",
    "\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "           \n",
    "    def forward(self,x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x) # ConvTranspose2d\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size = skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection,x), dim = 1) # concatenaate and the double conv\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.sig(self.final_conv(x))\n",
    "\n",
    "# def test():\n",
    "#     x = torch.randn((3,2,160,160))\n",
    "#     model = UNET(in_channels=2, out_channels=1)\n",
    "#     preds = model(x)\n",
    "#     print(f'pred - {preds.shape}')\n",
    "#     print(x.shape)\n",
    "#     # assert preds.shape == x.shape\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b6c5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(in_channels=2, out_channels = 1, features = [16,32,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28d78395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 100, step 1/80, loss = 0.8890\n",
      "epoch 1 / 100, step 2/80, loss = 0.8460\n",
      "epoch 1 / 100, step 3/80, loss = 0.8347\n",
      "epoch 1 / 100, step 4/80, loss = 0.8124\n",
      "epoch 1 / 100, step 5/80, loss = 0.7832\n",
      "epoch 1 / 100, step 6/80, loss = 0.7425\n",
      "epoch 1 / 100, step 7/80, loss = 0.7276\n",
      "epoch 1 / 100, step 8/80, loss = 0.7463\n",
      "epoch 1 / 100, step 9/80, loss = 0.7274\n",
      "epoch 1 / 100, step 10/80, loss = 0.6883\n",
      "epoch 1 / 100, step 11/80, loss = 0.6806\n",
      "epoch 1 / 100, step 12/80, loss = 0.6995\n",
      "epoch 1 / 100, step 13/80, loss = 0.6597\n",
      "epoch 1 / 100, step 14/80, loss = 0.6619\n",
      "epoch 1 / 100, step 15/80, loss = 0.6517\n",
      "epoch 1 / 100, step 16/80, loss = 0.6346\n",
      "epoch 1 / 100, step 17/80, loss = 0.6262\n",
      "epoch 1 / 100, step 18/80, loss = 0.6226\n",
      "epoch 1 / 100, step 19/80, loss = 0.6235\n",
      "epoch 1 / 100, step 20/80, loss = 0.5962\n",
      "epoch 1 / 100, step 21/80, loss = 0.6003\n",
      "epoch 1 / 100, step 22/80, loss = 0.6032\n",
      "epoch 1 / 100, step 23/80, loss = 0.5823\n",
      "epoch 1 / 100, step 24/80, loss = 0.5830\n",
      "epoch 1 / 100, step 25/80, loss = 0.5912\n",
      "epoch 1 / 100, step 26/80, loss = 0.5778\n",
      "epoch 1 / 100, step 27/80, loss = 0.5827\n",
      "epoch 1 / 100, step 28/80, loss = 0.5652\n",
      "epoch 1 / 100, step 29/80, loss = 0.5728\n",
      "epoch 1 / 100, step 30/80, loss = 0.5769\n",
      "epoch 1 / 100, step 31/80, loss = 0.5586\n",
      "epoch 1 / 100, step 32/80, loss = 0.5527\n",
      "epoch 1 / 100, step 33/80, loss = 0.5622\n",
      "epoch 1 / 100, step 34/80, loss = 0.5566\n",
      "epoch 1 / 100, step 35/80, loss = 0.5396\n",
      "epoch 1 / 100, step 36/80, loss = 0.5368\n",
      "epoch 1 / 100, step 37/80, loss = 0.5398\n",
      "epoch 1 / 100, step 38/80, loss = 0.5510\n",
      "epoch 1 / 100, step 39/80, loss = 0.5389\n",
      "epoch 1 / 100, step 40/80, loss = 0.5294\n",
      "epoch 1 / 100, step 41/80, loss = 0.5421\n",
      "epoch 1 / 100, step 42/80, loss = 0.5328\n",
      "epoch 1 / 100, step 43/80, loss = 0.5239\n",
      "epoch 1 / 100, step 44/80, loss = 0.5294\n",
      "epoch 1 / 100, step 45/80, loss = 0.5141\n",
      "epoch 1 / 100, step 46/80, loss = 0.5165\n",
      "epoch 1 / 100, step 47/80, loss = 0.5185\n",
      "epoch 1 / 100, step 48/80, loss = 0.5155\n",
      "epoch 1 / 100, step 49/80, loss = 0.5195\n",
      "epoch 1 / 100, step 50/80, loss = 0.5096\n",
      "epoch 1 / 100, step 51/80, loss = 0.5135\n",
      "epoch 1 / 100, step 52/80, loss = 0.4942\n",
      "epoch 1 / 100, step 53/80, loss = 0.5035\n",
      "epoch 1 / 100, step 54/80, loss = 0.4961\n",
      "epoch 1 / 100, step 55/80, loss = 0.5053\n",
      "epoch 1 / 100, step 56/80, loss = 0.4967\n",
      "epoch 1 / 100, step 57/80, loss = 0.4925\n",
      "epoch 1 / 100, step 58/80, loss = 0.5022\n",
      "epoch 1 / 100, step 59/80, loss = 0.4902\n",
      "epoch 1 / 100, step 60/80, loss = 0.4969\n",
      "epoch 1 / 100, step 61/80, loss = 0.4937\n",
      "epoch 1 / 100, step 62/80, loss = 0.4905\n",
      "epoch 1 / 100, step 63/80, loss = 0.4826\n",
      "epoch 1 / 100, step 64/80, loss = 0.4950\n",
      "epoch 1 / 100, step 65/80, loss = 0.4868\n",
      "epoch 1 / 100, step 66/80, loss = 0.4831\n",
      "epoch 1 / 100, step 67/80, loss = 0.4860\n",
      "epoch 1 / 100, step 68/80, loss = 0.4869\n",
      "epoch 1 / 100, step 69/80, loss = 0.4712\n",
      "epoch 1 / 100, step 70/80, loss = 0.4768\n",
      "epoch 1 / 100, step 71/80, loss = 0.4694\n",
      "epoch 1 / 100, step 72/80, loss = 0.4758\n",
      "epoch 1 / 100, step 73/80, loss = 0.4727\n",
      "epoch 1 / 100, step 74/80, loss = 0.4666\n",
      "epoch 1 / 100, step 75/80, loss = 0.4677\n",
      "epoch 1 / 100, step 76/80, loss = 0.4614\n",
      "epoch 1 / 100, step 77/80, loss = 0.4664\n",
      "epoch 1 / 100, step 78/80, loss = 0.4597\n",
      "epoch 1 / 100, step 79/80, loss = 0.4645\n",
      "epoch 1 / 100, step 80/80, loss = 0.4506\n",
      "epoch 2 / 100, step 1/80, loss = 0.4614\n",
      "epoch 2 / 100, step 2/80, loss = 0.4616\n",
      "epoch 2 / 100, step 3/80, loss = 0.4523\n",
      "epoch 2 / 100, step 4/80, loss = 0.4518\n",
      "epoch 2 / 100, step 5/80, loss = 0.4572\n",
      "epoch 2 / 100, step 6/80, loss = 0.4468\n",
      "epoch 2 / 100, step 7/80, loss = 0.4468\n",
      "epoch 2 / 100, step 8/80, loss = 0.4475\n",
      "epoch 2 / 100, step 9/80, loss = 0.4524\n",
      "epoch 2 / 100, step 10/80, loss = 0.4427\n",
      "epoch 2 / 100, step 11/80, loss = 0.4471\n",
      "epoch 2 / 100, step 12/80, loss = 0.4502\n",
      "epoch 2 / 100, step 13/80, loss = 0.4490\n",
      "epoch 2 / 100, step 14/80, loss = 0.4478\n",
      "epoch 2 / 100, step 15/80, loss = 0.4521\n",
      "epoch 2 / 100, step 16/80, loss = 0.4365\n",
      "epoch 2 / 100, step 17/80, loss = 0.4384\n",
      "epoch 2 / 100, step 18/80, loss = 0.4422\n",
      "epoch 2 / 100, step 19/80, loss = 0.4277\n",
      "epoch 2 / 100, step 20/80, loss = 0.4383\n",
      "epoch 2 / 100, step 21/80, loss = 0.4450\n",
      "epoch 2 / 100, step 22/80, loss = 0.4371\n",
      "epoch 2 / 100, step 23/80, loss = 0.4352\n",
      "epoch 2 / 100, step 24/80, loss = 0.4496\n",
      "epoch 2 / 100, step 25/80, loss = 0.4498\n",
      "epoch 2 / 100, step 26/80, loss = 0.4333\n",
      "epoch 2 / 100, step 27/80, loss = 0.4433\n",
      "epoch 2 / 100, step 28/80, loss = 0.4319\n",
      "epoch 2 / 100, step 29/80, loss = 0.4406\n",
      "epoch 2 / 100, step 30/80, loss = 0.4352\n",
      "epoch 2 / 100, step 31/80, loss = 0.4244\n",
      "epoch 2 / 100, step 32/80, loss = 0.4184\n",
      "epoch 2 / 100, step 33/80, loss = 0.4250\n",
      "epoch 2 / 100, step 34/80, loss = 0.4347\n",
      "epoch 2 / 100, step 35/80, loss = 0.4211\n",
      "epoch 2 / 100, step 36/80, loss = 0.4330\n",
      "epoch 2 / 100, step 37/80, loss = 0.4179\n",
      "epoch 2 / 100, step 38/80, loss = 0.4192\n",
      "epoch 2 / 100, step 39/80, loss = 0.4289\n",
      "epoch 2 / 100, step 40/80, loss = 0.4358\n",
      "epoch 2 / 100, step 41/80, loss = 0.4248\n",
      "epoch 2 / 100, step 42/80, loss = 0.4202\n",
      "epoch 2 / 100, step 43/80, loss = 0.4281\n",
      "epoch 2 / 100, step 44/80, loss = 0.4246\n",
      "epoch 2 / 100, step 45/80, loss = 0.4128\n",
      "epoch 2 / 100, step 46/80, loss = 0.4194\n",
      "epoch 2 / 100, step 47/80, loss = 0.4274\n",
      "epoch 2 / 100, step 48/80, loss = 0.4136\n",
      "epoch 2 / 100, step 49/80, loss = 0.4116\n",
      "epoch 2 / 100, step 50/80, loss = 0.4176\n",
      "epoch 2 / 100, step 51/80, loss = 0.4165\n",
      "epoch 2 / 100, step 52/80, loss = 0.4137\n",
      "epoch 2 / 100, step 53/80, loss = 0.4084\n",
      "epoch 2 / 100, step 54/80, loss = 0.4094\n",
      "epoch 2 / 100, step 55/80, loss = 0.4199\n",
      "epoch 2 / 100, step 56/80, loss = 0.4235\n",
      "epoch 2 / 100, step 57/80, loss = 0.4151\n",
      "epoch 2 / 100, step 58/80, loss = 0.4155\n",
      "epoch 2 / 100, step 59/80, loss = 0.4175\n",
      "epoch 2 / 100, step 60/80, loss = 0.4122\n",
      "epoch 2 / 100, step 61/80, loss = 0.4140\n",
      "epoch 2 / 100, step 62/80, loss = 0.4147\n",
      "epoch 2 / 100, step 63/80, loss = 0.4131\n",
      "epoch 2 / 100, step 64/80, loss = 0.4148\n",
      "epoch 2 / 100, step 65/80, loss = 0.4153\n",
      "epoch 2 / 100, step 66/80, loss = 0.4171\n",
      "epoch 2 / 100, step 67/80, loss = 0.4068\n",
      "epoch 2 / 100, step 68/80, loss = 0.4106\n",
      "epoch 2 / 100, step 69/80, loss = 0.4083\n",
      "epoch 2 / 100, step 70/80, loss = 0.4070\n",
      "epoch 2 / 100, step 71/80, loss = 0.4158\n",
      "epoch 2 / 100, step 72/80, loss = 0.4078\n",
      "epoch 2 / 100, step 73/80, loss = 0.4036\n",
      "epoch 2 / 100, step 74/80, loss = 0.4004\n",
      "epoch 2 / 100, step 75/80, loss = 0.4063\n",
      "epoch 2 / 100, step 76/80, loss = 0.4047\n",
      "epoch 2 / 100, step 77/80, loss = 0.4054\n",
      "epoch 2 / 100, step 78/80, loss = 0.4081\n",
      "epoch 2 / 100, step 79/80, loss = 0.4134\n",
      "epoch 2 / 100, step 80/80, loss = 0.4005\n",
      "epoch 3 / 100, step 1/80, loss = 0.4020\n",
      "epoch 3 / 100, step 2/80, loss = 0.4004\n",
      "epoch 3 / 100, step 3/80, loss = 0.4000\n",
      "epoch 3 / 100, step 4/80, loss = 0.4030\n",
      "epoch 3 / 100, step 5/80, loss = 0.4023\n",
      "epoch 3 / 100, step 6/80, loss = 0.4092\n",
      "epoch 3 / 100, step 7/80, loss = 0.4098\n",
      "epoch 3 / 100, step 8/80, loss = 0.4014\n",
      "epoch 3 / 100, step 9/80, loss = 0.4056\n",
      "epoch 3 / 100, step 10/80, loss = 0.4040\n",
      "epoch 3 / 100, step 11/80, loss = 0.3993\n",
      "epoch 3 / 100, step 12/80, loss = 0.3970\n",
      "epoch 3 / 100, step 13/80, loss = 0.3946\n",
      "epoch 3 / 100, step 14/80, loss = 0.3923\n",
      "epoch 3 / 100, step 15/80, loss = 0.3925\n",
      "epoch 3 / 100, step 16/80, loss = 0.4056\n",
      "epoch 3 / 100, step 17/80, loss = 0.3883\n",
      "epoch 3 / 100, step 18/80, loss = 0.3906\n",
      "epoch 3 / 100, step 19/80, loss = 0.3918\n",
      "epoch 3 / 100, step 20/80, loss = 0.3998\n",
      "epoch 3 / 100, step 21/80, loss = 0.3950\n",
      "epoch 3 / 100, step 22/80, loss = 0.3904\n",
      "epoch 3 / 100, step 23/80, loss = 0.4083\n",
      "epoch 3 / 100, step 24/80, loss = 0.4100\n",
      "epoch 3 / 100, step 25/80, loss = 0.4057\n",
      "epoch 3 / 100, step 26/80, loss = 0.3952\n",
      "epoch 3 / 100, step 27/80, loss = 0.3901\n",
      "epoch 3 / 100, step 28/80, loss = 0.3873\n",
      "epoch 3 / 100, step 29/80, loss = 0.3874\n",
      "epoch 3 / 100, step 30/80, loss = 0.3879\n",
      "epoch 3 / 100, step 31/80, loss = 0.3974\n",
      "epoch 3 / 100, step 32/80, loss = 0.3893\n",
      "epoch 3 / 100, step 33/80, loss = 0.3890\n",
      "epoch 3 / 100, step 34/80, loss = 0.3931\n",
      "epoch 3 / 100, step 35/80, loss = 0.3868\n",
      "epoch 3 / 100, step 36/80, loss = 0.3915\n",
      "epoch 3 / 100, step 37/80, loss = 0.3984\n",
      "epoch 3 / 100, step 38/80, loss = 0.3930\n",
      "epoch 3 / 100, step 39/80, loss = 0.3822\n",
      "epoch 3 / 100, step 40/80, loss = 0.3808\n",
      "epoch 3 / 100, step 41/80, loss = 0.3916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 / 100, step 42/80, loss = 0.3811\n",
      "epoch 3 / 100, step 43/80, loss = 0.3923\n",
      "epoch 3 / 100, step 44/80, loss = 0.3867\n",
      "epoch 3 / 100, step 45/80, loss = 0.3965\n",
      "epoch 3 / 100, step 46/80, loss = 0.3912\n",
      "epoch 3 / 100, step 47/80, loss = 0.3955\n",
      "epoch 3 / 100, step 48/80, loss = 0.3728\n",
      "epoch 3 / 100, step 49/80, loss = 0.3952\n",
      "epoch 3 / 100, step 50/80, loss = 0.3824\n",
      "epoch 3 / 100, step 51/80, loss = 0.3900\n",
      "epoch 3 / 100, step 52/80, loss = 0.3816\n",
      "epoch 3 / 100, step 53/80, loss = 0.3849\n",
      "epoch 3 / 100, step 54/80, loss = 0.3865\n",
      "epoch 3 / 100, step 55/80, loss = 0.3884\n",
      "epoch 3 / 100, step 56/80, loss = 0.3692\n",
      "epoch 3 / 100, step 57/80, loss = 0.3902\n",
      "epoch 3 / 100, step 58/80, loss = 0.3797\n",
      "epoch 3 / 100, step 59/80, loss = 0.3774\n",
      "epoch 3 / 100, step 60/80, loss = 0.3735\n",
      "epoch 3 / 100, step 61/80, loss = 0.3799\n",
      "epoch 3 / 100, step 62/80, loss = 0.3850\n",
      "epoch 3 / 100, step 63/80, loss = 0.3788\n",
      "epoch 3 / 100, step 64/80, loss = 0.3833\n",
      "epoch 3 / 100, step 65/80, loss = 0.3753\n",
      "epoch 3 / 100, step 66/80, loss = 0.3648\n",
      "epoch 3 / 100, step 67/80, loss = 0.3848\n",
      "epoch 3 / 100, step 68/80, loss = 0.3844\n",
      "epoch 3 / 100, step 69/80, loss = 0.3770\n",
      "epoch 3 / 100, step 70/80, loss = 0.3779\n",
      "epoch 3 / 100, step 71/80, loss = 0.3717\n",
      "epoch 3 / 100, step 72/80, loss = 0.3824\n",
      "epoch 3 / 100, step 73/80, loss = 0.3719\n",
      "epoch 3 / 100, step 74/80, loss = 0.3705\n",
      "epoch 3 / 100, step 75/80, loss = 0.3734\n",
      "epoch 3 / 100, step 76/80, loss = 0.3762\n",
      "epoch 3 / 100, step 77/80, loss = 0.3656\n",
      "epoch 3 / 100, step 78/80, loss = 0.3823\n",
      "epoch 3 / 100, step 79/80, loss = 0.3683\n",
      "epoch 3 / 100, step 80/80, loss = 0.3759\n",
      "epoch 4 / 100, step 1/80, loss = 0.3689\n",
      "epoch 4 / 100, step 2/80, loss = 0.3710\n",
      "epoch 4 / 100, step 3/80, loss = 0.3767\n",
      "epoch 4 / 100, step 4/80, loss = 0.3837\n",
      "epoch 4 / 100, step 5/80, loss = 0.3682\n",
      "epoch 4 / 100, step 6/80, loss = 0.3734\n",
      "epoch 4 / 100, step 7/80, loss = 0.3657\n",
      "epoch 4 / 100, step 8/80, loss = 0.3782\n",
      "epoch 4 / 100, step 9/80, loss = 0.3762\n",
      "epoch 4 / 100, step 10/80, loss = 0.3749\n",
      "epoch 4 / 100, step 11/80, loss = 0.3732\n",
      "epoch 4 / 100, step 12/80, loss = 0.3696\n",
      "epoch 4 / 100, step 13/80, loss = 0.3605\n",
      "epoch 4 / 100, step 14/80, loss = 0.3623\n",
      "epoch 4 / 100, step 15/80, loss = 0.3662\n",
      "epoch 4 / 100, step 16/80, loss = 0.3626\n",
      "epoch 4 / 100, step 17/80, loss = 0.3686\n",
      "epoch 4 / 100, step 18/80, loss = 0.3606\n",
      "epoch 4 / 100, step 19/80, loss = 0.3755\n",
      "epoch 4 / 100, step 20/80, loss = 0.3636\n",
      "epoch 4 / 100, step 21/80, loss = 0.3561\n",
      "epoch 4 / 100, step 22/80, loss = 0.3683\n",
      "epoch 4 / 100, step 23/80, loss = 0.3604\n",
      "epoch 4 / 100, step 24/80, loss = 0.3706\n",
      "epoch 4 / 100, step 25/80, loss = 0.3590\n",
      "epoch 4 / 100, step 26/80, loss = 0.3586\n",
      "epoch 4 / 100, step 27/80, loss = 0.3624\n",
      "epoch 4 / 100, step 28/80, loss = 0.3656\n",
      "epoch 4 / 100, step 29/80, loss = 0.3576\n",
      "epoch 4 / 100, step 30/80, loss = 0.3543\n",
      "epoch 4 / 100, step 31/80, loss = 0.3721\n",
      "epoch 4 / 100, step 32/80, loss = 0.3600\n",
      "epoch 4 / 100, step 33/80, loss = 0.3524\n",
      "epoch 4 / 100, step 34/80, loss = 0.3625\n",
      "epoch 4 / 100, step 35/80, loss = 0.3665\n",
      "epoch 4 / 100, step 36/80, loss = 0.3586\n",
      "epoch 4 / 100, step 37/80, loss = 0.3517\n",
      "epoch 4 / 100, step 38/80, loss = 0.3637\n",
      "epoch 4 / 100, step 39/80, loss = 0.3663\n",
      "epoch 4 / 100, step 40/80, loss = 0.3593\n",
      "epoch 4 / 100, step 41/80, loss = 0.3591\n",
      "epoch 4 / 100, step 42/80, loss = 0.3570\n",
      "epoch 4 / 100, step 43/80, loss = 0.3528\n",
      "epoch 4 / 100, step 44/80, loss = 0.3503\n",
      "epoch 4 / 100, step 45/80, loss = 0.3553\n",
      "epoch 4 / 100, step 46/80, loss = 0.3638\n",
      "epoch 4 / 100, step 47/80, loss = 0.3742\n",
      "epoch 4 / 100, step 48/80, loss = 0.3529\n",
      "epoch 4 / 100, step 49/80, loss = 0.3521\n",
      "epoch 4 / 100, step 50/80, loss = 0.3591\n",
      "epoch 4 / 100, step 51/80, loss = 0.3576\n",
      "epoch 4 / 100, step 52/80, loss = 0.3552\n",
      "epoch 4 / 100, step 53/80, loss = 0.3543\n",
      "epoch 4 / 100, step 54/80, loss = 0.3457\n",
      "epoch 4 / 100, step 55/80, loss = 0.3778\n",
      "epoch 4 / 100, step 56/80, loss = 0.3663\n",
      "epoch 4 / 100, step 57/80, loss = 0.3547\n",
      "epoch 4 / 100, step 58/80, loss = 0.3503\n",
      "epoch 4 / 100, step 59/80, loss = 0.3614\n",
      "epoch 4 / 100, step 60/80, loss = 0.3619\n",
      "epoch 4 / 100, step 61/80, loss = 0.3492\n",
      "epoch 4 / 100, step 62/80, loss = 0.3593\n",
      "epoch 4 / 100, step 63/80, loss = 0.3609\n",
      "epoch 4 / 100, step 64/80, loss = 0.3513\n",
      "epoch 4 / 100, step 65/80, loss = 0.3553\n",
      "epoch 4 / 100, step 66/80, loss = 0.3571\n",
      "epoch 4 / 100, step 67/80, loss = 0.3590\n",
      "epoch 4 / 100, step 68/80, loss = 0.3503\n",
      "epoch 4 / 100, step 69/80, loss = 0.3573\n",
      "epoch 4 / 100, step 70/80, loss = 0.3572\n",
      "epoch 4 / 100, step 71/80, loss = 0.3484\n",
      "epoch 4 / 100, step 72/80, loss = 0.3506\n",
      "epoch 4 / 100, step 73/80, loss = 0.3485\n",
      "epoch 4 / 100, step 74/80, loss = 0.3494\n",
      "epoch 4 / 100, step 75/80, loss = 0.3537\n",
      "epoch 4 / 100, step 76/80, loss = 0.3513\n",
      "epoch 4 / 100, step 77/80, loss = 0.3493\n",
      "epoch 4 / 100, step 78/80, loss = 0.3491\n",
      "epoch 4 / 100, step 79/80, loss = 0.3595\n",
      "epoch 4 / 100, step 80/80, loss = 0.3516\n",
      "epoch 5 / 100, step 1/80, loss = 0.3536\n",
      "epoch 5 / 100, step 2/80, loss = 0.3406\n",
      "epoch 5 / 100, step 3/80, loss = 0.3329\n",
      "epoch 5 / 100, step 4/80, loss = 0.3569\n",
      "epoch 5 / 100, step 5/80, loss = 0.3442\n",
      "epoch 5 / 100, step 6/80, loss = 0.3508\n",
      "epoch 5 / 100, step 7/80, loss = 0.3497\n",
      "epoch 5 / 100, step 8/80, loss = 0.3445\n",
      "epoch 5 / 100, step 9/80, loss = 0.3420\n",
      "epoch 5 / 100, step 10/80, loss = 0.3490\n",
      "epoch 5 / 100, step 11/80, loss = 0.3531\n",
      "epoch 5 / 100, step 12/80, loss = 0.3461\n",
      "epoch 5 / 100, step 13/80, loss = 0.3411\n",
      "epoch 5 / 100, step 14/80, loss = 0.3556\n",
      "epoch 5 / 100, step 15/80, loss = 0.3427\n",
      "epoch 5 / 100, step 16/80, loss = 0.3503\n",
      "epoch 5 / 100, step 17/80, loss = 0.3485\n",
      "epoch 5 / 100, step 18/80, loss = 0.3449\n",
      "epoch 5 / 100, step 19/80, loss = 0.3458\n",
      "epoch 5 / 100, step 20/80, loss = 0.3373\n",
      "epoch 5 / 100, step 21/80, loss = 0.3422\n",
      "epoch 5 / 100, step 22/80, loss = 0.3460\n",
      "epoch 5 / 100, step 23/80, loss = 0.3363\n",
      "epoch 5 / 100, step 24/80, loss = 0.3438\n",
      "epoch 5 / 100, step 25/80, loss = 0.3456\n",
      "epoch 5 / 100, step 26/80, loss = 0.3407\n",
      "epoch 5 / 100, step 27/80, loss = 0.3377\n",
      "epoch 5 / 100, step 28/80, loss = 0.3422\n",
      "epoch 5 / 100, step 29/80, loss = 0.3426\n",
      "epoch 5 / 100, step 30/80, loss = 0.3344\n",
      "epoch 5 / 100, step 31/80, loss = 0.3432\n",
      "epoch 5 / 100, step 32/80, loss = 0.3476\n",
      "epoch 5 / 100, step 33/80, loss = 0.3346\n",
      "epoch 5 / 100, step 34/80, loss = 0.3456\n",
      "epoch 5 / 100, step 35/80, loss = 0.3384\n",
      "epoch 5 / 100, step 36/80, loss = 0.3336\n",
      "epoch 5 / 100, step 37/80, loss = 0.3273\n",
      "epoch 5 / 100, step 38/80, loss = 0.3420\n",
      "epoch 5 / 100, step 39/80, loss = 0.3378\n",
      "epoch 5 / 100, step 40/80, loss = 0.3242\n",
      "epoch 5 / 100, step 41/80, loss = 0.3314\n",
      "epoch 5 / 100, step 42/80, loss = 0.3336\n",
      "epoch 5 / 100, step 43/80, loss = 0.3397\n",
      "epoch 5 / 100, step 44/80, loss = 0.3340\n",
      "epoch 5 / 100, step 45/80, loss = 0.3450\n",
      "epoch 5 / 100, step 46/80, loss = 0.3417\n",
      "epoch 5 / 100, step 47/80, loss = 0.3359\n",
      "epoch 5 / 100, step 48/80, loss = 0.3362\n",
      "epoch 5 / 100, step 49/80, loss = 0.3307\n",
      "epoch 5 / 100, step 50/80, loss = 0.3370\n",
      "epoch 5 / 100, step 51/80, loss = 0.3279\n",
      "epoch 5 / 100, step 52/80, loss = 0.3375\n",
      "epoch 5 / 100, step 53/80, loss = 0.3256\n",
      "epoch 5 / 100, step 54/80, loss = 0.3343\n",
      "epoch 5 / 100, step 55/80, loss = 0.3327\n",
      "epoch 5 / 100, step 56/80, loss = 0.3289\n",
      "epoch 5 / 100, step 57/80, loss = 0.3365\n",
      "epoch 5 / 100, step 58/80, loss = 0.3312\n",
      "epoch 5 / 100, step 59/80, loss = 0.3298\n",
      "epoch 5 / 100, step 60/80, loss = 0.3227\n",
      "epoch 5 / 100, step 61/80, loss = 0.3246\n",
      "epoch 5 / 100, step 62/80, loss = 0.3330\n",
      "epoch 5 / 100, step 63/80, loss = 0.3358\n",
      "epoch 5 / 100, step 64/80, loss = 0.3411\n",
      "epoch 5 / 100, step 65/80, loss = 0.3370\n",
      "epoch 5 / 100, step 66/80, loss = 0.3269\n",
      "epoch 5 / 100, step 67/80, loss = 0.3310\n",
      "epoch 5 / 100, step 68/80, loss = 0.3328\n",
      "epoch 5 / 100, step 69/80, loss = 0.3264\n",
      "epoch 5 / 100, step 70/80, loss = 0.3246\n",
      "epoch 5 / 100, step 71/80, loss = 0.3250\n",
      "epoch 5 / 100, step 72/80, loss = 0.3332\n",
      "epoch 5 / 100, step 73/80, loss = 0.3203\n",
      "epoch 5 / 100, step 74/80, loss = 0.3225\n",
      "epoch 5 / 100, step 75/80, loss = 0.3249\n",
      "epoch 5 / 100, step 76/80, loss = 0.3346\n",
      "epoch 5 / 100, step 77/80, loss = 0.3210\n",
      "epoch 5 / 100, step 78/80, loss = 0.3210\n",
      "epoch 5 / 100, step 79/80, loss = 0.3301\n",
      "epoch 5 / 100, step 80/80, loss = 0.3171\n",
      "epoch 6 / 100, step 1/80, loss = 0.3220\n",
      "epoch 6 / 100, step 2/80, loss = 0.3303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 / 100, step 3/80, loss = 0.3216\n",
      "epoch 6 / 100, step 4/80, loss = 0.3216\n",
      "epoch 6 / 100, step 5/80, loss = 0.3243\n",
      "epoch 6 / 100, step 6/80, loss = 0.3235\n",
      "epoch 6 / 100, step 7/80, loss = 0.3336\n",
      "epoch 6 / 100, step 8/80, loss = 0.3253\n",
      "epoch 6 / 100, step 9/80, loss = 0.3209\n",
      "epoch 6 / 100, step 10/80, loss = 0.3138\n",
      "epoch 6 / 100, step 11/80, loss = 0.3212\n",
      "epoch 6 / 100, step 12/80, loss = 0.3189\n",
      "epoch 6 / 100, step 13/80, loss = 0.3232\n",
      "epoch 6 / 100, step 14/80, loss = 0.3147\n",
      "epoch 6 / 100, step 15/80, loss = 0.3313\n",
      "epoch 6 / 100, step 16/80, loss = 0.3301\n",
      "epoch 6 / 100, step 17/80, loss = 0.3280\n",
      "epoch 6 / 100, step 18/80, loss = 0.3243\n",
      "epoch 6 / 100, step 19/80, loss = 0.3235\n",
      "epoch 6 / 100, step 20/80, loss = 0.3187\n",
      "epoch 6 / 100, step 21/80, loss = 0.3245\n",
      "epoch 6 / 100, step 22/80, loss = 0.3171\n",
      "epoch 6 / 100, step 23/80, loss = 0.3183\n",
      "epoch 6 / 100, step 24/80, loss = 0.3218\n",
      "epoch 6 / 100, step 25/80, loss = 0.3243\n",
      "epoch 6 / 100, step 26/80, loss = 0.3284\n",
      "epoch 6 / 100, step 27/80, loss = 0.3123\n",
      "epoch 6 / 100, step 28/80, loss = 0.3132\n",
      "epoch 6 / 100, step 29/80, loss = 0.3212\n",
      "epoch 6 / 100, step 30/80, loss = 0.3133\n",
      "epoch 6 / 100, step 31/80, loss = 0.3137\n",
      "epoch 6 / 100, step 32/80, loss = 0.3214\n",
      "epoch 6 / 100, step 33/80, loss = 0.3103\n",
      "epoch 6 / 100, step 34/80, loss = 0.3247\n",
      "epoch 6 / 100, step 35/80, loss = 0.3323\n",
      "epoch 6 / 100, step 36/80, loss = 0.3183\n",
      "epoch 6 / 100, step 37/80, loss = 0.3107\n",
      "epoch 6 / 100, step 38/80, loss = 0.3233\n",
      "epoch 6 / 100, step 39/80, loss = 0.3159\n",
      "epoch 6 / 100, step 40/80, loss = 0.3153\n",
      "epoch 6 / 100, step 41/80, loss = 0.3085\n",
      "epoch 6 / 100, step 42/80, loss = 0.3137\n",
      "epoch 6 / 100, step 43/80, loss = 0.3129\n",
      "epoch 6 / 100, step 44/80, loss = 0.3235\n",
      "epoch 6 / 100, step 45/80, loss = 0.3248\n",
      "epoch 6 / 100, step 46/80, loss = 0.3201\n",
      "epoch 6 / 100, step 47/80, loss = 0.3141\n",
      "epoch 6 / 100, step 48/80, loss = 0.3196\n",
      "epoch 6 / 100, step 49/80, loss = 0.3185\n",
      "epoch 6 / 100, step 50/80, loss = 0.3139\n",
      "epoch 6 / 100, step 51/80, loss = 0.3129\n",
      "epoch 6 / 100, step 52/80, loss = 0.3091\n",
      "epoch 6 / 100, step 53/80, loss = 0.3149\n",
      "epoch 6 / 100, step 54/80, loss = 0.3050\n",
      "epoch 6 / 100, step 55/80, loss = 0.3121\n",
      "epoch 6 / 100, step 56/80, loss = 0.3053\n",
      "epoch 6 / 100, step 57/80, loss = 0.3059\n",
      "epoch 6 / 100, step 58/80, loss = 0.3161\n",
      "epoch 6 / 100, step 59/80, loss = 0.3087\n",
      "epoch 6 / 100, step 60/80, loss = 0.3155\n",
      "epoch 6 / 100, step 61/80, loss = 0.3063\n",
      "epoch 6 / 100, step 62/80, loss = 0.3047\n",
      "epoch 6 / 100, step 63/80, loss = 0.3170\n",
      "epoch 6 / 100, step 64/80, loss = 0.3036\n",
      "epoch 6 / 100, step 65/80, loss = 0.3007\n",
      "epoch 6 / 100, step 66/80, loss = 0.3101\n",
      "epoch 6 / 100, step 67/80, loss = 0.3135\n",
      "epoch 6 / 100, step 68/80, loss = 0.3105\n",
      "epoch 6 / 100, step 69/80, loss = 0.2995\n",
      "epoch 6 / 100, step 70/80, loss = 0.3066\n",
      "epoch 6 / 100, step 71/80, loss = 0.2986\n",
      "epoch 6 / 100, step 72/80, loss = 0.3049\n",
      "epoch 6 / 100, step 73/80, loss = 0.3091\n",
      "epoch 6 / 100, step 74/80, loss = 0.3117\n",
      "epoch 6 / 100, step 75/80, loss = 0.3067\n",
      "epoch 6 / 100, step 76/80, loss = 0.3045\n",
      "epoch 6 / 100, step 77/80, loss = 0.3013\n",
      "epoch 6 / 100, step 78/80, loss = 0.3058\n",
      "epoch 6 / 100, step 79/80, loss = 0.3117\n",
      "epoch 6 / 100, step 80/80, loss = 0.2986\n",
      "epoch 7 / 100, step 1/80, loss = 0.3062\n",
      "epoch 7 / 100, step 2/80, loss = 0.3130\n",
      "epoch 7 / 100, step 3/80, loss = 0.3098\n",
      "epoch 7 / 100, step 4/80, loss = 0.2970\n",
      "epoch 7 / 100, step 5/80, loss = 0.3040\n",
      "epoch 7 / 100, step 6/80, loss = 0.3059\n",
      "epoch 7 / 100, step 7/80, loss = 0.3017\n",
      "epoch 7 / 100, step 8/80, loss = 0.2957\n",
      "epoch 7 / 100, step 9/80, loss = 0.3005\n",
      "epoch 7 / 100, step 10/80, loss = 0.3013\n",
      "epoch 7 / 100, step 11/80, loss = 0.3010\n",
      "epoch 7 / 100, step 12/80, loss = 0.3016\n",
      "epoch 7 / 100, step 13/80, loss = 0.3069\n",
      "epoch 7 / 100, step 14/80, loss = 0.3041\n",
      "epoch 7 / 100, step 15/80, loss = 0.3118\n",
      "epoch 7 / 100, step 16/80, loss = 0.2907\n",
      "epoch 7 / 100, step 17/80, loss = 0.2971\n",
      "epoch 7 / 100, step 18/80, loss = 0.3006\n",
      "epoch 7 / 100, step 19/80, loss = 0.3038\n",
      "epoch 7 / 100, step 20/80, loss = 0.3097\n",
      "epoch 7 / 100, step 21/80, loss = 0.3006\n",
      "epoch 7 / 100, step 22/80, loss = 0.3034\n",
      "epoch 7 / 100, step 23/80, loss = 0.2938\n",
      "epoch 7 / 100, step 24/80, loss = 0.3003\n",
      "epoch 7 / 100, step 25/80, loss = 0.2964\n",
      "epoch 7 / 100, step 26/80, loss = 0.3027\n",
      "epoch 7 / 100, step 27/80, loss = 0.2990\n",
      "epoch 7 / 100, step 28/80, loss = 0.2977\n",
      "epoch 7 / 100, step 29/80, loss = 0.3076\n",
      "epoch 7 / 100, step 30/80, loss = 0.3040\n",
      "epoch 7 / 100, step 31/80, loss = 0.3109\n",
      "epoch 7 / 100, step 32/80, loss = 0.2883\n",
      "epoch 7 / 100, step 33/80, loss = 0.3028\n",
      "epoch 7 / 100, step 34/80, loss = 0.2926\n",
      "epoch 7 / 100, step 35/80, loss = 0.3021\n",
      "epoch 7 / 100, step 36/80, loss = 0.2991\n",
      "epoch 7 / 100, step 37/80, loss = 0.2881\n",
      "epoch 7 / 100, step 38/80, loss = 0.2920\n",
      "epoch 7 / 100, step 39/80, loss = 0.2996\n",
      "epoch 7 / 100, step 40/80, loss = 0.2946\n",
      "epoch 7 / 100, step 41/80, loss = 0.2952\n",
      "epoch 7 / 100, step 42/80, loss = 0.3034\n",
      "epoch 7 / 100, step 43/80, loss = 0.2910\n",
      "epoch 7 / 100, step 44/80, loss = 0.2897\n",
      "epoch 7 / 100, step 45/80, loss = 0.2979\n",
      "epoch 7 / 100, step 46/80, loss = 0.2978\n",
      "epoch 7 / 100, step 47/80, loss = 0.2894\n",
      "epoch 7 / 100, step 48/80, loss = 0.2956\n",
      "epoch 7 / 100, step 49/80, loss = 0.2841\n",
      "epoch 7 / 100, step 50/80, loss = 0.3011\n",
      "epoch 7 / 100, step 51/80, loss = 0.2911\n",
      "epoch 7 / 100, step 52/80, loss = 0.2930\n",
      "epoch 7 / 100, step 53/80, loss = 0.2940\n",
      "epoch 7 / 100, step 54/80, loss = 0.2985\n",
      "epoch 7 / 100, step 55/80, loss = 0.2840\n",
      "epoch 7 / 100, step 56/80, loss = 0.2861\n",
      "epoch 7 / 100, step 57/80, loss = 0.2996\n",
      "epoch 7 / 100, step 58/80, loss = 0.2975\n",
      "epoch 7 / 100, step 59/80, loss = 0.2848\n",
      "epoch 7 / 100, step 60/80, loss = 0.2990\n",
      "epoch 7 / 100, step 61/80, loss = 0.2925\n",
      "epoch 7 / 100, step 62/80, loss = 0.2987\n",
      "epoch 7 / 100, step 63/80, loss = 0.2873\n",
      "epoch 7 / 100, step 64/80, loss = 0.2900\n",
      "epoch 7 / 100, step 65/80, loss = 0.2868\n",
      "epoch 7 / 100, step 66/80, loss = 0.2893\n",
      "epoch 7 / 100, step 67/80, loss = 0.2922\n",
      "epoch 7 / 100, step 68/80, loss = 0.2903\n",
      "epoch 7 / 100, step 69/80, loss = 0.2917\n",
      "epoch 7 / 100, step 70/80, loss = 0.3073\n",
      "epoch 7 / 100, step 71/80, loss = 0.2888\n",
      "epoch 7 / 100, step 72/80, loss = 0.2934\n",
      "epoch 7 / 100, step 73/80, loss = 0.2747\n",
      "epoch 7 / 100, step 74/80, loss = 0.2957\n",
      "epoch 7 / 100, step 75/80, loss = 0.2815\n",
      "epoch 7 / 100, step 76/80, loss = 0.2862\n",
      "epoch 7 / 100, step 77/80, loss = 0.2862\n",
      "epoch 7 / 100, step 78/80, loss = 0.2818\n",
      "epoch 7 / 100, step 79/80, loss = 0.2792\n",
      "epoch 7 / 100, step 80/80, loss = 0.2884\n",
      "epoch 8 / 100, step 1/80, loss = 0.2972\n",
      "epoch 8 / 100, step 2/80, loss = 0.2873\n",
      "epoch 8 / 100, step 3/80, loss = 0.2842\n",
      "epoch 8 / 100, step 4/80, loss = 0.2769\n",
      "epoch 8 / 100, step 5/80, loss = 0.2817\n",
      "epoch 8 / 100, step 6/80, loss = 0.2768\n",
      "epoch 8 / 100, step 7/80, loss = 0.2808\n",
      "epoch 8 / 100, step 8/80, loss = 0.2927\n",
      "epoch 8 / 100, step 9/80, loss = 0.2891\n",
      "epoch 8 / 100, step 10/80, loss = 0.2858\n",
      "epoch 8 / 100, step 11/80, loss = 0.2860\n",
      "epoch 8 / 100, step 12/80, loss = 0.2801\n",
      "epoch 8 / 100, step 13/80, loss = 0.2843\n",
      "epoch 8 / 100, step 14/80, loss = 0.2866\n",
      "epoch 8 / 100, step 15/80, loss = 0.2854\n",
      "epoch 8 / 100, step 16/80, loss = 0.2907\n",
      "epoch 8 / 100, step 17/80, loss = 0.2893\n",
      "epoch 8 / 100, step 18/80, loss = 0.2895\n",
      "epoch 8 / 100, step 19/80, loss = 0.2938\n",
      "epoch 8 / 100, step 20/80, loss = 0.2771\n",
      "epoch 8 / 100, step 21/80, loss = 0.2831\n",
      "epoch 8 / 100, step 22/80, loss = 0.2850\n",
      "epoch 8 / 100, step 23/80, loss = 0.2855\n",
      "epoch 8 / 100, step 24/80, loss = 0.2758\n",
      "epoch 8 / 100, step 25/80, loss = 0.2807\n",
      "epoch 8 / 100, step 26/80, loss = 0.2769\n",
      "epoch 8 / 100, step 27/80, loss = 0.2761\n",
      "epoch 8 / 100, step 28/80, loss = 0.2906\n",
      "epoch 8 / 100, step 29/80, loss = 0.2770\n",
      "epoch 8 / 100, step 30/80, loss = 0.2726\n",
      "epoch 8 / 100, step 31/80, loss = 0.2705\n",
      "epoch 8 / 100, step 32/80, loss = 0.2854\n",
      "epoch 8 / 100, step 33/80, loss = 0.2748\n",
      "epoch 8 / 100, step 34/80, loss = 0.2911\n",
      "epoch 8 / 100, step 35/80, loss = 0.2783\n",
      "epoch 8 / 100, step 36/80, loss = 0.2793\n",
      "epoch 8 / 100, step 37/80, loss = 0.2765\n",
      "epoch 8 / 100, step 38/80, loss = 0.2697\n",
      "epoch 8 / 100, step 39/80, loss = 0.2717\n",
      "epoch 8 / 100, step 40/80, loss = 0.2837\n",
      "epoch 8 / 100, step 41/80, loss = 0.2840\n",
      "epoch 8 / 100, step 42/80, loss = 0.2761\n",
      "epoch 8 / 100, step 43/80, loss = 0.2621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 / 100, step 44/80, loss = 0.2812\n",
      "epoch 8 / 100, step 45/80, loss = 0.2757\n",
      "epoch 8 / 100, step 46/80, loss = 0.2880\n",
      "epoch 8 / 100, step 47/80, loss = 0.2775\n",
      "epoch 8 / 100, step 48/80, loss = 0.2741\n",
      "epoch 8 / 100, step 49/80, loss = 0.2787\n",
      "epoch 8 / 100, step 50/80, loss = 0.2675\n",
      "epoch 8 / 100, step 51/80, loss = 0.2796\n",
      "epoch 8 / 100, step 52/80, loss = 0.2807\n",
      "epoch 8 / 100, step 53/80, loss = 0.2664\n",
      "epoch 8 / 100, step 54/80, loss = 0.2828\n",
      "epoch 8 / 100, step 55/80, loss = 0.2735\n",
      "epoch 8 / 100, step 56/80, loss = 0.2640\n",
      "epoch 8 / 100, step 57/80, loss = 0.2751\n",
      "epoch 8 / 100, step 58/80, loss = 0.2848\n",
      "epoch 8 / 100, step 59/80, loss = 0.2801\n",
      "epoch 8 / 100, step 60/80, loss = 0.2719\n",
      "epoch 8 / 100, step 61/80, loss = 0.2734\n",
      "epoch 8 / 100, step 62/80, loss = 0.2667\n",
      "epoch 8 / 100, step 63/80, loss = 0.2623\n",
      "epoch 8 / 100, step 64/80, loss = 0.2834\n",
      "epoch 8 / 100, step 65/80, loss = 0.2843\n",
      "epoch 8 / 100, step 66/80, loss = 0.2836\n",
      "epoch 8 / 100, step 67/80, loss = 0.2723\n",
      "epoch 8 / 100, step 68/80, loss = 0.2689\n",
      "epoch 8 / 100, step 69/80, loss = 0.2695\n",
      "epoch 8 / 100, step 70/80, loss = 0.2708\n",
      "epoch 8 / 100, step 71/80, loss = 0.2751\n",
      "epoch 8 / 100, step 72/80, loss = 0.2639\n",
      "epoch 8 / 100, step 73/80, loss = 0.2750\n",
      "epoch 8 / 100, step 74/80, loss = 0.2648\n",
      "epoch 8 / 100, step 75/80, loss = 0.2619\n",
      "epoch 8 / 100, step 76/80, loss = 0.2704\n",
      "epoch 8 / 100, step 77/80, loss = 0.2694\n",
      "epoch 8 / 100, step 78/80, loss = 0.2688\n",
      "epoch 8 / 100, step 79/80, loss = 0.2686\n",
      "epoch 8 / 100, step 80/80, loss = 0.2781\n",
      "epoch 9 / 100, step 1/80, loss = 0.2711\n",
      "epoch 9 / 100, step 2/80, loss = 0.2701\n",
      "epoch 9 / 100, step 3/80, loss = 0.2728\n",
      "epoch 9 / 100, step 4/80, loss = 0.2727\n",
      "epoch 9 / 100, step 5/80, loss = 0.2708\n",
      "epoch 9 / 100, step 6/80, loss = 0.2809\n",
      "epoch 9 / 100, step 7/80, loss = 0.2593\n",
      "epoch 9 / 100, step 8/80, loss = 0.2678\n",
      "epoch 9 / 100, step 9/80, loss = 0.2713\n",
      "epoch 9 / 100, step 10/80, loss = 0.2690\n",
      "epoch 9 / 100, step 11/80, loss = 0.2666\n",
      "epoch 9 / 100, step 12/80, loss = 0.2611\n",
      "epoch 9 / 100, step 13/80, loss = 0.2620\n",
      "epoch 9 / 100, step 14/80, loss = 0.2621\n",
      "epoch 9 / 100, step 15/80, loss = 0.2629\n",
      "epoch 9 / 100, step 16/80, loss = 0.2669\n",
      "epoch 9 / 100, step 17/80, loss = 0.2668\n",
      "epoch 9 / 100, step 18/80, loss = 0.2628\n",
      "epoch 9 / 100, step 19/80, loss = 0.2619\n",
      "epoch 9 / 100, step 20/80, loss = 0.2627\n",
      "epoch 9 / 100, step 21/80, loss = 0.2674\n",
      "epoch 9 / 100, step 22/80, loss = 0.2661\n",
      "epoch 9 / 100, step 23/80, loss = 0.2650\n",
      "epoch 9 / 100, step 24/80, loss = 0.2554\n",
      "epoch 9 / 100, step 25/80, loss = 0.2535\n",
      "epoch 9 / 100, step 26/80, loss = 0.2739\n",
      "epoch 9 / 100, step 27/80, loss = 0.2643\n",
      "epoch 9 / 100, step 28/80, loss = 0.2624\n",
      "epoch 9 / 100, step 29/80, loss = 0.2645\n",
      "epoch 9 / 100, step 30/80, loss = 0.2583\n",
      "epoch 9 / 100, step 31/80, loss = 0.2679\n",
      "epoch 9 / 100, step 32/80, loss = 0.2691\n",
      "epoch 9 / 100, step 33/80, loss = 0.2465\n",
      "epoch 9 / 100, step 34/80, loss = 0.2589\n",
      "epoch 9 / 100, step 35/80, loss = 0.2597\n",
      "epoch 9 / 100, step 36/80, loss = 0.2561\n",
      "epoch 9 / 100, step 37/80, loss = 0.2603\n",
      "epoch 9 / 100, step 38/80, loss = 0.2814\n",
      "epoch 9 / 100, step 39/80, loss = 0.2587\n",
      "epoch 9 / 100, step 40/80, loss = 0.2693\n",
      "epoch 9 / 100, step 41/80, loss = 0.2723\n",
      "epoch 9 / 100, step 42/80, loss = 0.2520\n",
      "epoch 9 / 100, step 43/80, loss = 0.2509\n",
      "epoch 9 / 100, step 44/80, loss = 0.2585\n",
      "epoch 9 / 100, step 45/80, loss = 0.2528\n",
      "epoch 9 / 100, step 46/80, loss = 0.2709\n",
      "epoch 9 / 100, step 47/80, loss = 0.2556\n",
      "epoch 9 / 100, step 48/80, loss = 0.2639\n",
      "epoch 9 / 100, step 49/80, loss = 0.2529\n",
      "epoch 9 / 100, step 50/80, loss = 0.2610\n",
      "epoch 9 / 100, step 51/80, loss = 0.2576\n",
      "epoch 9 / 100, step 52/80, loss = 0.2573\n",
      "epoch 9 / 100, step 53/80, loss = 0.2671\n",
      "epoch 9 / 100, step 54/80, loss = 0.2695\n",
      "epoch 9 / 100, step 55/80, loss = 0.2605\n",
      "epoch 9 / 100, step 56/80, loss = 0.2511\n",
      "epoch 9 / 100, step 57/80, loss = 0.2578\n",
      "epoch 9 / 100, step 58/80, loss = 0.2495\n",
      "epoch 9 / 100, step 59/80, loss = 0.2561\n",
      "epoch 9 / 100, step 60/80, loss = 0.2621\n",
      "epoch 9 / 100, step 61/80, loss = 0.2565\n",
      "epoch 9 / 100, step 62/80, loss = 0.2562\n",
      "epoch 9 / 100, step 63/80, loss = 0.2551\n",
      "epoch 9 / 100, step 64/80, loss = 0.2499\n",
      "epoch 9 / 100, step 65/80, loss = 0.2586\n",
      "epoch 9 / 100, step 66/80, loss = 0.2599\n",
      "epoch 9 / 100, step 67/80, loss = 0.2529\n",
      "epoch 9 / 100, step 68/80, loss = 0.2555\n",
      "epoch 9 / 100, step 69/80, loss = 0.2541\n",
      "epoch 9 / 100, step 70/80, loss = 0.2590\n",
      "epoch 9 / 100, step 71/80, loss = 0.2576\n",
      "epoch 9 / 100, step 72/80, loss = 0.2583\n",
      "epoch 9 / 100, step 73/80, loss = 0.2674\n",
      "epoch 9 / 100, step 74/80, loss = 0.2597\n",
      "epoch 9 / 100, step 75/80, loss = 0.2512\n",
      "epoch 9 / 100, step 76/80, loss = 0.2518\n",
      "epoch 9 / 100, step 77/80, loss = 0.2561\n",
      "epoch 9 / 100, step 78/80, loss = 0.2573\n",
      "epoch 9 / 100, step 79/80, loss = 0.2543\n",
      "epoch 9 / 100, step 80/80, loss = 0.2596\n",
      "epoch 10 / 100, step 1/80, loss = 0.2489\n",
      "epoch 10 / 100, step 2/80, loss = 0.2529\n",
      "epoch 10 / 100, step 3/80, loss = 0.2404\n",
      "epoch 10 / 100, step 4/80, loss = 0.2391\n",
      "epoch 10 / 100, step 5/80, loss = 0.2502\n",
      "epoch 10 / 100, step 6/80, loss = 0.2514\n",
      "epoch 10 / 100, step 7/80, loss = 0.2511\n",
      "epoch 10 / 100, step 8/80, loss = 0.2530\n",
      "epoch 10 / 100, step 9/80, loss = 0.2558\n",
      "epoch 10 / 100, step 10/80, loss = 0.2421\n",
      "epoch 10 / 100, step 11/80, loss = 0.2517\n",
      "epoch 10 / 100, step 12/80, loss = 0.2537\n",
      "epoch 10 / 100, step 13/80, loss = 0.2459\n",
      "epoch 10 / 100, step 14/80, loss = 0.2370\n",
      "epoch 10 / 100, step 15/80, loss = 0.2536\n",
      "epoch 10 / 100, step 16/80, loss = 0.2594\n",
      "epoch 10 / 100, step 17/80, loss = 0.2451\n",
      "epoch 10 / 100, step 18/80, loss = 0.2457\n",
      "epoch 10 / 100, step 19/80, loss = 0.2493\n",
      "epoch 10 / 100, step 20/80, loss = 0.2534\n",
      "epoch 10 / 100, step 21/80, loss = 0.2505\n",
      "epoch 10 / 100, step 22/80, loss = 0.2543\n",
      "epoch 10 / 100, step 23/80, loss = 0.2419\n",
      "epoch 10 / 100, step 24/80, loss = 0.2406\n",
      "epoch 10 / 100, step 25/80, loss = 0.2433\n",
      "epoch 10 / 100, step 26/80, loss = 0.2445\n",
      "epoch 10 / 100, step 27/80, loss = 0.2527\n",
      "epoch 10 / 100, step 28/80, loss = 0.2555\n",
      "epoch 10 / 100, step 29/80, loss = 0.2514\n",
      "epoch 10 / 100, step 30/80, loss = 0.2394\n",
      "epoch 10 / 100, step 31/80, loss = 0.2532\n",
      "epoch 10 / 100, step 32/80, loss = 0.2478\n",
      "epoch 10 / 100, step 33/80, loss = 0.2488\n",
      "epoch 10 / 100, step 34/80, loss = 0.2486\n",
      "epoch 10 / 100, step 35/80, loss = 0.2529\n",
      "epoch 10 / 100, step 36/80, loss = 0.2473\n",
      "epoch 10 / 100, step 37/80, loss = 0.2456\n",
      "epoch 10 / 100, step 38/80, loss = 0.2454\n",
      "epoch 10 / 100, step 39/80, loss = 0.2479\n",
      "epoch 10 / 100, step 40/80, loss = 0.2447\n",
      "epoch 10 / 100, step 41/80, loss = 0.2467\n",
      "epoch 10 / 100, step 42/80, loss = 0.2477\n",
      "epoch 10 / 100, step 43/80, loss = 0.2457\n",
      "epoch 10 / 100, step 44/80, loss = 0.2494\n",
      "epoch 10 / 100, step 45/80, loss = 0.2463\n",
      "epoch 10 / 100, step 46/80, loss = 0.2400\n",
      "epoch 10 / 100, step 47/80, loss = 0.2399\n",
      "epoch 10 / 100, step 48/80, loss = 0.2399\n",
      "epoch 10 / 100, step 49/80, loss = 0.2427\n",
      "epoch 10 / 100, step 50/80, loss = 0.2472\n",
      "epoch 10 / 100, step 51/80, loss = 0.2514\n",
      "epoch 10 / 100, step 52/80, loss = 0.2414\n",
      "epoch 10 / 100, step 53/80, loss = 0.2444\n",
      "epoch 10 / 100, step 54/80, loss = 0.2378\n",
      "epoch 10 / 100, step 55/80, loss = 0.2431\n",
      "epoch 10 / 100, step 56/80, loss = 0.2418\n",
      "epoch 10 / 100, step 57/80, loss = 0.2516\n",
      "epoch 10 / 100, step 58/80, loss = 0.2402\n",
      "epoch 10 / 100, step 59/80, loss = 0.2411\n",
      "epoch 10 / 100, step 60/80, loss = 0.2397\n",
      "epoch 10 / 100, step 61/80, loss = 0.2489\n",
      "epoch 10 / 100, step 62/80, loss = 0.2452\n",
      "epoch 10 / 100, step 63/80, loss = 0.2385\n",
      "epoch 10 / 100, step 64/80, loss = 0.2409\n",
      "epoch 10 / 100, step 65/80, loss = 0.2488\n",
      "epoch 10 / 100, step 66/80, loss = 0.2572\n",
      "epoch 10 / 100, step 67/80, loss = 0.2405\n",
      "epoch 10 / 100, step 68/80, loss = 0.2445\n",
      "epoch 10 / 100, step 69/80, loss = 0.2366\n",
      "epoch 10 / 100, step 70/80, loss = 0.2406\n",
      "epoch 10 / 100, step 71/80, loss = 0.2422\n",
      "epoch 10 / 100, step 72/80, loss = 0.2372\n",
      "epoch 10 / 100, step 73/80, loss = 0.2425\n",
      "epoch 10 / 100, step 74/80, loss = 0.2567\n",
      "epoch 10 / 100, step 75/80, loss = 0.2357\n",
      "epoch 10 / 100, step 76/80, loss = 0.2310\n",
      "epoch 10 / 100, step 77/80, loss = 0.2414\n",
      "epoch 10 / 100, step 78/80, loss = 0.2494\n",
      "epoch 10 / 100, step 79/80, loss = 0.2409\n",
      "epoch 10 / 100, step 80/80, loss = 0.2467\n",
      "epoch 11 / 100, step 1/80, loss = 0.2429\n",
      "epoch 11 / 100, step 2/80, loss = 0.2435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 / 100, step 3/80, loss = 0.2508\n",
      "epoch 11 / 100, step 4/80, loss = 0.2360\n",
      "epoch 11 / 100, step 5/80, loss = 0.2421\n",
      "epoch 11 / 100, step 6/80, loss = 0.2362\n",
      "epoch 11 / 100, step 7/80, loss = 0.2387\n",
      "epoch 11 / 100, step 8/80, loss = 0.2277\n",
      "epoch 11 / 100, step 9/80, loss = 0.2288\n",
      "epoch 11 / 100, step 10/80, loss = 0.2256\n",
      "epoch 11 / 100, step 11/80, loss = 0.2306\n",
      "epoch 11 / 100, step 12/80, loss = 0.2349\n",
      "epoch 11 / 100, step 13/80, loss = 0.2291\n",
      "epoch 11 / 100, step 14/80, loss = 0.2399\n",
      "epoch 11 / 100, step 15/80, loss = 0.2419\n",
      "epoch 11 / 100, step 16/80, loss = 0.2449\n",
      "epoch 11 / 100, step 17/80, loss = 0.2384\n",
      "epoch 11 / 100, step 18/80, loss = 0.2303\n",
      "epoch 11 / 100, step 19/80, loss = 0.2402\n",
      "epoch 11 / 100, step 20/80, loss = 0.2290\n",
      "epoch 11 / 100, step 21/80, loss = 0.2406\n",
      "epoch 11 / 100, step 22/80, loss = 0.2358\n",
      "epoch 11 / 100, step 23/80, loss = 0.2286\n",
      "epoch 11 / 100, step 24/80, loss = 0.2309\n",
      "epoch 11 / 100, step 25/80, loss = 0.2296\n",
      "epoch 11 / 100, step 26/80, loss = 0.2283\n",
      "epoch 11 / 100, step 27/80, loss = 0.2284\n",
      "epoch 11 / 100, step 28/80, loss = 0.2380\n",
      "epoch 11 / 100, step 29/80, loss = 0.2333\n",
      "epoch 11 / 100, step 30/80, loss = 0.2333\n",
      "epoch 11 / 100, step 31/80, loss = 0.2326\n",
      "epoch 11 / 100, step 32/80, loss = 0.2412\n",
      "epoch 11 / 100, step 33/80, loss = 0.2216\n",
      "epoch 11 / 100, step 34/80, loss = 0.2281\n",
      "epoch 11 / 100, step 35/80, loss = 0.2339\n",
      "epoch 11 / 100, step 36/80, loss = 0.2358\n",
      "epoch 11 / 100, step 37/80, loss = 0.2376\n",
      "epoch 11 / 100, step 38/80, loss = 0.2332\n",
      "epoch 11 / 100, step 39/80, loss = 0.2379\n",
      "epoch 11 / 100, step 40/80, loss = 0.2352\n",
      "epoch 11 / 100, step 41/80, loss = 0.2370\n",
      "epoch 11 / 100, step 42/80, loss = 0.2367\n",
      "epoch 11 / 100, step 43/80, loss = 0.2249\n",
      "epoch 11 / 100, step 44/80, loss = 0.2384\n",
      "epoch 11 / 100, step 45/80, loss = 0.2344\n",
      "epoch 11 / 100, step 46/80, loss = 0.2275\n",
      "epoch 11 / 100, step 47/80, loss = 0.2244\n",
      "epoch 11 / 100, step 48/80, loss = 0.2389\n",
      "epoch 11 / 100, step 49/80, loss = 0.2253\n",
      "epoch 11 / 100, step 50/80, loss = 0.2198\n",
      "epoch 11 / 100, step 51/80, loss = 0.2277\n",
      "epoch 11 / 100, step 52/80, loss = 0.2267\n",
      "epoch 11 / 100, step 53/80, loss = 0.2300\n",
      "epoch 11 / 100, step 54/80, loss = 0.2306\n",
      "epoch 11 / 100, step 55/80, loss = 0.2356\n",
      "epoch 11 / 100, step 56/80, loss = 0.2321\n",
      "epoch 11 / 100, step 57/80, loss = 0.2235\n",
      "epoch 11 / 100, step 58/80, loss = 0.2339\n",
      "epoch 11 / 100, step 59/80, loss = 0.2246\n",
      "epoch 11 / 100, step 60/80, loss = 0.2213\n",
      "epoch 11 / 100, step 61/80, loss = 0.2328\n",
      "epoch 11 / 100, step 62/80, loss = 0.2285\n",
      "epoch 11 / 100, step 63/80, loss = 0.2270\n",
      "epoch 11 / 100, step 64/80, loss = 0.2351\n",
      "epoch 11 / 100, step 65/80, loss = 0.2240\n",
      "epoch 11 / 100, step 66/80, loss = 0.2304\n",
      "epoch 11 / 100, step 67/80, loss = 0.2275\n",
      "epoch 11 / 100, step 68/80, loss = 0.2290\n",
      "epoch 11 / 100, step 69/80, loss = 0.2274\n",
      "epoch 11 / 100, step 70/80, loss = 0.2200\n",
      "epoch 11 / 100, step 71/80, loss = 0.2303\n",
      "epoch 11 / 100, step 72/80, loss = 0.2270\n",
      "epoch 11 / 100, step 73/80, loss = 0.2298\n",
      "epoch 11 / 100, step 74/80, loss = 0.2314\n",
      "epoch 11 / 100, step 75/80, loss = 0.2200\n",
      "epoch 11 / 100, step 76/80, loss = 0.2274\n",
      "epoch 11 / 100, step 77/80, loss = 0.2261\n",
      "epoch 11 / 100, step 78/80, loss = 0.2352\n",
      "epoch 11 / 100, step 79/80, loss = 0.2199\n",
      "epoch 11 / 100, step 80/80, loss = 0.2177\n",
      "epoch 12 / 100, step 1/80, loss = 0.2225\n",
      "epoch 12 / 100, step 2/80, loss = 0.2221\n",
      "epoch 12 / 100, step 3/80, loss = 0.2226\n",
      "epoch 12 / 100, step 4/80, loss = 0.2155\n",
      "epoch 12 / 100, step 5/80, loss = 0.2199\n",
      "epoch 12 / 100, step 6/80, loss = 0.2183\n",
      "epoch 12 / 100, step 7/80, loss = 0.2230\n",
      "epoch 12 / 100, step 8/80, loss = 0.2211\n",
      "epoch 12 / 100, step 9/80, loss = 0.2244\n",
      "epoch 12 / 100, step 10/80, loss = 0.2258\n",
      "epoch 12 / 100, step 11/80, loss = 0.2128\n",
      "epoch 12 / 100, step 12/80, loss = 0.2209\n",
      "epoch 12 / 100, step 13/80, loss = 0.2173\n",
      "epoch 12 / 100, step 14/80, loss = 0.2269\n",
      "epoch 12 / 100, step 15/80, loss = 0.2256\n",
      "epoch 12 / 100, step 16/80, loss = 0.2184\n",
      "epoch 12 / 100, step 17/80, loss = 0.2229\n",
      "epoch 12 / 100, step 18/80, loss = 0.2241\n",
      "epoch 12 / 100, step 19/80, loss = 0.2252\n",
      "epoch 12 / 100, step 20/80, loss = 0.2169\n",
      "epoch 12 / 100, step 21/80, loss = 0.2341\n",
      "epoch 12 / 100, step 22/80, loss = 0.2240\n",
      "epoch 12 / 100, step 23/80, loss = 0.2201\n",
      "epoch 12 / 100, step 24/80, loss = 0.2270\n",
      "epoch 12 / 100, step 25/80, loss = 0.2336\n",
      "epoch 12 / 100, step 26/80, loss = 0.2207\n",
      "epoch 12 / 100, step 27/80, loss = 0.2152\n",
      "epoch 12 / 100, step 28/80, loss = 0.2304\n",
      "epoch 12 / 100, step 29/80, loss = 0.2209\n",
      "epoch 12 / 100, step 30/80, loss = 0.2203\n",
      "epoch 12 / 100, step 31/80, loss = 0.2221\n",
      "epoch 12 / 100, step 32/80, loss = 0.2283\n",
      "epoch 12 / 100, step 33/80, loss = 0.2173\n",
      "epoch 12 / 100, step 34/80, loss = 0.2163\n",
      "epoch 12 / 100, step 35/80, loss = 0.2247\n",
      "epoch 12 / 100, step 36/80, loss = 0.2109\n",
      "epoch 12 / 100, step 37/80, loss = 0.2197\n",
      "epoch 12 / 100, step 38/80, loss = 0.2155\n",
      "epoch 12 / 100, step 39/80, loss = 0.2353\n",
      "epoch 12 / 100, step 40/80, loss = 0.2216\n",
      "epoch 12 / 100, step 41/80, loss = 0.2058\n",
      "epoch 12 / 100, step 42/80, loss = 0.2164\n",
      "epoch 12 / 100, step 43/80, loss = 0.2227\n",
      "epoch 12 / 100, step 44/80, loss = 0.2119\n",
      "epoch 12 / 100, step 45/80, loss = 0.2266\n",
      "epoch 12 / 100, step 46/80, loss = 0.2257\n",
      "epoch 12 / 100, step 47/80, loss = 0.2159\n",
      "epoch 12 / 100, step 48/80, loss = 0.2142\n",
      "epoch 12 / 100, step 49/80, loss = 0.2140\n",
      "epoch 12 / 100, step 50/80, loss = 0.2115\n",
      "epoch 12 / 100, step 51/80, loss = 0.2190\n",
      "epoch 12 / 100, step 52/80, loss = 0.2065\n",
      "epoch 12 / 100, step 53/80, loss = 0.2123\n",
      "epoch 12 / 100, step 54/80, loss = 0.2138\n",
      "epoch 12 / 100, step 55/80, loss = 0.2091\n",
      "epoch 12 / 100, step 56/80, loss = 0.2171\n",
      "epoch 12 / 100, step 57/80, loss = 0.2213\n",
      "epoch 12 / 100, step 58/80, loss = 0.2068\n",
      "epoch 12 / 100, step 59/80, loss = 0.2170\n",
      "epoch 12 / 100, step 60/80, loss = 0.2216\n",
      "epoch 12 / 100, step 61/80, loss = 0.2090\n",
      "epoch 12 / 100, step 62/80, loss = 0.2081\n",
      "epoch 12 / 100, step 63/80, loss = 0.2081\n",
      "epoch 12 / 100, step 64/80, loss = 0.2153\n",
      "epoch 12 / 100, step 65/80, loss = 0.2085\n",
      "epoch 12 / 100, step 66/80, loss = 0.2072\n",
      "epoch 12 / 100, step 67/80, loss = 0.2164\n",
      "epoch 12 / 100, step 68/80, loss = 0.2221\n",
      "epoch 12 / 100, step 69/80, loss = 0.2107\n",
      "epoch 12 / 100, step 70/80, loss = 0.2162\n",
      "epoch 12 / 100, step 71/80, loss = 0.2193\n",
      "epoch 12 / 100, step 72/80, loss = 0.2139\n",
      "epoch 12 / 100, step 73/80, loss = 0.2076\n",
      "epoch 12 / 100, step 74/80, loss = 0.2077\n",
      "epoch 12 / 100, step 75/80, loss = 0.2193\n",
      "epoch 12 / 100, step 76/80, loss = 0.2181\n",
      "epoch 12 / 100, step 77/80, loss = 0.2115\n",
      "epoch 12 / 100, step 78/80, loss = 0.2157\n",
      "epoch 12 / 100, step 79/80, loss = 0.2158\n",
      "epoch 12 / 100, step 80/80, loss = 0.2157\n",
      "epoch 13 / 100, step 1/80, loss = 0.2045\n",
      "epoch 13 / 100, step 2/80, loss = 0.2065\n",
      "epoch 13 / 100, step 3/80, loss = 0.2095\n",
      "epoch 13 / 100, step 4/80, loss = 0.2156\n",
      "epoch 13 / 100, step 5/80, loss = 0.2061\n",
      "epoch 13 / 100, step 6/80, loss = 0.2047\n",
      "epoch 13 / 100, step 7/80, loss = 0.2113\n",
      "epoch 13 / 100, step 8/80, loss = 0.2126\n",
      "epoch 13 / 100, step 9/80, loss = 0.2098\n",
      "epoch 13 / 100, step 10/80, loss = 0.2044\n",
      "epoch 13 / 100, step 11/80, loss = 0.2113\n",
      "epoch 13 / 100, step 12/80, loss = 0.2158\n",
      "epoch 13 / 100, step 13/80, loss = 0.2091\n",
      "epoch 13 / 100, step 14/80, loss = 0.2104\n",
      "epoch 13 / 100, step 15/80, loss = 0.2071\n",
      "epoch 13 / 100, step 16/80, loss = 0.2097\n",
      "epoch 13 / 100, step 17/80, loss = 0.2066\n",
      "epoch 13 / 100, step 18/80, loss = 0.2086\n",
      "epoch 13 / 100, step 19/80, loss = 0.2047\n",
      "epoch 13 / 100, step 20/80, loss = 0.2031\n",
      "epoch 13 / 100, step 21/80, loss = 0.2115\n",
      "epoch 13 / 100, step 22/80, loss = 0.2087\n",
      "epoch 13 / 100, step 23/80, loss = 0.1979\n",
      "epoch 13 / 100, step 24/80, loss = 0.2053\n",
      "epoch 13 / 100, step 25/80, loss = 0.2047\n",
      "epoch 13 / 100, step 26/80, loss = 0.2117\n",
      "epoch 13 / 100, step 27/80, loss = 0.2152\n",
      "epoch 13 / 100, step 28/80, loss = 0.2013\n",
      "epoch 13 / 100, step 29/80, loss = 0.2022\n",
      "epoch 13 / 100, step 30/80, loss = 0.2060\n",
      "epoch 13 / 100, step 31/80, loss = 0.2114\n",
      "epoch 13 / 100, step 32/80, loss = 0.2044\n",
      "epoch 13 / 100, step 33/80, loss = 0.2116\n",
      "epoch 13 / 100, step 34/80, loss = 0.2030\n",
      "epoch 13 / 100, step 35/80, loss = 0.1991\n",
      "epoch 13 / 100, step 36/80, loss = 0.2039\n",
      "epoch 13 / 100, step 37/80, loss = 0.2043\n",
      "epoch 13 / 100, step 38/80, loss = 0.2086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 / 100, step 39/80, loss = 0.2035\n",
      "epoch 13 / 100, step 40/80, loss = 0.2082\n",
      "epoch 13 / 100, step 41/80, loss = 0.1996\n",
      "epoch 13 / 100, step 42/80, loss = 0.2087\n",
      "epoch 13 / 100, step 43/80, loss = 0.2113\n",
      "epoch 13 / 100, step 44/80, loss = 0.2028\n",
      "epoch 13 / 100, step 45/80, loss = 0.2143\n",
      "epoch 13 / 100, step 46/80, loss = 0.2087\n",
      "epoch 13 / 100, step 47/80, loss = 0.2003\n",
      "epoch 13 / 100, step 48/80, loss = 0.2038\n",
      "epoch 13 / 100, step 49/80, loss = 0.1993\n",
      "epoch 13 / 100, step 50/80, loss = 0.2047\n",
      "epoch 13 / 100, step 51/80, loss = 0.2013\n",
      "epoch 13 / 100, step 52/80, loss = 0.2069\n",
      "epoch 13 / 100, step 53/80, loss = 0.2029\n",
      "epoch 13 / 100, step 54/80, loss = 0.2035\n",
      "epoch 13 / 100, step 55/80, loss = 0.2083\n",
      "epoch 13 / 100, step 56/80, loss = 0.2043\n",
      "epoch 13 / 100, step 57/80, loss = 0.1975\n",
      "epoch 13 / 100, step 58/80, loss = 0.2021\n",
      "epoch 13 / 100, step 59/80, loss = 0.2008\n",
      "epoch 13 / 100, step 60/80, loss = 0.2017\n",
      "epoch 13 / 100, step 61/80, loss = 0.2050\n",
      "epoch 13 / 100, step 62/80, loss = 0.2135\n",
      "epoch 13 / 100, step 63/80, loss = 0.2010\n",
      "epoch 13 / 100, step 64/80, loss = 0.2036\n",
      "epoch 13 / 100, step 65/80, loss = 0.2011\n",
      "epoch 13 / 100, step 66/80, loss = 0.1962\n",
      "epoch 13 / 100, step 67/80, loss = 0.2013\n",
      "epoch 13 / 100, step 68/80, loss = 0.1998\n",
      "epoch 13 / 100, step 69/80, loss = 0.2052\n",
      "epoch 13 / 100, step 70/80, loss = 0.2082\n",
      "epoch 13 / 100, step 71/80, loss = 0.2029\n",
      "epoch 13 / 100, step 72/80, loss = 0.2019\n",
      "epoch 13 / 100, step 73/80, loss = 0.2147\n",
      "epoch 13 / 100, step 74/80, loss = 0.2054\n",
      "epoch 13 / 100, step 75/80, loss = 0.2084\n",
      "epoch 13 / 100, step 76/80, loss = 0.2146\n",
      "epoch 13 / 100, step 77/80, loss = 0.2064\n",
      "epoch 13 / 100, step 78/80, loss = 0.2027\n",
      "epoch 13 / 100, step 79/80, loss = 0.1984\n",
      "epoch 13 / 100, step 80/80, loss = 0.1965\n",
      "epoch 14 / 100, step 1/80, loss = 0.2042\n",
      "epoch 14 / 100, step 2/80, loss = 0.2026\n",
      "epoch 14 / 100, step 3/80, loss = 0.2081\n",
      "epoch 14 / 100, step 4/80, loss = 0.2039\n",
      "epoch 14 / 100, step 5/80, loss = 0.1913\n",
      "epoch 14 / 100, step 6/80, loss = 0.1939\n",
      "epoch 14 / 100, step 7/80, loss = 0.2044\n",
      "epoch 14 / 100, step 8/80, loss = 0.2050\n",
      "epoch 14 / 100, step 9/80, loss = 0.1977\n",
      "epoch 14 / 100, step 10/80, loss = 0.1956\n",
      "epoch 14 / 100, step 11/80, loss = 0.2034\n",
      "epoch 14 / 100, step 12/80, loss = 0.1900\n",
      "epoch 14 / 100, step 13/80, loss = 0.1888\n",
      "epoch 14 / 100, step 14/80, loss = 0.1975\n",
      "epoch 14 / 100, step 15/80, loss = 0.1901\n",
      "epoch 14 / 100, step 16/80, loss = 0.1943\n",
      "epoch 14 / 100, step 17/80, loss = 0.1971\n",
      "epoch 14 / 100, step 18/80, loss = 0.1963\n",
      "epoch 14 / 100, step 19/80, loss = 0.1928\n",
      "epoch 14 / 100, step 20/80, loss = 0.1967\n",
      "epoch 14 / 100, step 21/80, loss = 0.1977\n",
      "epoch 14 / 100, step 22/80, loss = 0.1847\n",
      "epoch 14 / 100, step 23/80, loss = 0.1982\n",
      "epoch 14 / 100, step 24/80, loss = 0.2052\n",
      "epoch 14 / 100, step 25/80, loss = 0.1940\n",
      "epoch 14 / 100, step 26/80, loss = 0.2069\n",
      "epoch 14 / 100, step 27/80, loss = 0.1926\n",
      "epoch 14 / 100, step 28/80, loss = 0.1983\n",
      "epoch 14 / 100, step 29/80, loss = 0.1897\n",
      "epoch 14 / 100, step 30/80, loss = 0.1943\n",
      "epoch 14 / 100, step 31/80, loss = 0.1973\n",
      "epoch 14 / 100, step 32/80, loss = 0.1986\n",
      "epoch 14 / 100, step 33/80, loss = 0.1999\n",
      "epoch 14 / 100, step 34/80, loss = 0.1981\n",
      "epoch 14 / 100, step 35/80, loss = 0.1873\n",
      "epoch 14 / 100, step 36/80, loss = 0.1941\n",
      "epoch 14 / 100, step 37/80, loss = 0.1908\n",
      "epoch 14 / 100, step 38/80, loss = 0.2015\n",
      "epoch 14 / 100, step 39/80, loss = 0.1875\n",
      "epoch 14 / 100, step 40/80, loss = 0.1904\n",
      "epoch 14 / 100, step 41/80, loss = 0.1942\n",
      "epoch 14 / 100, step 42/80, loss = 0.1944\n",
      "epoch 14 / 100, step 43/80, loss = 0.1912\n",
      "epoch 14 / 100, step 44/80, loss = 0.1960\n",
      "epoch 14 / 100, step 45/80, loss = 0.1879\n",
      "epoch 14 / 100, step 46/80, loss = 0.1938\n",
      "epoch 14 / 100, step 47/80, loss = 0.1852\n",
      "epoch 14 / 100, step 48/80, loss = 0.2136\n",
      "epoch 14 / 100, step 49/80, loss = 0.1935\n",
      "epoch 14 / 100, step 50/80, loss = 0.1888\n",
      "epoch 14 / 100, step 51/80, loss = 0.1966\n",
      "epoch 14 / 100, step 52/80, loss = 0.1927\n",
      "epoch 14 / 100, step 53/80, loss = 0.1871\n",
      "epoch 14 / 100, step 54/80, loss = 0.1950\n",
      "epoch 14 / 100, step 55/80, loss = 0.1839\n",
      "epoch 14 / 100, step 56/80, loss = 0.1956\n",
      "epoch 14 / 100, step 57/80, loss = 0.1896\n",
      "epoch 14 / 100, step 58/80, loss = 0.1965\n",
      "epoch 14 / 100, step 59/80, loss = 0.2014\n",
      "epoch 14 / 100, step 60/80, loss = 0.1887\n",
      "epoch 14 / 100, step 61/80, loss = 0.1858\n",
      "epoch 14 / 100, step 62/80, loss = 0.1923\n",
      "epoch 14 / 100, step 63/80, loss = 0.1904\n",
      "epoch 14 / 100, step 64/80, loss = 0.1843\n",
      "epoch 14 / 100, step 65/80, loss = 0.1952\n",
      "epoch 14 / 100, step 66/80, loss = 0.1902\n",
      "epoch 14 / 100, step 67/80, loss = 0.1873\n",
      "epoch 14 / 100, step 68/80, loss = 0.1964\n",
      "epoch 14 / 100, step 69/80, loss = 0.1899\n",
      "epoch 14 / 100, step 70/80, loss = 0.1899\n",
      "epoch 14 / 100, step 71/80, loss = 0.1957\n",
      "epoch 14 / 100, step 72/80, loss = 0.1929\n",
      "epoch 14 / 100, step 73/80, loss = 0.1923\n",
      "epoch 14 / 100, step 74/80, loss = 0.1972\n",
      "epoch 14 / 100, step 75/80, loss = 0.1990\n",
      "epoch 14 / 100, step 76/80, loss = 0.2010\n",
      "epoch 14 / 100, step 77/80, loss = 0.1868\n",
      "epoch 14 / 100, step 78/80, loss = 0.1846\n",
      "epoch 14 / 100, step 79/80, loss = 0.1887\n",
      "epoch 14 / 100, step 80/80, loss = 0.1890\n",
      "epoch 15 / 100, step 1/80, loss = 0.1836\n",
      "epoch 15 / 100, step 2/80, loss = 0.1907\n",
      "epoch 15 / 100, step 3/80, loss = 0.1785\n",
      "epoch 15 / 100, step 4/80, loss = 0.1968\n",
      "epoch 15 / 100, step 5/80, loss = 0.1880\n",
      "epoch 15 / 100, step 6/80, loss = 0.1845\n",
      "epoch 15 / 100, step 7/80, loss = 0.1850\n",
      "epoch 15 / 100, step 8/80, loss = 0.1887\n",
      "epoch 15 / 100, step 9/80, loss = 0.1902\n",
      "epoch 15 / 100, step 10/80, loss = 0.1910\n",
      "epoch 15 / 100, step 11/80, loss = 0.1925\n",
      "epoch 15 / 100, step 12/80, loss = 0.1841\n",
      "epoch 15 / 100, step 13/80, loss = 0.1826\n",
      "epoch 15 / 100, step 14/80, loss = 0.1827\n",
      "epoch 15 / 100, step 15/80, loss = 0.1885\n",
      "epoch 15 / 100, step 16/80, loss = 0.1801\n",
      "epoch 15 / 100, step 17/80, loss = 0.1845\n",
      "epoch 15 / 100, step 18/80, loss = 0.1823\n",
      "epoch 15 / 100, step 19/80, loss = 0.1821\n",
      "epoch 15 / 100, step 20/80, loss = 0.1850\n",
      "epoch 15 / 100, step 21/80, loss = 0.1948\n",
      "epoch 15 / 100, step 22/80, loss = 0.1896\n",
      "epoch 15 / 100, step 23/80, loss = 0.1859\n",
      "epoch 15 / 100, step 24/80, loss = 0.1920\n",
      "epoch 15 / 100, step 25/80, loss = 0.1832\n",
      "epoch 15 / 100, step 26/80, loss = 0.1834\n",
      "epoch 15 / 100, step 27/80, loss = 0.1850\n",
      "epoch 15 / 100, step 28/80, loss = 0.1821\n",
      "epoch 15 / 100, step 29/80, loss = 0.1840\n",
      "epoch 15 / 100, step 30/80, loss = 0.1885\n",
      "epoch 15 / 100, step 31/80, loss = 0.1827\n",
      "epoch 15 / 100, step 32/80, loss = 0.1830\n",
      "epoch 15 / 100, step 33/80, loss = 0.1938\n",
      "epoch 15 / 100, step 34/80, loss = 0.1935\n",
      "epoch 15 / 100, step 35/80, loss = 0.1800\n",
      "epoch 15 / 100, step 36/80, loss = 0.1796\n",
      "epoch 15 / 100, step 37/80, loss = 0.1829\n",
      "epoch 15 / 100, step 38/80, loss = 0.1867\n",
      "epoch 15 / 100, step 39/80, loss = 0.1841\n",
      "epoch 15 / 100, step 40/80, loss = 0.1908\n",
      "epoch 15 / 100, step 41/80, loss = 0.1818\n",
      "epoch 15 / 100, step 42/80, loss = 0.1810\n",
      "epoch 15 / 100, step 43/80, loss = 0.1805\n",
      "epoch 15 / 100, step 44/80, loss = 0.1843\n",
      "epoch 15 / 100, step 45/80, loss = 0.1849\n",
      "epoch 15 / 100, step 46/80, loss = 0.1795\n",
      "epoch 15 / 100, step 47/80, loss = 0.1773\n",
      "epoch 15 / 100, step 48/80, loss = 0.1818\n",
      "epoch 15 / 100, step 49/80, loss = 0.1813\n",
      "epoch 15 / 100, step 50/80, loss = 0.1734\n",
      "epoch 15 / 100, step 51/80, loss = 0.1889\n",
      "epoch 15 / 100, step 52/80, loss = 0.1872\n",
      "epoch 15 / 100, step 53/80, loss = 0.1835\n",
      "epoch 15 / 100, step 54/80, loss = 0.1827\n",
      "epoch 15 / 100, step 55/80, loss = 0.1795\n",
      "epoch 15 / 100, step 56/80, loss = 0.1861\n",
      "epoch 15 / 100, step 57/80, loss = 0.1824\n",
      "epoch 15 / 100, step 58/80, loss = 0.1813\n",
      "epoch 15 / 100, step 59/80, loss = 0.1862\n",
      "epoch 15 / 100, step 60/80, loss = 0.1819\n",
      "epoch 15 / 100, step 61/80, loss = 0.1825\n",
      "epoch 15 / 100, step 62/80, loss = 0.1779\n",
      "epoch 15 / 100, step 63/80, loss = 0.1765\n",
      "epoch 15 / 100, step 64/80, loss = 0.1914\n",
      "epoch 15 / 100, step 65/80, loss = 0.1838\n",
      "epoch 15 / 100, step 66/80, loss = 0.1808\n",
      "epoch 15 / 100, step 67/80, loss = 0.1795\n",
      "epoch 15 / 100, step 68/80, loss = 0.1774\n",
      "epoch 15 / 100, step 69/80, loss = 0.1912\n",
      "epoch 15 / 100, step 70/80, loss = 0.1757\n",
      "epoch 15 / 100, step 71/80, loss = 0.1757\n",
      "epoch 15 / 100, step 72/80, loss = 0.1835\n",
      "epoch 15 / 100, step 73/80, loss = 0.1804\n",
      "epoch 15 / 100, step 74/80, loss = 0.1745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 / 100, step 75/80, loss = 0.1870\n",
      "epoch 15 / 100, step 76/80, loss = 0.1812\n",
      "epoch 15 / 100, step 77/80, loss = 0.1822\n",
      "epoch 15 / 100, step 78/80, loss = 0.1724\n",
      "epoch 15 / 100, step 79/80, loss = 0.1705\n",
      "epoch 15 / 100, step 80/80, loss = 0.1739\n",
      "epoch 16 / 100, step 1/80, loss = 0.1753\n",
      "epoch 16 / 100, step 2/80, loss = 0.1672\n",
      "epoch 16 / 100, step 3/80, loss = 0.1853\n",
      "epoch 16 / 100, step 4/80, loss = 0.1759\n",
      "epoch 16 / 100, step 5/80, loss = 0.1703\n",
      "epoch 16 / 100, step 6/80, loss = 0.1759\n",
      "epoch 16 / 100, step 7/80, loss = 0.1750\n",
      "epoch 16 / 100, step 8/80, loss = 0.1731\n",
      "epoch 16 / 100, step 9/80, loss = 0.1800\n",
      "epoch 16 / 100, step 10/80, loss = 0.1735\n",
      "epoch 16 / 100, step 11/80, loss = 0.1784\n",
      "epoch 16 / 100, step 12/80, loss = 0.1731\n",
      "epoch 16 / 100, step 13/80, loss = 0.1781\n",
      "epoch 16 / 100, step 14/80, loss = 0.1823\n",
      "epoch 16 / 100, step 15/80, loss = 0.1700\n",
      "epoch 16 / 100, step 16/80, loss = 0.1757\n",
      "epoch 16 / 100, step 17/80, loss = 0.1985\n",
      "epoch 16 / 100, step 18/80, loss = 0.1789\n",
      "epoch 16 / 100, step 19/80, loss = 0.1852\n",
      "epoch 16 / 100, step 20/80, loss = 0.1833\n",
      "epoch 16 / 100, step 21/80, loss = 0.1747\n",
      "epoch 16 / 100, step 22/80, loss = 0.1715\n",
      "epoch 16 / 100, step 23/80, loss = 0.1819\n",
      "epoch 16 / 100, step 24/80, loss = 0.1771\n",
      "epoch 16 / 100, step 25/80, loss = 0.1681\n",
      "epoch 16 / 100, step 26/80, loss = 0.1676\n",
      "epoch 16 / 100, step 27/80, loss = 0.1728\n",
      "epoch 16 / 100, step 28/80, loss = 0.1839\n",
      "epoch 16 / 100, step 29/80, loss = 0.1773\n",
      "epoch 16 / 100, step 30/80, loss = 0.1726\n",
      "epoch 16 / 100, step 31/80, loss = 0.1793\n",
      "epoch 16 / 100, step 32/80, loss = 0.1731\n",
      "epoch 16 / 100, step 33/80, loss = 0.1761\n",
      "epoch 16 / 100, step 34/80, loss = 0.1756\n",
      "epoch 16 / 100, step 35/80, loss = 0.1749\n",
      "epoch 16 / 100, step 36/80, loss = 0.1686\n",
      "epoch 16 / 100, step 37/80, loss = 0.1702\n",
      "epoch 16 / 100, step 38/80, loss = 0.1731\n",
      "epoch 16 / 100, step 39/80, loss = 0.1714\n",
      "epoch 16 / 100, step 40/80, loss = 0.1728\n",
      "epoch 16 / 100, step 41/80, loss = 0.1698\n",
      "epoch 16 / 100, step 42/80, loss = 0.1760\n",
      "epoch 16 / 100, step 43/80, loss = 0.1765\n",
      "epoch 16 / 100, step 44/80, loss = 0.1668\n",
      "epoch 16 / 100, step 45/80, loss = 0.1753\n",
      "epoch 16 / 100, step 46/80, loss = 0.1700\n",
      "epoch 16 / 100, step 47/80, loss = 0.1800\n",
      "epoch 16 / 100, step 48/80, loss = 0.1756\n",
      "epoch 16 / 100, step 49/80, loss = 0.1764\n",
      "epoch 16 / 100, step 50/80, loss = 0.1731\n",
      "epoch 16 / 100, step 51/80, loss = 0.1719\n",
      "epoch 16 / 100, step 52/80, loss = 0.1730\n",
      "epoch 16 / 100, step 53/80, loss = 0.1716\n",
      "epoch 16 / 100, step 54/80, loss = 0.1745\n",
      "epoch 16 / 100, step 55/80, loss = 0.1726\n",
      "epoch 16 / 100, step 56/80, loss = 0.1634\n",
      "epoch 16 / 100, step 57/80, loss = 0.1750\n",
      "epoch 16 / 100, step 58/80, loss = 0.1793\n",
      "epoch 16 / 100, step 59/80, loss = 0.1678\n",
      "epoch 16 / 100, step 60/80, loss = 0.1722\n",
      "epoch 16 / 100, step 61/80, loss = 0.1731\n",
      "epoch 16 / 100, step 62/80, loss = 0.1755\n",
      "epoch 16 / 100, step 63/80, loss = 0.1631\n",
      "epoch 16 / 100, step 64/80, loss = 0.1758\n",
      "epoch 16 / 100, step 65/80, loss = 0.1714\n",
      "epoch 16 / 100, step 66/80, loss = 0.1713\n",
      "epoch 16 / 100, step 67/80, loss = 0.1631\n",
      "epoch 16 / 100, step 68/80, loss = 0.1760\n",
      "epoch 16 / 100, step 69/80, loss = 0.1746\n",
      "epoch 16 / 100, step 70/80, loss = 0.1785\n",
      "epoch 16 / 100, step 71/80, loss = 0.1682\n",
      "epoch 16 / 100, step 72/80, loss = 0.1623\n",
      "epoch 16 / 100, step 73/80, loss = 0.1727\n",
      "epoch 16 / 100, step 74/80, loss = 0.1729\n",
      "epoch 16 / 100, step 75/80, loss = 0.1654\n",
      "epoch 16 / 100, step 76/80, loss = 0.1765\n",
      "epoch 16 / 100, step 77/80, loss = 0.1613\n",
      "epoch 16 / 100, step 78/80, loss = 0.1716\n",
      "epoch 16 / 100, step 79/80, loss = 0.1770\n",
      "epoch 16 / 100, step 80/80, loss = 0.1705\n",
      "epoch 17 / 100, step 1/80, loss = 0.1772\n",
      "epoch 17 / 100, step 2/80, loss = 0.1638\n",
      "epoch 17 / 100, step 3/80, loss = 0.1607\n",
      "epoch 17 / 100, step 4/80, loss = 0.1688\n",
      "epoch 17 / 100, step 5/80, loss = 0.1677\n",
      "epoch 17 / 100, step 6/80, loss = 0.1684\n",
      "epoch 17 / 100, step 7/80, loss = 0.1751\n",
      "epoch 17 / 100, step 8/80, loss = 0.1702\n",
      "epoch 17 / 100, step 9/80, loss = 0.1712\n",
      "epoch 17 / 100, step 10/80, loss = 0.1627\n",
      "epoch 17 / 100, step 11/80, loss = 0.1636\n",
      "epoch 17 / 100, step 12/80, loss = 0.1673\n",
      "epoch 17 / 100, step 13/80, loss = 0.1675\n",
      "epoch 17 / 100, step 14/80, loss = 0.1723\n",
      "epoch 17 / 100, step 15/80, loss = 0.1631\n",
      "epoch 17 / 100, step 16/80, loss = 0.1686\n",
      "epoch 17 / 100, step 17/80, loss = 0.1684\n",
      "epoch 17 / 100, step 18/80, loss = 0.1768\n",
      "epoch 17 / 100, step 19/80, loss = 0.1718\n",
      "epoch 17 / 100, step 20/80, loss = 0.1665\n",
      "epoch 17 / 100, step 21/80, loss = 0.1644\n",
      "epoch 17 / 100, step 22/80, loss = 0.1691\n",
      "epoch 17 / 100, step 23/80, loss = 0.1683\n",
      "epoch 17 / 100, step 24/80, loss = 0.1636\n",
      "epoch 17 / 100, step 25/80, loss = 0.1631\n",
      "epoch 17 / 100, step 26/80, loss = 0.1635\n",
      "epoch 17 / 100, step 27/80, loss = 0.1604\n",
      "epoch 17 / 100, step 28/80, loss = 0.1767\n",
      "epoch 17 / 100, step 29/80, loss = 0.1625\n",
      "epoch 17 / 100, step 30/80, loss = 0.1704\n",
      "epoch 17 / 100, step 31/80, loss = 0.1617\n",
      "epoch 17 / 100, step 32/80, loss = 0.1650\n",
      "epoch 17 / 100, step 33/80, loss = 0.1572\n",
      "epoch 17 / 100, step 34/80, loss = 0.1716\n",
      "epoch 17 / 100, step 35/80, loss = 0.1648\n",
      "epoch 17 / 100, step 36/80, loss = 0.1731\n",
      "epoch 17 / 100, step 37/80, loss = 0.1613\n",
      "epoch 17 / 100, step 38/80, loss = 0.1666\n",
      "epoch 17 / 100, step 39/80, loss = 0.1632\n",
      "epoch 17 / 100, step 40/80, loss = 0.1626\n",
      "epoch 17 / 100, step 41/80, loss = 0.1690\n",
      "epoch 17 / 100, step 42/80, loss = 0.1629\n",
      "epoch 17 / 100, step 43/80, loss = 0.1734\n",
      "epoch 17 / 100, step 44/80, loss = 0.1549\n",
      "epoch 17 / 100, step 45/80, loss = 0.1632\n",
      "epoch 17 / 100, step 46/80, loss = 0.1667\n",
      "epoch 17 / 100, step 47/80, loss = 0.1641\n",
      "epoch 17 / 100, step 48/80, loss = 0.1707\n",
      "epoch 17 / 100, step 49/80, loss = 0.1641\n",
      "epoch 17 / 100, step 50/80, loss = 0.1629\n",
      "epoch 17 / 100, step 51/80, loss = 0.1734\n",
      "epoch 17 / 100, step 52/80, loss = 0.1587\n",
      "epoch 17 / 100, step 53/80, loss = 0.1676\n",
      "epoch 17 / 100, step 54/80, loss = 0.1624\n",
      "epoch 17 / 100, step 55/80, loss = 0.1624\n",
      "epoch 17 / 100, step 56/80, loss = 0.1585\n",
      "epoch 17 / 100, step 57/80, loss = 0.1614\n",
      "epoch 17 / 100, step 58/80, loss = 0.1618\n",
      "epoch 17 / 100, step 59/80, loss = 0.1655\n",
      "epoch 17 / 100, step 60/80, loss = 0.1706\n",
      "epoch 17 / 100, step 61/80, loss = 0.1559\n",
      "epoch 17 / 100, step 62/80, loss = 0.1668\n",
      "epoch 17 / 100, step 63/80, loss = 0.1625\n",
      "epoch 17 / 100, step 64/80, loss = 0.1595\n",
      "epoch 17 / 100, step 65/80, loss = 0.1570\n",
      "epoch 17 / 100, step 66/80, loss = 0.1586\n",
      "epoch 17 / 100, step 67/80, loss = 0.1616\n",
      "epoch 17 / 100, step 68/80, loss = 0.1656\n",
      "epoch 17 / 100, step 69/80, loss = 0.1612\n",
      "epoch 17 / 100, step 70/80, loss = 0.1598\n",
      "epoch 17 / 100, step 71/80, loss = 0.1651\n",
      "epoch 17 / 100, step 72/80, loss = 0.1671\n",
      "epoch 17 / 100, step 73/80, loss = 0.1588\n",
      "epoch 17 / 100, step 74/80, loss = 0.1597\n",
      "epoch 17 / 100, step 75/80, loss = 0.1555\n",
      "epoch 17 / 100, step 76/80, loss = 0.1636\n",
      "epoch 17 / 100, step 77/80, loss = 0.1634\n",
      "epoch 17 / 100, step 78/80, loss = 0.1580\n",
      "epoch 17 / 100, step 79/80, loss = 0.1598\n",
      "epoch 17 / 100, step 80/80, loss = 0.1616\n",
      "epoch 18 / 100, step 1/80, loss = 0.1637\n",
      "epoch 18 / 100, step 2/80, loss = 0.1607\n",
      "epoch 18 / 100, step 3/80, loss = 0.1599\n",
      "epoch 18 / 100, step 4/80, loss = 0.1655\n",
      "epoch 18 / 100, step 5/80, loss = 0.1606\n",
      "epoch 18 / 100, step 6/80, loss = 0.1605\n",
      "epoch 18 / 100, step 7/80, loss = 0.1536\n",
      "epoch 18 / 100, step 8/80, loss = 0.1571\n",
      "epoch 18 / 100, step 9/80, loss = 0.1579\n",
      "epoch 18 / 100, step 10/80, loss = 0.1547\n",
      "epoch 18 / 100, step 11/80, loss = 0.1524\n",
      "epoch 18 / 100, step 12/80, loss = 0.1597\n",
      "epoch 18 / 100, step 13/80, loss = 0.1544\n",
      "epoch 18 / 100, step 14/80, loss = 0.1597\n",
      "epoch 18 / 100, step 15/80, loss = 0.1552\n",
      "epoch 18 / 100, step 16/80, loss = 0.1650\n",
      "epoch 18 / 100, step 17/80, loss = 0.1526\n",
      "epoch 18 / 100, step 18/80, loss = 0.1676\n",
      "epoch 18 / 100, step 19/80, loss = 0.1600\n",
      "epoch 18 / 100, step 20/80, loss = 0.1622\n",
      "epoch 18 / 100, step 21/80, loss = 0.1570\n",
      "epoch 18 / 100, step 22/80, loss = 0.1607\n",
      "epoch 18 / 100, step 23/80, loss = 0.1555\n",
      "epoch 18 / 100, step 24/80, loss = 0.1523\n",
      "epoch 18 / 100, step 25/80, loss = 0.1532\n",
      "epoch 18 / 100, step 26/80, loss = 0.1517\n",
      "epoch 18 / 100, step 27/80, loss = 0.1610\n",
      "epoch 18 / 100, step 28/80, loss = 0.1550\n",
      "epoch 18 / 100, step 29/80, loss = 0.1575\n",
      "epoch 18 / 100, step 30/80, loss = 0.1541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 / 100, step 31/80, loss = 0.1553\n",
      "epoch 18 / 100, step 32/80, loss = 0.1619\n",
      "epoch 18 / 100, step 33/80, loss = 0.1589\n",
      "epoch 18 / 100, step 34/80, loss = 0.1489\n",
      "epoch 18 / 100, step 35/80, loss = 0.1561\n",
      "epoch 18 / 100, step 36/80, loss = 0.1629\n",
      "epoch 18 / 100, step 37/80, loss = 0.1590\n",
      "epoch 18 / 100, step 38/80, loss = 0.1570\n",
      "epoch 18 / 100, step 39/80, loss = 0.1593\n",
      "epoch 18 / 100, step 40/80, loss = 0.1560\n",
      "epoch 18 / 100, step 41/80, loss = 0.1562\n",
      "epoch 18 / 100, step 42/80, loss = 0.1511\n",
      "epoch 18 / 100, step 43/80, loss = 0.1567\n",
      "epoch 18 / 100, step 44/80, loss = 0.1538\n",
      "epoch 18 / 100, step 45/80, loss = 0.1583\n",
      "epoch 18 / 100, step 46/80, loss = 0.1622\n",
      "epoch 18 / 100, step 47/80, loss = 0.1531\n",
      "epoch 18 / 100, step 48/80, loss = 0.1583\n",
      "epoch 18 / 100, step 49/80, loss = 0.1564\n",
      "epoch 18 / 100, step 50/80, loss = 0.1518\n",
      "epoch 18 / 100, step 51/80, loss = 0.1609\n",
      "epoch 18 / 100, step 52/80, loss = 0.1663\n",
      "epoch 18 / 100, step 53/80, loss = 0.1608\n",
      "epoch 18 / 100, step 54/80, loss = 0.1551\n",
      "epoch 18 / 100, step 55/80, loss = 0.1517\n",
      "epoch 18 / 100, step 56/80, loss = 0.1608\n",
      "epoch 18 / 100, step 57/80, loss = 0.1568\n",
      "epoch 18 / 100, step 58/80, loss = 0.1553\n",
      "epoch 18 / 100, step 59/80, loss = 0.1546\n",
      "epoch 18 / 100, step 60/80, loss = 0.1565\n",
      "epoch 18 / 100, step 61/80, loss = 0.1651\n",
      "epoch 18 / 100, step 62/80, loss = 0.1627\n",
      "epoch 18 / 100, step 63/80, loss = 0.1519\n",
      "epoch 18 / 100, step 64/80, loss = 0.1502\n",
      "epoch 18 / 100, step 65/80, loss = 0.1470\n",
      "epoch 18 / 100, step 66/80, loss = 0.1510\n",
      "epoch 18 / 100, step 67/80, loss = 0.1552\n",
      "epoch 18 / 100, step 68/80, loss = 0.1516\n",
      "epoch 18 / 100, step 69/80, loss = 0.1494\n",
      "epoch 18 / 100, step 70/80, loss = 0.1532\n",
      "epoch 18 / 100, step 71/80, loss = 0.1597\n",
      "epoch 18 / 100, step 72/80, loss = 0.1524\n",
      "epoch 18 / 100, step 73/80, loss = 0.1563\n",
      "epoch 18 / 100, step 74/80, loss = 0.1559\n",
      "epoch 18 / 100, step 75/80, loss = 0.1541\n",
      "epoch 18 / 100, step 76/80, loss = 0.1475\n",
      "epoch 18 / 100, step 77/80, loss = 0.1482\n",
      "epoch 18 / 100, step 78/80, loss = 0.1473\n",
      "epoch 18 / 100, step 79/80, loss = 0.1528\n",
      "epoch 18 / 100, step 80/80, loss = 0.1540\n",
      "epoch 19 / 100, step 1/80, loss = 0.1516\n",
      "epoch 19 / 100, step 2/80, loss = 0.1526\n",
      "epoch 19 / 100, step 3/80, loss = 0.1505\n",
      "epoch 19 / 100, step 4/80, loss = 0.1469\n",
      "epoch 19 / 100, step 5/80, loss = 0.1542\n",
      "epoch 19 / 100, step 6/80, loss = 0.1484\n",
      "epoch 19 / 100, step 7/80, loss = 0.1438\n",
      "epoch 19 / 100, step 8/80, loss = 0.1495\n",
      "epoch 19 / 100, step 9/80, loss = 0.1450\n",
      "epoch 19 / 100, step 10/80, loss = 0.1587\n",
      "epoch 19 / 100, step 11/80, loss = 0.1489\n",
      "epoch 19 / 100, step 12/80, loss = 0.1513\n",
      "epoch 19 / 100, step 13/80, loss = 0.1460\n",
      "epoch 19 / 100, step 14/80, loss = 0.1521\n",
      "epoch 19 / 100, step 15/80, loss = 0.1564\n",
      "epoch 19 / 100, step 16/80, loss = 0.1512\n",
      "epoch 19 / 100, step 17/80, loss = 0.1545\n",
      "epoch 19 / 100, step 18/80, loss = 0.1478\n",
      "epoch 19 / 100, step 19/80, loss = 0.1503\n",
      "epoch 19 / 100, step 20/80, loss = 0.1506\n",
      "epoch 19 / 100, step 21/80, loss = 0.1557\n",
      "epoch 19 / 100, step 22/80, loss = 0.1445\n",
      "epoch 19 / 100, step 23/80, loss = 0.1498\n",
      "epoch 19 / 100, step 24/80, loss = 0.1466\n",
      "epoch 19 / 100, step 25/80, loss = 0.1466\n",
      "epoch 19 / 100, step 26/80, loss = 0.1465\n",
      "epoch 19 / 100, step 27/80, loss = 0.1448\n",
      "epoch 19 / 100, step 28/80, loss = 0.1506\n",
      "epoch 19 / 100, step 29/80, loss = 0.1505\n",
      "epoch 19 / 100, step 30/80, loss = 0.1539\n",
      "epoch 19 / 100, step 31/80, loss = 0.1540\n",
      "epoch 19 / 100, step 32/80, loss = 0.1482\n",
      "epoch 19 / 100, step 33/80, loss = 0.1470\n",
      "epoch 19 / 100, step 34/80, loss = 0.1455\n",
      "epoch 19 / 100, step 35/80, loss = 0.1482\n",
      "epoch 19 / 100, step 36/80, loss = 0.1441\n",
      "epoch 19 / 100, step 37/80, loss = 0.1505\n",
      "epoch 19 / 100, step 38/80, loss = 0.1428\n",
      "epoch 19 / 100, step 39/80, loss = 0.1508\n",
      "epoch 19 / 100, step 40/80, loss = 0.1423\n",
      "epoch 19 / 100, step 41/80, loss = 0.1450\n",
      "epoch 19 / 100, step 42/80, loss = 0.1501\n",
      "epoch 19 / 100, step 43/80, loss = 0.1475\n",
      "epoch 19 / 100, step 44/80, loss = 0.1473\n",
      "epoch 19 / 100, step 45/80, loss = 0.1569\n",
      "epoch 19 / 100, step 46/80, loss = 0.1461\n",
      "epoch 19 / 100, step 47/80, loss = 0.1510\n",
      "epoch 19 / 100, step 48/80, loss = 0.1517\n",
      "epoch 19 / 100, step 49/80, loss = 0.1399\n",
      "epoch 19 / 100, step 50/80, loss = 0.1522\n",
      "epoch 19 / 100, step 51/80, loss = 0.1470\n",
      "epoch 19 / 100, step 52/80, loss = 0.1522\n",
      "epoch 19 / 100, step 53/80, loss = 0.1535\n",
      "epoch 19 / 100, step 54/80, loss = 0.1442\n",
      "epoch 19 / 100, step 55/80, loss = 0.1501\n",
      "epoch 19 / 100, step 56/80, loss = 0.1523\n",
      "epoch 19 / 100, step 57/80, loss = 0.1408\n",
      "epoch 19 / 100, step 58/80, loss = 0.1505\n",
      "epoch 19 / 100, step 59/80, loss = 0.1446\n",
      "epoch 19 / 100, step 60/80, loss = 0.1546\n",
      "epoch 19 / 100, step 61/80, loss = 0.1541\n",
      "epoch 19 / 100, step 62/80, loss = 0.1559\n",
      "epoch 19 / 100, step 63/80, loss = 0.1405\n",
      "epoch 19 / 100, step 64/80, loss = 0.1425\n",
      "epoch 19 / 100, step 65/80, loss = 0.1555\n",
      "epoch 19 / 100, step 66/80, loss = 0.1462\n",
      "epoch 19 / 100, step 67/80, loss = 0.1499\n",
      "epoch 19 / 100, step 68/80, loss = 0.1461\n",
      "epoch 19 / 100, step 69/80, loss = 0.1539\n",
      "epoch 19 / 100, step 70/80, loss = 0.1500\n",
      "epoch 19 / 100, step 71/80, loss = 0.1510\n",
      "epoch 19 / 100, step 72/80, loss = 0.1467\n",
      "epoch 19 / 100, step 73/80, loss = 0.1496\n",
      "epoch 19 / 100, step 74/80, loss = 0.1481\n",
      "epoch 19 / 100, step 75/80, loss = 0.1392\n",
      "epoch 19 / 100, step 76/80, loss = 0.1440\n",
      "epoch 19 / 100, step 77/80, loss = 0.1513\n",
      "epoch 19 / 100, step 78/80, loss = 0.1388\n",
      "epoch 19 / 100, step 79/80, loss = 0.1415\n",
      "epoch 19 / 100, step 80/80, loss = 0.1480\n",
      "epoch 20 / 100, step 1/80, loss = 0.1378\n",
      "epoch 20 / 100, step 2/80, loss = 0.1459\n",
      "epoch 20 / 100, step 3/80, loss = 0.1416\n",
      "epoch 20 / 100, step 4/80, loss = 0.1466\n",
      "epoch 20 / 100, step 5/80, loss = 0.1448\n",
      "epoch 20 / 100, step 6/80, loss = 0.1451\n",
      "epoch 20 / 100, step 7/80, loss = 0.1426\n",
      "epoch 20 / 100, step 8/80, loss = 0.1467\n",
      "epoch 20 / 100, step 9/80, loss = 0.1443\n",
      "epoch 20 / 100, step 10/80, loss = 0.1398\n",
      "epoch 20 / 100, step 11/80, loss = 0.1457\n",
      "epoch 20 / 100, step 12/80, loss = 0.1488\n",
      "epoch 20 / 100, step 13/80, loss = 0.1445\n",
      "epoch 20 / 100, step 14/80, loss = 0.1430\n",
      "epoch 20 / 100, step 15/80, loss = 0.1405\n",
      "epoch 20 / 100, step 16/80, loss = 0.1457\n",
      "epoch 20 / 100, step 17/80, loss = 0.1403\n",
      "epoch 20 / 100, step 18/80, loss = 0.1387\n",
      "epoch 20 / 100, step 19/80, loss = 0.1473\n",
      "epoch 20 / 100, step 20/80, loss = 0.1437\n",
      "epoch 20 / 100, step 21/80, loss = 0.1460\n",
      "epoch 20 / 100, step 22/80, loss = 0.1355\n",
      "epoch 20 / 100, step 23/80, loss = 0.1403\n",
      "epoch 20 / 100, step 24/80, loss = 0.1537\n",
      "epoch 20 / 100, step 25/80, loss = 0.1402\n",
      "epoch 20 / 100, step 26/80, loss = 0.1486\n",
      "epoch 20 / 100, step 27/80, loss = 0.1495\n",
      "epoch 20 / 100, step 28/80, loss = 0.1498\n",
      "epoch 20 / 100, step 29/80, loss = 0.1391\n",
      "epoch 20 / 100, step 30/80, loss = 0.1386\n",
      "epoch 20 / 100, step 31/80, loss = 0.1423\n",
      "epoch 20 / 100, step 32/80, loss = 0.1411\n",
      "epoch 20 / 100, step 33/80, loss = 0.1469\n",
      "epoch 20 / 100, step 34/80, loss = 0.1460\n",
      "epoch 20 / 100, step 35/80, loss = 0.1429\n",
      "epoch 20 / 100, step 36/80, loss = 0.1341\n",
      "epoch 20 / 100, step 37/80, loss = 0.1375\n",
      "epoch 20 / 100, step 38/80, loss = 0.1395\n",
      "epoch 20 / 100, step 39/80, loss = 0.1423\n",
      "epoch 20 / 100, step 40/80, loss = 0.1425\n",
      "epoch 20 / 100, step 41/80, loss = 0.1454\n",
      "epoch 20 / 100, step 42/80, loss = 0.1474\n",
      "epoch 20 / 100, step 43/80, loss = 0.1390\n",
      "epoch 20 / 100, step 44/80, loss = 0.1414\n",
      "epoch 20 / 100, step 45/80, loss = 0.1343\n",
      "epoch 20 / 100, step 46/80, loss = 0.1354\n",
      "epoch 20 / 100, step 47/80, loss = 0.1331\n",
      "epoch 20 / 100, step 48/80, loss = 0.1423\n",
      "epoch 20 / 100, step 49/80, loss = 0.1436\n",
      "epoch 20 / 100, step 50/80, loss = 0.1445\n",
      "epoch 20 / 100, step 51/80, loss = 0.1420\n",
      "epoch 20 / 100, step 52/80, loss = 0.1415\n",
      "epoch 20 / 100, step 53/80, loss = 0.1404\n",
      "epoch 20 / 100, step 54/80, loss = 0.1376\n",
      "epoch 20 / 100, step 55/80, loss = 0.1453\n",
      "epoch 20 / 100, step 56/80, loss = 0.1372\n",
      "epoch 20 / 100, step 57/80, loss = 0.1414\n",
      "epoch 20 / 100, step 58/80, loss = 0.1394\n",
      "epoch 20 / 100, step 59/80, loss = 0.1528\n",
      "epoch 20 / 100, step 60/80, loss = 0.1421\n",
      "epoch 20 / 100, step 61/80, loss = 0.1381\n",
      "epoch 20 / 100, step 62/80, loss = 0.1419\n",
      "epoch 20 / 100, step 63/80, loss = 0.1463\n",
      "epoch 20 / 100, step 64/80, loss = 0.1429\n",
      "epoch 20 / 100, step 65/80, loss = 0.1414\n",
      "epoch 20 / 100, step 66/80, loss = 0.1417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 / 100, step 67/80, loss = 0.1488\n",
      "epoch 20 / 100, step 68/80, loss = 0.1371\n",
      "epoch 20 / 100, step 69/80, loss = 0.1416\n",
      "epoch 20 / 100, step 70/80, loss = 0.1351\n",
      "epoch 20 / 100, step 71/80, loss = 0.1488\n",
      "epoch 20 / 100, step 72/80, loss = 0.1403\n",
      "epoch 20 / 100, step 73/80, loss = 0.1489\n",
      "epoch 20 / 100, step 74/80, loss = 0.1389\n",
      "epoch 20 / 100, step 75/80, loss = 0.1447\n",
      "epoch 20 / 100, step 76/80, loss = 0.1383\n",
      "epoch 20 / 100, step 77/80, loss = 0.1329\n",
      "epoch 20 / 100, step 78/80, loss = 0.1398\n",
      "epoch 20 / 100, step 79/80, loss = 0.1372\n",
      "epoch 20 / 100, step 80/80, loss = 0.1355\n",
      "epoch 21 / 100, step 1/80, loss = 0.1369\n",
      "epoch 21 / 100, step 2/80, loss = 0.1335\n",
      "epoch 21 / 100, step 3/80, loss = 0.1371\n",
      "epoch 21 / 100, step 4/80, loss = 0.1330\n",
      "epoch 21 / 100, step 5/80, loss = 0.1375\n",
      "epoch 21 / 100, step 6/80, loss = 0.1411\n",
      "epoch 21 / 100, step 7/80, loss = 0.1326\n",
      "epoch 21 / 100, step 8/80, loss = 0.1402\n",
      "epoch 21 / 100, step 9/80, loss = 0.1363\n",
      "epoch 21 / 100, step 10/80, loss = 0.1411\n",
      "epoch 21 / 100, step 11/80, loss = 0.1341\n",
      "epoch 21 / 100, step 12/80, loss = 0.1365\n",
      "epoch 21 / 100, step 13/80, loss = 0.1402\n",
      "epoch 21 / 100, step 14/80, loss = 0.1334\n",
      "epoch 21 / 100, step 15/80, loss = 0.1414\n",
      "epoch 21 / 100, step 16/80, loss = 0.1352\n",
      "epoch 21 / 100, step 17/80, loss = 0.1295\n",
      "epoch 21 / 100, step 18/80, loss = 0.1354\n",
      "epoch 21 / 100, step 19/80, loss = 0.1331\n",
      "epoch 21 / 100, step 20/80, loss = 0.1311\n",
      "epoch 21 / 100, step 21/80, loss = 0.1260\n",
      "epoch 21 / 100, step 22/80, loss = 0.1354\n",
      "epoch 21 / 100, step 23/80, loss = 0.1434\n",
      "epoch 21 / 100, step 24/80, loss = 0.1330\n",
      "epoch 21 / 100, step 25/80, loss = 0.1366\n",
      "epoch 21 / 100, step 26/80, loss = 0.1347\n",
      "epoch 21 / 100, step 27/80, loss = 0.1404\n",
      "epoch 21 / 100, step 28/80, loss = 0.1349\n",
      "epoch 21 / 100, step 29/80, loss = 0.1294\n",
      "epoch 21 / 100, step 30/80, loss = 0.1390\n",
      "epoch 21 / 100, step 31/80, loss = 0.1403\n",
      "epoch 21 / 100, step 32/80, loss = 0.1418\n",
      "epoch 21 / 100, step 33/80, loss = 0.1373\n",
      "epoch 21 / 100, step 34/80, loss = 0.1395\n",
      "epoch 21 / 100, step 35/80, loss = 0.1329\n",
      "epoch 21 / 100, step 36/80, loss = 0.1308\n",
      "epoch 21 / 100, step 37/80, loss = 0.1309\n",
      "epoch 21 / 100, step 38/80, loss = 0.1400\n",
      "epoch 21 / 100, step 39/80, loss = 0.1365\n",
      "epoch 21 / 100, step 40/80, loss = 0.1437\n",
      "epoch 21 / 100, step 41/80, loss = 0.1304\n",
      "epoch 21 / 100, step 42/80, loss = 0.1373\n",
      "epoch 21 / 100, step 43/80, loss = 0.1367\n",
      "epoch 21 / 100, step 44/80, loss = 0.1465\n",
      "epoch 21 / 100, step 45/80, loss = 0.1343\n",
      "epoch 21 / 100, step 46/80, loss = 0.1345\n",
      "epoch 21 / 100, step 47/80, loss = 0.1458\n",
      "epoch 21 / 100, step 48/80, loss = 0.1316\n",
      "epoch 21 / 100, step 49/80, loss = 0.1289\n",
      "epoch 21 / 100, step 50/80, loss = 0.1387\n",
      "epoch 21 / 100, step 51/80, loss = 0.1313\n",
      "epoch 21 / 100, step 52/80, loss = 0.1385\n",
      "epoch 21 / 100, step 53/80, loss = 0.1317\n",
      "epoch 21 / 100, step 54/80, loss = 0.1336\n",
      "epoch 21 / 100, step 55/80, loss = 0.1255\n",
      "epoch 21 / 100, step 56/80, loss = 0.1290\n",
      "epoch 21 / 100, step 57/80, loss = 0.1257\n",
      "epoch 21 / 100, step 58/80, loss = 0.1385\n",
      "epoch 21 / 100, step 59/80, loss = 0.1387\n",
      "epoch 21 / 100, step 60/80, loss = 0.1387\n",
      "epoch 21 / 100, step 61/80, loss = 0.1367\n",
      "epoch 21 / 100, step 62/80, loss = 0.1294\n",
      "epoch 21 / 100, step 63/80, loss = 0.1363\n",
      "epoch 21 / 100, step 64/80, loss = 0.1326\n",
      "epoch 21 / 100, step 65/80, loss = 0.1305\n",
      "epoch 21 / 100, step 66/80, loss = 0.1364\n",
      "epoch 21 / 100, step 67/80, loss = 0.1307\n",
      "epoch 21 / 100, step 68/80, loss = 0.1383\n",
      "epoch 21 / 100, step 69/80, loss = 0.1308\n",
      "epoch 21 / 100, step 70/80, loss = 0.1369\n",
      "epoch 21 / 100, step 71/80, loss = 0.1381\n",
      "epoch 21 / 100, step 72/80, loss = 0.1330\n",
      "epoch 21 / 100, step 73/80, loss = 0.1345\n",
      "epoch 21 / 100, step 74/80, loss = 0.1296\n",
      "epoch 21 / 100, step 75/80, loss = 0.1324\n",
      "epoch 21 / 100, step 76/80, loss = 0.1296\n",
      "epoch 21 / 100, step 77/80, loss = 0.1297\n",
      "epoch 21 / 100, step 78/80, loss = 0.1404\n",
      "epoch 21 / 100, step 79/80, loss = 0.1384\n",
      "epoch 21 / 100, step 80/80, loss = 0.1396\n",
      "epoch 22 / 100, step 1/80, loss = 0.1284\n",
      "epoch 22 / 100, step 2/80, loss = 0.1255\n",
      "epoch 22 / 100, step 3/80, loss = 0.1267\n",
      "epoch 22 / 100, step 4/80, loss = 0.1314\n",
      "epoch 22 / 100, step 5/80, loss = 0.1289\n",
      "epoch 22 / 100, step 6/80, loss = 0.1252\n",
      "epoch 22 / 100, step 7/80, loss = 0.1301\n",
      "epoch 22 / 100, step 8/80, loss = 0.1320\n",
      "epoch 22 / 100, step 9/80, loss = 0.1243\n",
      "epoch 22 / 100, step 10/80, loss = 0.1290\n",
      "epoch 22 / 100, step 11/80, loss = 0.1281\n",
      "epoch 22 / 100, step 12/80, loss = 0.1354\n",
      "epoch 22 / 100, step 13/80, loss = 0.1294\n",
      "epoch 22 / 100, step 14/80, loss = 0.1335\n",
      "epoch 22 / 100, step 15/80, loss = 0.1312\n",
      "epoch 22 / 100, step 16/80, loss = 0.1303\n",
      "epoch 22 / 100, step 17/80, loss = 0.1405\n",
      "epoch 22 / 100, step 18/80, loss = 0.1290\n",
      "epoch 22 / 100, step 19/80, loss = 0.1274\n",
      "epoch 22 / 100, step 20/80, loss = 0.1283\n",
      "epoch 22 / 100, step 21/80, loss = 0.1336\n",
      "epoch 22 / 100, step 22/80, loss = 0.1254\n",
      "epoch 22 / 100, step 23/80, loss = 0.1309\n",
      "epoch 22 / 100, step 24/80, loss = 0.1312\n",
      "epoch 22 / 100, step 25/80, loss = 0.1273\n",
      "epoch 22 / 100, step 26/80, loss = 0.1301\n",
      "epoch 22 / 100, step 27/80, loss = 0.1315\n",
      "epoch 22 / 100, step 28/80, loss = 0.1240\n",
      "epoch 22 / 100, step 29/80, loss = 0.1299\n",
      "epoch 22 / 100, step 30/80, loss = 0.1329\n",
      "epoch 22 / 100, step 31/80, loss = 0.1257\n",
      "epoch 22 / 100, step 32/80, loss = 0.1238\n",
      "epoch 22 / 100, step 33/80, loss = 0.1289\n",
      "epoch 22 / 100, step 34/80, loss = 0.1264\n",
      "epoch 22 / 100, step 35/80, loss = 0.1340\n",
      "epoch 22 / 100, step 36/80, loss = 0.1302\n",
      "epoch 22 / 100, step 37/80, loss = 0.1306\n",
      "epoch 22 / 100, step 38/80, loss = 0.1286\n",
      "epoch 22 / 100, step 39/80, loss = 0.1243\n",
      "epoch 22 / 100, step 40/80, loss = 0.1295\n",
      "epoch 22 / 100, step 41/80, loss = 0.1346\n",
      "epoch 22 / 100, step 42/80, loss = 0.1269\n",
      "epoch 22 / 100, step 43/80, loss = 0.1315\n",
      "epoch 22 / 100, step 44/80, loss = 0.1259\n",
      "epoch 22 / 100, step 45/80, loss = 0.1249\n",
      "epoch 22 / 100, step 46/80, loss = 0.1240\n",
      "epoch 22 / 100, step 47/80, loss = 0.1261\n",
      "epoch 22 / 100, step 48/80, loss = 0.1294\n",
      "epoch 22 / 100, step 49/80, loss = 0.1269\n",
      "epoch 22 / 100, step 50/80, loss = 0.1294\n",
      "epoch 22 / 100, step 51/80, loss = 0.1307\n",
      "epoch 22 / 100, step 52/80, loss = 0.1236\n",
      "epoch 22 / 100, step 53/80, loss = 0.1279\n",
      "epoch 22 / 100, step 54/80, loss = 0.1289\n",
      "epoch 22 / 100, step 55/80, loss = 0.1214\n",
      "epoch 22 / 100, step 56/80, loss = 0.1254\n",
      "epoch 22 / 100, step 57/80, loss = 0.1287\n",
      "epoch 22 / 100, step 58/80, loss = 0.1294\n",
      "epoch 22 / 100, step 59/80, loss = 0.1220\n",
      "epoch 22 / 100, step 60/80, loss = 0.1246\n",
      "epoch 22 / 100, step 61/80, loss = 0.1285\n",
      "epoch 22 / 100, step 62/80, loss = 0.1377\n",
      "epoch 22 / 100, step 63/80, loss = 0.1277\n",
      "epoch 22 / 100, step 64/80, loss = 0.1364\n",
      "epoch 22 / 100, step 65/80, loss = 0.1314\n",
      "epoch 22 / 100, step 66/80, loss = 0.1262\n",
      "epoch 22 / 100, step 67/80, loss = 0.1303\n",
      "epoch 22 / 100, step 68/80, loss = 0.1196\n",
      "epoch 22 / 100, step 69/80, loss = 0.1317\n",
      "epoch 22 / 100, step 70/80, loss = 0.1297\n",
      "epoch 22 / 100, step 71/80, loss = 0.1259\n",
      "epoch 22 / 100, step 72/80, loss = 0.1303\n",
      "epoch 22 / 100, step 73/80, loss = 0.1219\n",
      "epoch 22 / 100, step 74/80, loss = 0.1340\n",
      "epoch 22 / 100, step 75/80, loss = 0.1258\n",
      "epoch 22 / 100, step 76/80, loss = 0.1384\n",
      "epoch 22 / 100, step 77/80, loss = 0.1206\n",
      "epoch 22 / 100, step 78/80, loss = 0.1293\n",
      "epoch 22 / 100, step 79/80, loss = 0.1440\n",
      "epoch 22 / 100, step 80/80, loss = 0.1226\n",
      "epoch 23 / 100, step 1/80, loss = 0.1221\n",
      "epoch 23 / 100, step 2/80, loss = 0.1236\n",
      "epoch 23 / 100, step 3/80, loss = 0.1256\n",
      "epoch 23 / 100, step 4/80, loss = 0.1227\n",
      "epoch 23 / 100, step 5/80, loss = 0.1317\n",
      "epoch 23 / 100, step 6/80, loss = 0.1230\n",
      "epoch 23 / 100, step 7/80, loss = 0.1246\n",
      "epoch 23 / 100, step 8/80, loss = 0.1264\n",
      "epoch 23 / 100, step 9/80, loss = 0.1226\n",
      "epoch 23 / 100, step 10/80, loss = 0.1241\n",
      "epoch 23 / 100, step 11/80, loss = 0.1297\n",
      "epoch 23 / 100, step 12/80, loss = 0.1277\n",
      "epoch 23 / 100, step 13/80, loss = 0.1224\n",
      "epoch 23 / 100, step 14/80, loss = 0.1275\n",
      "epoch 23 / 100, step 15/80, loss = 0.1261\n",
      "epoch 23 / 100, step 16/80, loss = 0.1260\n",
      "epoch 23 / 100, step 17/80, loss = 0.1198\n",
      "epoch 23 / 100, step 18/80, loss = 0.1223\n",
      "epoch 23 / 100, step 19/80, loss = 0.1241\n",
      "epoch 23 / 100, step 20/80, loss = 0.1274\n",
      "epoch 23 / 100, step 21/80, loss = 0.1261\n",
      "epoch 23 / 100, step 22/80, loss = 0.1266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 / 100, step 23/80, loss = 0.1250\n",
      "epoch 23 / 100, step 24/80, loss = 0.1188\n",
      "epoch 23 / 100, step 25/80, loss = 0.1282\n",
      "epoch 23 / 100, step 26/80, loss = 0.1242\n",
      "epoch 23 / 100, step 27/80, loss = 0.1254\n",
      "epoch 23 / 100, step 28/80, loss = 0.1249\n",
      "epoch 23 / 100, step 29/80, loss = 0.1313\n",
      "epoch 23 / 100, step 30/80, loss = 0.1212\n",
      "epoch 23 / 100, step 31/80, loss = 0.1279\n",
      "epoch 23 / 100, step 32/80, loss = 0.1249\n",
      "epoch 23 / 100, step 33/80, loss = 0.1201\n",
      "epoch 23 / 100, step 34/80, loss = 0.1175\n",
      "epoch 23 / 100, step 35/80, loss = 0.1178\n",
      "epoch 23 / 100, step 36/80, loss = 0.1219\n",
      "epoch 23 / 100, step 37/80, loss = 0.1289\n",
      "epoch 23 / 100, step 38/80, loss = 0.1367\n",
      "epoch 23 / 100, step 39/80, loss = 0.1216\n",
      "epoch 23 / 100, step 40/80, loss = 0.1222\n",
      "epoch 23 / 100, step 41/80, loss = 0.1202\n",
      "epoch 23 / 100, step 42/80, loss = 0.1240\n",
      "epoch 23 / 100, step 43/80, loss = 0.1237\n",
      "epoch 23 / 100, step 44/80, loss = 0.1205\n",
      "epoch 23 / 100, step 45/80, loss = 0.1303\n",
      "epoch 23 / 100, step 46/80, loss = 0.1279\n",
      "epoch 23 / 100, step 47/80, loss = 0.1235\n",
      "epoch 23 / 100, step 48/80, loss = 0.1268\n",
      "epoch 23 / 100, step 49/80, loss = 0.1203\n",
      "epoch 23 / 100, step 50/80, loss = 0.1237\n",
      "epoch 23 / 100, step 51/80, loss = 0.1197\n",
      "epoch 23 / 100, step 52/80, loss = 0.1203\n",
      "epoch 23 / 100, step 53/80, loss = 0.1174\n",
      "epoch 23 / 100, step 54/80, loss = 0.1244\n",
      "epoch 23 / 100, step 55/80, loss = 0.1219\n",
      "epoch 23 / 100, step 56/80, loss = 0.1222\n",
      "epoch 23 / 100, step 57/80, loss = 0.1176\n",
      "epoch 23 / 100, step 58/80, loss = 0.1188\n",
      "epoch 23 / 100, step 59/80, loss = 0.1222\n",
      "epoch 23 / 100, step 60/80, loss = 0.1264\n",
      "epoch 23 / 100, step 61/80, loss = 0.1268\n",
      "epoch 23 / 100, step 62/80, loss = 0.1249\n",
      "epoch 23 / 100, step 63/80, loss = 0.1314\n",
      "epoch 23 / 100, step 64/80, loss = 0.1137\n",
      "epoch 23 / 100, step 65/80, loss = 0.1183\n",
      "epoch 23 / 100, step 66/80, loss = 0.1236\n",
      "epoch 23 / 100, step 67/80, loss = 0.1230\n",
      "epoch 23 / 100, step 68/80, loss = 0.1198\n",
      "epoch 23 / 100, step 69/80, loss = 0.1244\n",
      "epoch 23 / 100, step 70/80, loss = 0.1213\n",
      "epoch 23 / 100, step 71/80, loss = 0.1153\n",
      "epoch 23 / 100, step 72/80, loss = 0.1230\n",
      "epoch 23 / 100, step 73/80, loss = 0.1171\n",
      "epoch 23 / 100, step 74/80, loss = 0.1221\n",
      "epoch 23 / 100, step 75/80, loss = 0.1204\n",
      "epoch 23 / 100, step 76/80, loss = 0.1237\n",
      "epoch 23 / 100, step 77/80, loss = 0.1205\n",
      "epoch 23 / 100, step 78/80, loss = 0.1199\n",
      "epoch 23 / 100, step 79/80, loss = 0.1214\n",
      "epoch 23 / 100, step 80/80, loss = 0.1140\n",
      "epoch 24 / 100, step 1/80, loss = 0.1284\n",
      "epoch 24 / 100, step 2/80, loss = 0.1203\n",
      "epoch 24 / 100, step 3/80, loss = 0.1213\n",
      "epoch 24 / 100, step 4/80, loss = 0.1253\n",
      "epoch 24 / 100, step 5/80, loss = 0.1157\n",
      "epoch 24 / 100, step 6/80, loss = 0.1232\n",
      "epoch 24 / 100, step 7/80, loss = 0.1225\n",
      "epoch 24 / 100, step 8/80, loss = 0.1228\n",
      "epoch 24 / 100, step 9/80, loss = 0.1182\n",
      "epoch 24 / 100, step 10/80, loss = 0.1169\n",
      "epoch 24 / 100, step 11/80, loss = 0.1281\n",
      "epoch 24 / 100, step 12/80, loss = 0.1180\n",
      "epoch 24 / 100, step 13/80, loss = 0.1155\n",
      "epoch 24 / 100, step 14/80, loss = 0.1199\n",
      "epoch 24 / 100, step 15/80, loss = 0.1128\n",
      "epoch 24 / 100, step 16/80, loss = 0.1172\n",
      "epoch 24 / 100, step 17/80, loss = 0.1179\n",
      "epoch 24 / 100, step 18/80, loss = 0.1170\n",
      "epoch 24 / 100, step 19/80, loss = 0.1176\n",
      "epoch 24 / 100, step 20/80, loss = 0.1227\n",
      "epoch 24 / 100, step 21/80, loss = 0.1190\n",
      "epoch 24 / 100, step 22/80, loss = 0.1191\n",
      "epoch 24 / 100, step 23/80, loss = 0.1111\n",
      "epoch 24 / 100, step 24/80, loss = 0.1234\n",
      "epoch 24 / 100, step 25/80, loss = 0.1199\n",
      "epoch 24 / 100, step 26/80, loss = 0.1276\n",
      "epoch 24 / 100, step 27/80, loss = 0.1313\n",
      "epoch 24 / 100, step 28/80, loss = 0.1161\n",
      "epoch 24 / 100, step 29/80, loss = 0.1211\n",
      "epoch 24 / 100, step 30/80, loss = 0.1144\n",
      "epoch 24 / 100, step 31/80, loss = 0.1171\n",
      "epoch 24 / 100, step 32/80, loss = 0.1164\n",
      "epoch 24 / 100, step 33/80, loss = 0.1231\n",
      "epoch 24 / 100, step 34/80, loss = 0.1156\n",
      "epoch 24 / 100, step 35/80, loss = 0.1216\n",
      "epoch 24 / 100, step 36/80, loss = 0.1265\n",
      "epoch 24 / 100, step 37/80, loss = 0.1172\n",
      "epoch 24 / 100, step 38/80, loss = 0.1185\n",
      "epoch 24 / 100, step 39/80, loss = 0.1245\n",
      "epoch 24 / 100, step 40/80, loss = 0.1196\n",
      "epoch 24 / 100, step 41/80, loss = 0.1201\n",
      "epoch 24 / 100, step 42/80, loss = 0.1116\n",
      "epoch 24 / 100, step 43/80, loss = 0.1179\n",
      "epoch 24 / 100, step 44/80, loss = 0.1178\n",
      "epoch 24 / 100, step 45/80, loss = 0.1134\n",
      "epoch 24 / 100, step 46/80, loss = 0.1173\n",
      "epoch 24 / 100, step 47/80, loss = 0.1177\n",
      "epoch 24 / 100, step 48/80, loss = 0.1165\n",
      "epoch 24 / 100, step 49/80, loss = 0.1176\n",
      "epoch 24 / 100, step 50/80, loss = 0.1154\n",
      "epoch 24 / 100, step 51/80, loss = 0.1129\n",
      "epoch 24 / 100, step 52/80, loss = 0.1207\n",
      "epoch 24 / 100, step 53/80, loss = 0.1164\n",
      "epoch 24 / 100, step 54/80, loss = 0.1223\n",
      "epoch 24 / 100, step 55/80, loss = 0.1186\n",
      "epoch 24 / 100, step 56/80, loss = 0.1166\n",
      "epoch 24 / 100, step 57/80, loss = 0.1172\n",
      "epoch 24 / 100, step 58/80, loss = 0.1124\n",
      "epoch 24 / 100, step 59/80, loss = 0.1268\n",
      "epoch 24 / 100, step 60/80, loss = 0.1198\n",
      "epoch 24 / 100, step 61/80, loss = 0.1149\n",
      "epoch 24 / 100, step 62/80, loss = 0.1119\n",
      "epoch 24 / 100, step 63/80, loss = 0.1202\n",
      "epoch 24 / 100, step 64/80, loss = 0.1169\n",
      "epoch 24 / 100, step 65/80, loss = 0.1179\n",
      "epoch 24 / 100, step 66/80, loss = 0.1255\n",
      "epoch 24 / 100, step 67/80, loss = 0.1182\n",
      "epoch 24 / 100, step 68/80, loss = 0.1189\n",
      "epoch 24 / 100, step 69/80, loss = 0.1219\n",
      "epoch 24 / 100, step 70/80, loss = 0.1157\n",
      "epoch 24 / 100, step 71/80, loss = 0.1148\n",
      "epoch 24 / 100, step 72/80, loss = 0.1185\n",
      "epoch 24 / 100, step 73/80, loss = 0.1126\n",
      "epoch 24 / 100, step 74/80, loss = 0.1180\n",
      "epoch 24 / 100, step 75/80, loss = 0.1208\n",
      "epoch 24 / 100, step 76/80, loss = 0.1186\n",
      "epoch 24 / 100, step 77/80, loss = 0.1152\n",
      "epoch 24 / 100, step 78/80, loss = 0.1075\n",
      "epoch 24 / 100, step 79/80, loss = 0.1167\n",
      "epoch 24 / 100, step 80/80, loss = 0.1172\n",
      "epoch 25 / 100, step 1/80, loss = 0.1148\n",
      "epoch 25 / 100, step 2/80, loss = 0.1165\n",
      "epoch 25 / 100, step 3/80, loss = 0.1137\n",
      "epoch 25 / 100, step 4/80, loss = 0.1145\n",
      "epoch 25 / 100, step 5/80, loss = 0.1102\n",
      "epoch 25 / 100, step 6/80, loss = 0.1197\n",
      "epoch 25 / 100, step 7/80, loss = 0.1124\n",
      "epoch 25 / 100, step 8/80, loss = 0.1127\n",
      "epoch 25 / 100, step 9/80, loss = 0.1100\n",
      "epoch 25 / 100, step 10/80, loss = 0.1082\n",
      "epoch 25 / 100, step 11/80, loss = 0.1093\n",
      "epoch 25 / 100, step 12/80, loss = 0.1148\n",
      "epoch 25 / 100, step 13/80, loss = 0.1219\n",
      "epoch 25 / 100, step 14/80, loss = 0.1111\n",
      "epoch 25 / 100, step 15/80, loss = 0.1182\n",
      "epoch 25 / 100, step 16/80, loss = 0.1142\n",
      "epoch 25 / 100, step 17/80, loss = 0.1068\n",
      "epoch 25 / 100, step 18/80, loss = 0.1175\n",
      "epoch 25 / 100, step 19/80, loss = 0.1117\n",
      "epoch 25 / 100, step 20/80, loss = 0.1138\n",
      "epoch 25 / 100, step 21/80, loss = 0.1114\n",
      "epoch 25 / 100, step 22/80, loss = 0.1154\n",
      "epoch 25 / 100, step 23/80, loss = 0.1083\n",
      "epoch 25 / 100, step 24/80, loss = 0.1147\n",
      "epoch 25 / 100, step 25/80, loss = 0.1119\n",
      "epoch 25 / 100, step 26/80, loss = 0.1103\n",
      "epoch 25 / 100, step 27/80, loss = 0.1080\n",
      "epoch 25 / 100, step 28/80, loss = 0.1159\n",
      "epoch 25 / 100, step 29/80, loss = 0.1147\n",
      "epoch 25 / 100, step 30/80, loss = 0.1111\n",
      "epoch 25 / 100, step 31/80, loss = 0.1168\n",
      "epoch 25 / 100, step 32/80, loss = 0.1147\n",
      "epoch 25 / 100, step 33/80, loss = 0.1216\n",
      "epoch 25 / 100, step 34/80, loss = 0.1161\n",
      "epoch 25 / 100, step 35/80, loss = 0.1099\n",
      "epoch 25 / 100, step 36/80, loss = 0.1134\n",
      "epoch 25 / 100, step 37/80, loss = 0.1140\n",
      "epoch 25 / 100, step 38/80, loss = 0.1217\n",
      "epoch 25 / 100, step 39/80, loss = 0.1141\n",
      "epoch 25 / 100, step 40/80, loss = 0.1140\n",
      "epoch 25 / 100, step 41/80, loss = 0.1130\n",
      "epoch 25 / 100, step 42/80, loss = 0.1161\n",
      "epoch 25 / 100, step 43/80, loss = 0.1135\n",
      "epoch 25 / 100, step 44/80, loss = 0.1163\n",
      "epoch 25 / 100, step 45/80, loss = 0.1126\n",
      "epoch 25 / 100, step 46/80, loss = 0.1120\n",
      "epoch 25 / 100, step 47/80, loss = 0.1143\n",
      "epoch 25 / 100, step 48/80, loss = 0.1165\n",
      "epoch 25 / 100, step 49/80, loss = 0.1186\n",
      "epoch 25 / 100, step 50/80, loss = 0.1092\n",
      "epoch 25 / 100, step 51/80, loss = 0.1197\n",
      "epoch 25 / 100, step 52/80, loss = 0.1174\n",
      "epoch 25 / 100, step 53/80, loss = 0.1047\n",
      "epoch 25 / 100, step 54/80, loss = 0.1161\n",
      "epoch 25 / 100, step 55/80, loss = 0.1169\n",
      "epoch 25 / 100, step 56/80, loss = 0.1093\n",
      "epoch 25 / 100, step 57/80, loss = 0.1121\n",
      "epoch 25 / 100, step 58/80, loss = 0.1121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 / 100, step 59/80, loss = 0.1139\n",
      "epoch 25 / 100, step 60/80, loss = 0.1156\n",
      "epoch 25 / 100, step 61/80, loss = 0.1175\n",
      "epoch 25 / 100, step 62/80, loss = 0.1046\n",
      "epoch 25 / 100, step 63/80, loss = 0.1165\n",
      "epoch 25 / 100, step 64/80, loss = 0.1098\n",
      "epoch 25 / 100, step 65/80, loss = 0.1154\n",
      "epoch 25 / 100, step 66/80, loss = 0.1055\n",
      "epoch 25 / 100, step 67/80, loss = 0.1099\n",
      "epoch 25 / 100, step 68/80, loss = 0.1100\n",
      "epoch 25 / 100, step 69/80, loss = 0.1078\n",
      "epoch 25 / 100, step 70/80, loss = 0.1204\n",
      "epoch 25 / 100, step 71/80, loss = 0.1107\n",
      "epoch 25 / 100, step 72/80, loss = 0.1093\n",
      "epoch 25 / 100, step 73/80, loss = 0.1113\n",
      "epoch 25 / 100, step 74/80, loss = 0.1120\n",
      "epoch 25 / 100, step 75/80, loss = 0.1166\n",
      "epoch 25 / 100, step 76/80, loss = 0.1097\n",
      "epoch 25 / 100, step 77/80, loss = 0.1090\n",
      "epoch 25 / 100, step 78/80, loss = 0.1148\n",
      "epoch 25 / 100, step 79/80, loss = 0.1045\n",
      "epoch 25 / 100, step 80/80, loss = 0.1067\n",
      "epoch 26 / 100, step 1/80, loss = 0.1040\n",
      "epoch 26 / 100, step 2/80, loss = 0.1089\n",
      "epoch 26 / 100, step 3/80, loss = 0.1094\n",
      "epoch 26 / 100, step 4/80, loss = 0.1054\n",
      "epoch 26 / 100, step 5/80, loss = 0.1181\n",
      "epoch 26 / 100, step 6/80, loss = 0.1074\n",
      "epoch 26 / 100, step 7/80, loss = 0.1103\n",
      "epoch 26 / 100, step 8/80, loss = 0.1119\n",
      "epoch 26 / 100, step 9/80, loss = 0.1062\n",
      "epoch 26 / 100, step 10/80, loss = 0.1121\n",
      "epoch 26 / 100, step 11/80, loss = 0.1129\n",
      "epoch 26 / 100, step 12/80, loss = 0.1089\n",
      "epoch 26 / 100, step 13/80, loss = 0.1103\n",
      "epoch 26 / 100, step 14/80, loss = 0.1146\n",
      "epoch 26 / 100, step 15/80, loss = 0.1128\n",
      "epoch 26 / 100, step 16/80, loss = 0.1113\n",
      "epoch 26 / 100, step 17/80, loss = 0.1083\n",
      "epoch 26 / 100, step 18/80, loss = 0.1121\n",
      "epoch 26 / 100, step 19/80, loss = 0.1058\n",
      "epoch 26 / 100, step 20/80, loss = 0.1111\n",
      "epoch 26 / 100, step 21/80, loss = 0.1173\n",
      "epoch 26 / 100, step 22/80, loss = 0.1104\n",
      "epoch 26 / 100, step 23/80, loss = 0.1075\n",
      "epoch 26 / 100, step 24/80, loss = 0.1083\n",
      "epoch 26 / 100, step 25/80, loss = 0.1068\n",
      "epoch 26 / 100, step 26/80, loss = 0.1096\n",
      "epoch 26 / 100, step 27/80, loss = 0.1072\n",
      "epoch 26 / 100, step 28/80, loss = 0.1159\n",
      "epoch 26 / 100, step 29/80, loss = 0.1133\n",
      "epoch 26 / 100, step 30/80, loss = 0.1099\n",
      "epoch 26 / 100, step 31/80, loss = 0.1047\n",
      "epoch 26 / 100, step 32/80, loss = 0.1052\n",
      "epoch 26 / 100, step 33/80, loss = 0.1062\n",
      "epoch 26 / 100, step 34/80, loss = 0.1039\n",
      "epoch 26 / 100, step 35/80, loss = 0.1076\n",
      "epoch 26 / 100, step 36/80, loss = 0.1074\n",
      "epoch 26 / 100, step 37/80, loss = 0.1043\n",
      "epoch 26 / 100, step 38/80, loss = 0.1058\n",
      "epoch 26 / 100, step 39/80, loss = 0.1050\n",
      "epoch 26 / 100, step 40/80, loss = 0.1058\n",
      "epoch 26 / 100, step 41/80, loss = 0.1050\n",
      "epoch 26 / 100, step 42/80, loss = 0.1131\n",
      "epoch 26 / 100, step 43/80, loss = 0.1102\n",
      "epoch 26 / 100, step 44/80, loss = 0.1023\n",
      "epoch 26 / 100, step 45/80, loss = 0.1091\n",
      "epoch 26 / 100, step 46/80, loss = 0.1091\n",
      "epoch 26 / 100, step 47/80, loss = 0.1088\n",
      "epoch 26 / 100, step 48/80, loss = 0.1079\n",
      "epoch 26 / 100, step 49/80, loss = 0.1088\n",
      "epoch 26 / 100, step 50/80, loss = 0.1121\n",
      "epoch 26 / 100, step 51/80, loss = 0.1072\n",
      "epoch 26 / 100, step 52/80, loss = 0.1054\n",
      "epoch 26 / 100, step 53/80, loss = 0.1094\n",
      "epoch 26 / 100, step 54/80, loss = 0.1060\n",
      "epoch 26 / 100, step 55/80, loss = 0.1073\n",
      "epoch 26 / 100, step 56/80, loss = 0.1028\n",
      "epoch 26 / 100, step 57/80, loss = 0.1120\n",
      "epoch 26 / 100, step 58/80, loss = 0.1127\n",
      "epoch 26 / 100, step 59/80, loss = 0.1080\n",
      "epoch 26 / 100, step 60/80, loss = 0.1058\n",
      "epoch 26 / 100, step 61/80, loss = 0.1118\n",
      "epoch 26 / 100, step 62/80, loss = 0.1037\n",
      "epoch 26 / 100, step 63/80, loss = 0.1102\n",
      "epoch 26 / 100, step 64/80, loss = 0.1100\n",
      "epoch 26 / 100, step 65/80, loss = 0.1078\n",
      "epoch 26 / 100, step 66/80, loss = 0.1042\n",
      "epoch 26 / 100, step 67/80, loss = 0.1109\n",
      "epoch 26 / 100, step 68/80, loss = 0.1032\n",
      "epoch 26 / 100, step 69/80, loss = 0.1020\n",
      "epoch 26 / 100, step 70/80, loss = 0.1080\n",
      "epoch 26 / 100, step 71/80, loss = 0.1021\n",
      "epoch 26 / 100, step 72/80, loss = 0.1046\n",
      "epoch 26 / 100, step 73/80, loss = 0.1099\n",
      "epoch 26 / 100, step 74/80, loss = 0.1110\n",
      "epoch 26 / 100, step 75/80, loss = 0.1133\n",
      "epoch 26 / 100, step 76/80, loss = 0.1135\n",
      "epoch 26 / 100, step 77/80, loss = 0.1047\n",
      "epoch 26 / 100, step 78/80, loss = 0.1086\n",
      "epoch 26 / 100, step 79/80, loss = 0.1091\n",
      "epoch 26 / 100, step 80/80, loss = 0.1103\n",
      "epoch 27 / 100, step 1/80, loss = 0.1078\n",
      "epoch 27 / 100, step 2/80, loss = 0.1052\n",
      "epoch 27 / 100, step 3/80, loss = 0.1046\n",
      "epoch 27 / 100, step 4/80, loss = 0.1061\n",
      "epoch 27 / 100, step 5/80, loss = 0.1072\n",
      "epoch 27 / 100, step 6/80, loss = 0.1027\n",
      "epoch 27 / 100, step 7/80, loss = 0.1077\n",
      "epoch 27 / 100, step 8/80, loss = 0.1088\n",
      "epoch 27 / 100, step 9/80, loss = 0.1003\n",
      "epoch 27 / 100, step 10/80, loss = 0.1007\n",
      "epoch 27 / 100, step 11/80, loss = 0.0983\n",
      "epoch 27 / 100, step 12/80, loss = 0.1015\n",
      "epoch 27 / 100, step 13/80, loss = 0.1044\n",
      "epoch 27 / 100, step 14/80, loss = 0.1046\n",
      "epoch 27 / 100, step 15/80, loss = 0.1116\n",
      "epoch 27 / 100, step 16/80, loss = 0.0995\n",
      "epoch 27 / 100, step 17/80, loss = 0.1046\n",
      "epoch 27 / 100, step 18/80, loss = 0.1060\n",
      "epoch 27 / 100, step 19/80, loss = 0.1024\n",
      "epoch 27 / 100, step 20/80, loss = 0.1084\n",
      "epoch 27 / 100, step 21/80, loss = 0.1083\n",
      "epoch 27 / 100, step 22/80, loss = 0.1055\n",
      "epoch 27 / 100, step 23/80, loss = 0.1048\n",
      "epoch 27 / 100, step 24/80, loss = 0.1048\n",
      "epoch 27 / 100, step 25/80, loss = 0.1055\n",
      "epoch 27 / 100, step 26/80, loss = 0.1017\n",
      "epoch 27 / 100, step 27/80, loss = 0.1078\n",
      "epoch 27 / 100, step 28/80, loss = 0.1045\n",
      "epoch 27 / 100, step 29/80, loss = 0.0991\n",
      "epoch 27 / 100, step 30/80, loss = 0.1048\n",
      "epoch 27 / 100, step 31/80, loss = 0.1069\n",
      "epoch 27 / 100, step 32/80, loss = 0.1089\n",
      "epoch 27 / 100, step 33/80, loss = 0.0995\n",
      "epoch 27 / 100, step 34/80, loss = 0.1059\n",
      "epoch 27 / 100, step 35/80, loss = 0.1038\n",
      "epoch 27 / 100, step 36/80, loss = 0.1092\n",
      "epoch 27 / 100, step 37/80, loss = 0.1003\n",
      "epoch 27 / 100, step 38/80, loss = 0.1022\n",
      "epoch 27 / 100, step 39/80, loss = 0.1037\n",
      "epoch 27 / 100, step 40/80, loss = 0.1118\n",
      "epoch 27 / 100, step 41/80, loss = 0.1026\n",
      "epoch 27 / 100, step 42/80, loss = 0.0986\n",
      "epoch 27 / 100, step 43/80, loss = 0.1115\n",
      "epoch 27 / 100, step 44/80, loss = 0.1042\n",
      "epoch 27 / 100, step 45/80, loss = 0.1052\n",
      "epoch 27 / 100, step 46/80, loss = 0.1123\n",
      "epoch 27 / 100, step 47/80, loss = 0.1103\n",
      "epoch 27 / 100, step 48/80, loss = 0.1108\n",
      "epoch 27 / 100, step 49/80, loss = 0.1047\n",
      "epoch 27 / 100, step 50/80, loss = 0.1095\n",
      "epoch 27 / 100, step 51/80, loss = 0.1022\n",
      "epoch 27 / 100, step 52/80, loss = 0.1054\n",
      "epoch 27 / 100, step 53/80, loss = 0.1017\n",
      "epoch 27 / 100, step 54/80, loss = 0.1039\n",
      "epoch 27 / 100, step 55/80, loss = 0.1010\n",
      "epoch 27 / 100, step 56/80, loss = 0.1035\n",
      "epoch 27 / 100, step 57/80, loss = 0.1058\n",
      "epoch 27 / 100, step 58/80, loss = 0.1072\n",
      "epoch 27 / 100, step 59/80, loss = 0.0972\n",
      "epoch 27 / 100, step 60/80, loss = 0.1020\n",
      "epoch 27 / 100, step 61/80, loss = 0.0989\n",
      "epoch 27 / 100, step 62/80, loss = 0.1053\n",
      "epoch 27 / 100, step 63/80, loss = 0.1053\n",
      "epoch 27 / 100, step 64/80, loss = 0.1004\n",
      "epoch 27 / 100, step 65/80, loss = 0.0993\n",
      "epoch 27 / 100, step 66/80, loss = 0.1073\n",
      "epoch 27 / 100, step 67/80, loss = 0.1052\n",
      "epoch 27 / 100, step 68/80, loss = 0.1138\n",
      "epoch 27 / 100, step 69/80, loss = 0.1035\n",
      "epoch 27 / 100, step 70/80, loss = 0.1073\n",
      "epoch 27 / 100, step 71/80, loss = 0.1019\n",
      "epoch 27 / 100, step 72/80, loss = 0.1006\n",
      "epoch 27 / 100, step 73/80, loss = 0.1050\n",
      "epoch 27 / 100, step 74/80, loss = 0.1010\n",
      "epoch 27 / 100, step 75/80, loss = 0.0972\n",
      "epoch 27 / 100, step 76/80, loss = 0.1011\n",
      "epoch 27 / 100, step 77/80, loss = 0.1037\n",
      "epoch 27 / 100, step 78/80, loss = 0.1001\n",
      "epoch 27 / 100, step 79/80, loss = 0.0987\n",
      "epoch 27 / 100, step 80/80, loss = 0.1109\n",
      "epoch 28 / 100, step 1/80, loss = 0.0970\n",
      "epoch 28 / 100, step 2/80, loss = 0.1071\n",
      "epoch 28 / 100, step 3/80, loss = 0.0988\n",
      "epoch 28 / 100, step 4/80, loss = 0.1010\n",
      "epoch 28 / 100, step 5/80, loss = 0.1004\n",
      "epoch 28 / 100, step 6/80, loss = 0.0966\n",
      "epoch 28 / 100, step 7/80, loss = 0.0990\n",
      "epoch 28 / 100, step 8/80, loss = 0.1036\n",
      "epoch 28 / 100, step 9/80, loss = 0.1062\n",
      "epoch 28 / 100, step 10/80, loss = 0.1006\n",
      "epoch 28 / 100, step 11/80, loss = 0.0980\n",
      "epoch 28 / 100, step 12/80, loss = 0.1031\n",
      "epoch 28 / 100, step 13/80, loss = 0.0997\n",
      "epoch 28 / 100, step 14/80, loss = 0.0990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 / 100, step 15/80, loss = 0.1042\n",
      "epoch 28 / 100, step 16/80, loss = 0.1031\n",
      "epoch 28 / 100, step 17/80, loss = 0.0983\n",
      "epoch 28 / 100, step 18/80, loss = 0.0959\n",
      "epoch 28 / 100, step 19/80, loss = 0.0985\n",
      "epoch 28 / 100, step 20/80, loss = 0.1022\n",
      "epoch 28 / 100, step 21/80, loss = 0.1033\n",
      "epoch 28 / 100, step 22/80, loss = 0.0970\n",
      "epoch 28 / 100, step 23/80, loss = 0.0966\n",
      "epoch 28 / 100, step 24/80, loss = 0.1021\n",
      "epoch 28 / 100, step 25/80, loss = 0.0967\n",
      "epoch 28 / 100, step 26/80, loss = 0.0943\n",
      "epoch 28 / 100, step 27/80, loss = 0.1027\n",
      "epoch 28 / 100, step 28/80, loss = 0.0986\n",
      "epoch 28 / 100, step 29/80, loss = 0.1047\n",
      "epoch 28 / 100, step 30/80, loss = 0.0996\n",
      "epoch 28 / 100, step 31/80, loss = 0.1007\n",
      "epoch 28 / 100, step 32/80, loss = 0.1014\n",
      "epoch 28 / 100, step 33/80, loss = 0.0958\n",
      "epoch 28 / 100, step 34/80, loss = 0.1010\n",
      "epoch 28 / 100, step 35/80, loss = 0.1001\n",
      "epoch 28 / 100, step 36/80, loss = 0.1034\n",
      "epoch 28 / 100, step 37/80, loss = 0.0942\n",
      "epoch 28 / 100, step 38/80, loss = 0.0981\n",
      "epoch 28 / 100, step 39/80, loss = 0.0994\n",
      "epoch 28 / 100, step 40/80, loss = 0.1003\n",
      "epoch 28 / 100, step 41/80, loss = 0.0970\n",
      "epoch 28 / 100, step 42/80, loss = 0.1082\n",
      "epoch 28 / 100, step 43/80, loss = 0.1034\n",
      "epoch 28 / 100, step 44/80, loss = 0.1030\n",
      "epoch 28 / 100, step 45/80, loss = 0.0994\n",
      "epoch 28 / 100, step 46/80, loss = 0.1046\n",
      "epoch 28 / 100, step 47/80, loss = 0.0929\n",
      "epoch 28 / 100, step 48/80, loss = 0.0974\n",
      "epoch 28 / 100, step 49/80, loss = 0.0972\n",
      "epoch 28 / 100, step 50/80, loss = 0.1000\n",
      "epoch 28 / 100, step 51/80, loss = 0.0998\n",
      "epoch 28 / 100, step 52/80, loss = 0.0978\n",
      "epoch 28 / 100, step 53/80, loss = 0.1005\n",
      "epoch 28 / 100, step 54/80, loss = 0.1084\n",
      "epoch 28 / 100, step 55/80, loss = 0.0984\n",
      "epoch 28 / 100, step 56/80, loss = 0.1015\n",
      "epoch 28 / 100, step 57/80, loss = 0.1041\n",
      "epoch 28 / 100, step 58/80, loss = 0.0989\n",
      "epoch 28 / 100, step 59/80, loss = 0.0950\n",
      "epoch 28 / 100, step 60/80, loss = 0.1036\n",
      "epoch 28 / 100, step 61/80, loss = 0.1006\n",
      "epoch 28 / 100, step 62/80, loss = 0.1116\n",
      "epoch 28 / 100, step 63/80, loss = 0.1136\n",
      "epoch 28 / 100, step 64/80, loss = 0.0999\n",
      "epoch 28 / 100, step 65/80, loss = 0.1003\n",
      "epoch 28 / 100, step 66/80, loss = 0.1004\n",
      "epoch 28 / 100, step 67/80, loss = 0.1027\n",
      "epoch 28 / 100, step 68/80, loss = 0.0950\n",
      "epoch 28 / 100, step 69/80, loss = 0.0994\n",
      "epoch 28 / 100, step 70/80, loss = 0.1011\n",
      "epoch 28 / 100, step 71/80, loss = 0.0971\n",
      "epoch 28 / 100, step 72/80, loss = 0.1022\n",
      "epoch 28 / 100, step 73/80, loss = 0.1043\n",
      "epoch 28 / 100, step 74/80, loss = 0.1021\n",
      "epoch 28 / 100, step 75/80, loss = 0.0954\n",
      "epoch 28 / 100, step 76/80, loss = 0.1078\n",
      "epoch 28 / 100, step 77/80, loss = 0.0982\n",
      "epoch 28 / 100, step 78/80, loss = 0.0956\n",
      "epoch 28 / 100, step 79/80, loss = 0.1021\n",
      "epoch 28 / 100, step 80/80, loss = 0.1024\n",
      "epoch 29 / 100, step 1/80, loss = 0.1105\n",
      "epoch 29 / 100, step 2/80, loss = 0.0934\n",
      "epoch 29 / 100, step 3/80, loss = 0.0943\n",
      "epoch 29 / 100, step 4/80, loss = 0.1057\n",
      "epoch 29 / 100, step 5/80, loss = 0.0974\n",
      "epoch 29 / 100, step 6/80, loss = 0.0947\n",
      "epoch 29 / 100, step 7/80, loss = 0.0976\n",
      "epoch 29 / 100, step 8/80, loss = 0.0998\n",
      "epoch 29 / 100, step 9/80, loss = 0.1001\n",
      "epoch 29 / 100, step 10/80, loss = 0.0995\n",
      "epoch 29 / 100, step 11/80, loss = 0.0924\n",
      "epoch 29 / 100, step 12/80, loss = 0.0973\n",
      "epoch 29 / 100, step 13/80, loss = 0.0968\n",
      "epoch 29 / 100, step 14/80, loss = 0.0933\n",
      "epoch 29 / 100, step 15/80, loss = 0.0954\n",
      "epoch 29 / 100, step 16/80, loss = 0.0900\n",
      "epoch 29 / 100, step 17/80, loss = 0.0975\n",
      "epoch 29 / 100, step 18/80, loss = 0.0968\n",
      "epoch 29 / 100, step 19/80, loss = 0.0985\n",
      "epoch 29 / 100, step 20/80, loss = 0.0993\n",
      "epoch 29 / 100, step 21/80, loss = 0.0976\n",
      "epoch 29 / 100, step 22/80, loss = 0.0974\n",
      "epoch 29 / 100, step 23/80, loss = 0.0954\n",
      "epoch 29 / 100, step 24/80, loss = 0.0977\n",
      "epoch 29 / 100, step 25/80, loss = 0.1037\n",
      "epoch 29 / 100, step 26/80, loss = 0.1003\n",
      "epoch 29 / 100, step 27/80, loss = 0.0953\n",
      "epoch 29 / 100, step 28/80, loss = 0.0984\n",
      "epoch 29 / 100, step 29/80, loss = 0.1023\n",
      "epoch 29 / 100, step 30/80, loss = 0.0975\n",
      "epoch 29 / 100, step 31/80, loss = 0.1022\n",
      "epoch 29 / 100, step 32/80, loss = 0.0947\n",
      "epoch 29 / 100, step 33/80, loss = 0.0910\n",
      "epoch 29 / 100, step 34/80, loss = 0.0970\n",
      "epoch 29 / 100, step 35/80, loss = 0.0968\n",
      "epoch 29 / 100, step 36/80, loss = 0.0953\n",
      "epoch 29 / 100, step 37/80, loss = 0.0997\n",
      "epoch 29 / 100, step 38/80, loss = 0.0957\n",
      "epoch 29 / 100, step 39/80, loss = 0.0968\n",
      "epoch 29 / 100, step 40/80, loss = 0.0941\n",
      "epoch 29 / 100, step 41/80, loss = 0.0956\n",
      "epoch 29 / 100, step 42/80, loss = 0.0994\n",
      "epoch 29 / 100, step 43/80, loss = 0.0955\n",
      "epoch 29 / 100, step 44/80, loss = 0.0921\n",
      "epoch 29 / 100, step 45/80, loss = 0.0995\n",
      "epoch 29 / 100, step 46/80, loss = 0.0956\n",
      "epoch 29 / 100, step 47/80, loss = 0.1009\n",
      "epoch 29 / 100, step 48/80, loss = 0.1001\n",
      "epoch 29 / 100, step 49/80, loss = 0.0946\n",
      "epoch 29 / 100, step 50/80, loss = 0.0981\n",
      "epoch 29 / 100, step 51/80, loss = 0.0964\n",
      "epoch 29 / 100, step 52/80, loss = 0.0902\n",
      "epoch 29 / 100, step 53/80, loss = 0.0956\n",
      "epoch 29 / 100, step 54/80, loss = 0.0930\n",
      "epoch 29 / 100, step 55/80, loss = 0.0990\n",
      "epoch 29 / 100, step 56/80, loss = 0.0956\n",
      "epoch 29 / 100, step 57/80, loss = 0.0965\n",
      "epoch 29 / 100, step 58/80, loss = 0.0938\n",
      "epoch 29 / 100, step 59/80, loss = 0.0931\n",
      "epoch 29 / 100, step 60/80, loss = 0.0912\n",
      "epoch 29 / 100, step 61/80, loss = 0.0967\n",
      "epoch 29 / 100, step 62/80, loss = 0.0980\n",
      "epoch 29 / 100, step 63/80, loss = 0.1075\n",
      "epoch 29 / 100, step 64/80, loss = 0.0961\n",
      "epoch 29 / 100, step 65/80, loss = 0.0915\n",
      "epoch 29 / 100, step 66/80, loss = 0.1028\n",
      "epoch 29 / 100, step 67/80, loss = 0.0945\n",
      "epoch 29 / 100, step 68/80, loss = 0.0974\n",
      "epoch 29 / 100, step 69/80, loss = 0.1042\n",
      "epoch 29 / 100, step 70/80, loss = 0.0981\n",
      "epoch 29 / 100, step 71/80, loss = 0.0911\n",
      "epoch 29 / 100, step 72/80, loss = 0.0934\n",
      "epoch 29 / 100, step 73/80, loss = 0.0968\n",
      "epoch 29 / 100, step 74/80, loss = 0.0951\n",
      "epoch 29 / 100, step 75/80, loss = 0.0978\n",
      "epoch 29 / 100, step 76/80, loss = 0.0966\n",
      "epoch 29 / 100, step 77/80, loss = 0.0987\n",
      "epoch 29 / 100, step 78/80, loss = 0.0991\n",
      "epoch 29 / 100, step 79/80, loss = 0.0982\n",
      "epoch 29 / 100, step 80/80, loss = 0.0980\n",
      "epoch 30 / 100, step 1/80, loss = 0.0934\n",
      "epoch 30 / 100, step 2/80, loss = 0.0942\n",
      "epoch 30 / 100, step 3/80, loss = 0.0877\n",
      "epoch 30 / 100, step 4/80, loss = 0.0937\n",
      "epoch 30 / 100, step 5/80, loss = 0.0882\n",
      "epoch 30 / 100, step 6/80, loss = 0.0990\n",
      "epoch 30 / 100, step 7/80, loss = 0.0923\n",
      "epoch 30 / 100, step 8/80, loss = 0.0958\n",
      "epoch 30 / 100, step 9/80, loss = 0.1004\n",
      "epoch 30 / 100, step 10/80, loss = 0.1015\n",
      "epoch 30 / 100, step 11/80, loss = 0.0961\n",
      "epoch 30 / 100, step 12/80, loss = 0.0918\n",
      "epoch 30 / 100, step 13/80, loss = 0.0938\n",
      "epoch 30 / 100, step 14/80, loss = 0.1013\n",
      "epoch 30 / 100, step 15/80, loss = 0.0925\n",
      "epoch 30 / 100, step 16/80, loss = 0.0931\n",
      "epoch 30 / 100, step 17/80, loss = 0.0903\n",
      "epoch 30 / 100, step 18/80, loss = 0.0947\n",
      "epoch 30 / 100, step 19/80, loss = 0.0878\n",
      "epoch 30 / 100, step 20/80, loss = 0.0919\n",
      "epoch 30 / 100, step 21/80, loss = 0.0954\n",
      "epoch 30 / 100, step 22/80, loss = 0.0937\n",
      "epoch 30 / 100, step 23/80, loss = 0.0946\n",
      "epoch 30 / 100, step 24/80, loss = 0.0960\n",
      "epoch 30 / 100, step 25/80, loss = 0.0936\n",
      "epoch 30 / 100, step 26/80, loss = 0.0893\n",
      "epoch 30 / 100, step 27/80, loss = 0.0887\n",
      "epoch 30 / 100, step 28/80, loss = 0.0918\n",
      "epoch 30 / 100, step 29/80, loss = 0.0950\n",
      "epoch 30 / 100, step 30/80, loss = 0.0876\n",
      "epoch 30 / 100, step 31/80, loss = 0.0972\n",
      "epoch 30 / 100, step 32/80, loss = 0.0917\n",
      "epoch 30 / 100, step 33/80, loss = 0.0951\n",
      "epoch 30 / 100, step 34/80, loss = 0.0900\n",
      "epoch 30 / 100, step 35/80, loss = 0.0899\n",
      "epoch 30 / 100, step 36/80, loss = 0.0931\n",
      "epoch 30 / 100, step 37/80, loss = 0.0911\n",
      "epoch 30 / 100, step 38/80, loss = 0.0914\n",
      "epoch 30 / 100, step 39/80, loss = 0.0879\n",
      "epoch 30 / 100, step 40/80, loss = 0.0901\n",
      "epoch 30 / 100, step 41/80, loss = 0.0908\n",
      "epoch 30 / 100, step 42/80, loss = 0.0949\n",
      "epoch 30 / 100, step 43/80, loss = 0.0952\n",
      "epoch 30 / 100, step 44/80, loss = 0.0948\n",
      "epoch 30 / 100, step 45/80, loss = 0.0903\n",
      "epoch 30 / 100, step 46/80, loss = 0.0945\n",
      "epoch 30 / 100, step 47/80, loss = 0.0877\n",
      "epoch 30 / 100, step 48/80, loss = 0.0907\n",
      "epoch 30 / 100, step 49/80, loss = 0.0915\n",
      "epoch 30 / 100, step 50/80, loss = 0.0906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 / 100, step 51/80, loss = 0.0936\n",
      "epoch 30 / 100, step 52/80, loss = 0.0931\n",
      "epoch 30 / 100, step 53/80, loss = 0.0937\n",
      "epoch 30 / 100, step 54/80, loss = 0.0920\n",
      "epoch 30 / 100, step 55/80, loss = 0.0971\n",
      "epoch 30 / 100, step 56/80, loss = 0.0991\n",
      "epoch 30 / 100, step 57/80, loss = 0.0940\n",
      "epoch 30 / 100, step 58/80, loss = 0.0900\n",
      "epoch 30 / 100, step 59/80, loss = 0.1009\n",
      "epoch 30 / 100, step 60/80, loss = 0.0866\n",
      "epoch 30 / 100, step 61/80, loss = 0.0894\n",
      "epoch 30 / 100, step 62/80, loss = 0.0917\n",
      "epoch 30 / 100, step 63/80, loss = 0.0949\n",
      "epoch 30 / 100, step 64/80, loss = 0.0965\n",
      "epoch 30 / 100, step 65/80, loss = 0.0923\n",
      "epoch 30 / 100, step 66/80, loss = 0.0888\n",
      "epoch 30 / 100, step 67/80, loss = 0.0967\n",
      "epoch 30 / 100, step 68/80, loss = 0.0944\n",
      "epoch 30 / 100, step 69/80, loss = 0.0916\n",
      "epoch 30 / 100, step 70/80, loss = 0.0870\n",
      "epoch 30 / 100, step 71/80, loss = 0.1012\n",
      "epoch 30 / 100, step 72/80, loss = 0.0988\n",
      "epoch 30 / 100, step 73/80, loss = 0.0883\n",
      "epoch 30 / 100, step 74/80, loss = 0.1059\n",
      "epoch 30 / 100, step 75/80, loss = 0.0909\n",
      "epoch 30 / 100, step 76/80, loss = 0.0888\n",
      "epoch 30 / 100, step 77/80, loss = 0.0926\n",
      "epoch 30 / 100, step 78/80, loss = 0.0917\n",
      "epoch 30 / 100, step 79/80, loss = 0.0906\n",
      "epoch 30 / 100, step 80/80, loss = 0.0926\n",
      "epoch 31 / 100, step 1/80, loss = 0.0891\n",
      "epoch 31 / 100, step 2/80, loss = 0.0882\n",
      "epoch 31 / 100, step 3/80, loss = 0.0939\n",
      "epoch 31 / 100, step 4/80, loss = 0.0920\n",
      "epoch 31 / 100, step 5/80, loss = 0.0900\n",
      "epoch 31 / 100, step 6/80, loss = 0.0934\n",
      "epoch 31 / 100, step 7/80, loss = 0.0966\n",
      "epoch 31 / 100, step 8/80, loss = 0.0906\n",
      "epoch 31 / 100, step 9/80, loss = 0.0837\n",
      "epoch 31 / 100, step 10/80, loss = 0.0904\n",
      "epoch 31 / 100, step 11/80, loss = 0.0867\n",
      "epoch 31 / 100, step 12/80, loss = 0.0968\n",
      "epoch 31 / 100, step 13/80, loss = 0.0859\n",
      "epoch 31 / 100, step 14/80, loss = 0.0857\n",
      "epoch 31 / 100, step 15/80, loss = 0.0882\n",
      "epoch 31 / 100, step 16/80, loss = 0.0901\n",
      "epoch 31 / 100, step 17/80, loss = 0.0885\n",
      "epoch 31 / 100, step 18/80, loss = 0.0870\n",
      "epoch 31 / 100, step 19/80, loss = 0.0855\n",
      "epoch 31 / 100, step 20/80, loss = 0.0867\n",
      "epoch 31 / 100, step 21/80, loss = 0.0956\n",
      "epoch 31 / 100, step 22/80, loss = 0.0900\n",
      "epoch 31 / 100, step 23/80, loss = 0.0855\n",
      "epoch 31 / 100, step 24/80, loss = 0.0878\n",
      "epoch 31 / 100, step 25/80, loss = 0.0995\n",
      "epoch 31 / 100, step 26/80, loss = 0.0896\n",
      "epoch 31 / 100, step 27/80, loss = 0.0936\n",
      "epoch 31 / 100, step 28/80, loss = 0.0856\n",
      "epoch 31 / 100, step 29/80, loss = 0.0876\n",
      "epoch 31 / 100, step 30/80, loss = 0.0863\n",
      "epoch 31 / 100, step 31/80, loss = 0.0891\n",
      "epoch 31 / 100, step 32/80, loss = 0.0889\n",
      "epoch 31 / 100, step 33/80, loss = 0.0920\n",
      "epoch 31 / 100, step 34/80, loss = 0.0972\n",
      "epoch 31 / 100, step 35/80, loss = 0.0896\n",
      "epoch 31 / 100, step 36/80, loss = 0.0875\n",
      "epoch 31 / 100, step 37/80, loss = 0.0867\n",
      "epoch 31 / 100, step 38/80, loss = 0.0886\n",
      "epoch 31 / 100, step 39/80, loss = 0.0922\n",
      "epoch 31 / 100, step 40/80, loss = 0.0918\n",
      "epoch 31 / 100, step 41/80, loss = 0.0873\n",
      "epoch 31 / 100, step 42/80, loss = 0.0889\n",
      "epoch 31 / 100, step 43/80, loss = 0.0875\n",
      "epoch 31 / 100, step 44/80, loss = 0.0934\n",
      "epoch 31 / 100, step 45/80, loss = 0.0878\n",
      "epoch 31 / 100, step 46/80, loss = 0.0906\n",
      "epoch 31 / 100, step 47/80, loss = 0.0957\n",
      "epoch 31 / 100, step 48/80, loss = 0.0883\n",
      "epoch 31 / 100, step 49/80, loss = 0.0891\n",
      "epoch 31 / 100, step 50/80, loss = 0.0895\n",
      "epoch 31 / 100, step 51/80, loss = 0.0898\n",
      "epoch 31 / 100, step 52/80, loss = 0.0903\n",
      "epoch 31 / 100, step 53/80, loss = 0.0885\n",
      "epoch 31 / 100, step 54/80, loss = 0.0921\n",
      "epoch 31 / 100, step 55/80, loss = 0.0852\n",
      "epoch 31 / 100, step 56/80, loss = 0.0921\n",
      "epoch 31 / 100, step 57/80, loss = 0.0890\n",
      "epoch 31 / 100, step 58/80, loss = 0.0935\n",
      "epoch 31 / 100, step 59/80, loss = 0.0857\n",
      "epoch 31 / 100, step 60/80, loss = 0.0869\n",
      "epoch 31 / 100, step 61/80, loss = 0.0872\n",
      "epoch 31 / 100, step 62/80, loss = 0.0938\n",
      "epoch 31 / 100, step 63/80, loss = 0.0885\n",
      "epoch 31 / 100, step 64/80, loss = 0.0885\n",
      "epoch 31 / 100, step 65/80, loss = 0.0910\n",
      "epoch 31 / 100, step 66/80, loss = 0.0902\n",
      "epoch 31 / 100, step 67/80, loss = 0.0897\n",
      "epoch 31 / 100, step 68/80, loss = 0.0903\n",
      "epoch 31 / 100, step 69/80, loss = 0.0969\n",
      "epoch 31 / 100, step 70/80, loss = 0.0911\n",
      "epoch 31 / 100, step 71/80, loss = 0.0859\n",
      "epoch 31 / 100, step 72/80, loss = 0.0911\n",
      "epoch 31 / 100, step 73/80, loss = 0.0949\n",
      "epoch 31 / 100, step 74/80, loss = 0.0922\n",
      "epoch 31 / 100, step 75/80, loss = 0.0915\n",
      "epoch 31 / 100, step 76/80, loss = 0.0842\n",
      "epoch 31 / 100, step 77/80, loss = 0.0888\n",
      "epoch 31 / 100, step 78/80, loss = 0.0849\n",
      "epoch 31 / 100, step 79/80, loss = 0.0859\n",
      "epoch 31 / 100, step 80/80, loss = 0.0935\n",
      "epoch 32 / 100, step 1/80, loss = 0.0860\n",
      "epoch 32 / 100, step 2/80, loss = 0.0858\n",
      "epoch 32 / 100, step 3/80, loss = 0.0837\n",
      "epoch 32 / 100, step 4/80, loss = 0.0874\n",
      "epoch 32 / 100, step 5/80, loss = 0.0870\n",
      "epoch 32 / 100, step 6/80, loss = 0.0813\n",
      "epoch 32 / 100, step 7/80, loss = 0.0869\n",
      "epoch 32 / 100, step 8/80, loss = 0.0919\n",
      "epoch 32 / 100, step 9/80, loss = 0.0877\n",
      "epoch 32 / 100, step 10/80, loss = 0.0845\n",
      "epoch 32 / 100, step 11/80, loss = 0.0922\n",
      "epoch 32 / 100, step 12/80, loss = 0.0846\n",
      "epoch 32 / 100, step 13/80, loss = 0.0851\n",
      "epoch 32 / 100, step 14/80, loss = 0.0819\n",
      "epoch 32 / 100, step 15/80, loss = 0.0865\n",
      "epoch 32 / 100, step 16/80, loss = 0.0854\n",
      "epoch 32 / 100, step 17/80, loss = 0.0850\n",
      "epoch 32 / 100, step 18/80, loss = 0.0853\n",
      "epoch 32 / 100, step 19/80, loss = 0.0856\n",
      "epoch 32 / 100, step 20/80, loss = 0.0940\n",
      "epoch 32 / 100, step 21/80, loss = 0.0846\n",
      "epoch 32 / 100, step 22/80, loss = 0.0893\n",
      "epoch 32 / 100, step 23/80, loss = 0.0861\n",
      "epoch 32 / 100, step 24/80, loss = 0.0826\n",
      "epoch 32 / 100, step 25/80, loss = 0.0860\n",
      "epoch 32 / 100, step 26/80, loss = 0.0839\n",
      "epoch 32 / 100, step 27/80, loss = 0.0882\n",
      "epoch 32 / 100, step 28/80, loss = 0.0872\n",
      "epoch 32 / 100, step 29/80, loss = 0.0821\n",
      "epoch 32 / 100, step 30/80, loss = 0.0860\n",
      "epoch 32 / 100, step 31/80, loss = 0.0843\n",
      "epoch 32 / 100, step 32/80, loss = 0.0875\n",
      "epoch 32 / 100, step 33/80, loss = 0.0869\n",
      "epoch 32 / 100, step 34/80, loss = 0.0861\n",
      "epoch 32 / 100, step 35/80, loss = 0.0923\n",
      "epoch 32 / 100, step 36/80, loss = 0.0865\n",
      "epoch 32 / 100, step 37/80, loss = 0.0874\n",
      "epoch 32 / 100, step 38/80, loss = 0.0875\n",
      "epoch 32 / 100, step 39/80, loss = 0.0857\n",
      "epoch 32 / 100, step 40/80, loss = 0.0854\n",
      "epoch 32 / 100, step 41/80, loss = 0.0945\n",
      "epoch 32 / 100, step 42/80, loss = 0.0852\n",
      "epoch 32 / 100, step 43/80, loss = 0.0904\n",
      "epoch 32 / 100, step 44/80, loss = 0.0867\n",
      "epoch 32 / 100, step 45/80, loss = 0.0869\n",
      "epoch 32 / 100, step 46/80, loss = 0.0896\n",
      "epoch 32 / 100, step 47/80, loss = 0.0863\n",
      "epoch 32 / 100, step 48/80, loss = 0.0858\n",
      "epoch 32 / 100, step 49/80, loss = 0.0855\n",
      "epoch 32 / 100, step 50/80, loss = 0.0895\n",
      "epoch 32 / 100, step 51/80, loss = 0.0826\n",
      "epoch 32 / 100, step 52/80, loss = 0.0884\n",
      "epoch 32 / 100, step 53/80, loss = 0.0857\n",
      "epoch 32 / 100, step 54/80, loss = 0.0857\n",
      "epoch 32 / 100, step 55/80, loss = 0.0795\n",
      "epoch 32 / 100, step 56/80, loss = 0.0845\n",
      "epoch 32 / 100, step 57/80, loss = 0.0876\n",
      "epoch 32 / 100, step 58/80, loss = 0.0836\n",
      "epoch 32 / 100, step 59/80, loss = 0.0784\n",
      "epoch 32 / 100, step 60/80, loss = 0.0879\n",
      "epoch 32 / 100, step 61/80, loss = 0.0933\n",
      "epoch 32 / 100, step 62/80, loss = 0.0831\n",
      "epoch 32 / 100, step 63/80, loss = 0.0848\n",
      "epoch 32 / 100, step 64/80, loss = 0.0855\n",
      "epoch 32 / 100, step 65/80, loss = 0.0842\n",
      "epoch 32 / 100, step 66/80, loss = 0.0840\n",
      "epoch 32 / 100, step 67/80, loss = 0.0834\n",
      "epoch 32 / 100, step 68/80, loss = 0.0890\n",
      "epoch 32 / 100, step 69/80, loss = 0.0919\n",
      "epoch 32 / 100, step 70/80, loss = 0.0849\n",
      "epoch 32 / 100, step 71/80, loss = 0.0877\n",
      "epoch 32 / 100, step 72/80, loss = 0.0912\n",
      "epoch 32 / 100, step 73/80, loss = 0.0874\n",
      "epoch 32 / 100, step 74/80, loss = 0.0874\n",
      "epoch 32 / 100, step 75/80, loss = 0.0903\n",
      "epoch 32 / 100, step 76/80, loss = 0.0889\n",
      "epoch 32 / 100, step 77/80, loss = 0.0868\n",
      "epoch 32 / 100, step 78/80, loss = 0.0867\n",
      "epoch 32 / 100, step 79/80, loss = 0.0876\n",
      "epoch 32 / 100, step 80/80, loss = 0.0825\n",
      "epoch 33 / 100, step 1/80, loss = 0.0850\n",
      "epoch 33 / 100, step 2/80, loss = 0.0858\n",
      "epoch 33 / 100, step 3/80, loss = 0.0870\n",
      "epoch 33 / 100, step 4/80, loss = 0.0855\n",
      "epoch 33 / 100, step 5/80, loss = 0.0823\n",
      "epoch 33 / 100, step 6/80, loss = 0.0856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 / 100, step 7/80, loss = 0.0838\n",
      "epoch 33 / 100, step 8/80, loss = 0.0793\n",
      "epoch 33 / 100, step 9/80, loss = 0.0915\n",
      "epoch 33 / 100, step 10/80, loss = 0.0824\n",
      "epoch 33 / 100, step 11/80, loss = 0.0845\n",
      "epoch 33 / 100, step 12/80, loss = 0.0854\n",
      "epoch 33 / 100, step 13/80, loss = 0.0838\n",
      "epoch 33 / 100, step 14/80, loss = 0.0860\n",
      "epoch 33 / 100, step 15/80, loss = 0.0839\n",
      "epoch 33 / 100, step 16/80, loss = 0.0826\n",
      "epoch 33 / 100, step 17/80, loss = 0.0850\n",
      "epoch 33 / 100, step 18/80, loss = 0.0772\n",
      "epoch 33 / 100, step 19/80, loss = 0.0866\n",
      "epoch 33 / 100, step 20/80, loss = 0.0852\n",
      "epoch 33 / 100, step 21/80, loss = 0.0799\n",
      "epoch 33 / 100, step 22/80, loss = 0.0818\n",
      "epoch 33 / 100, step 23/80, loss = 0.0846\n",
      "epoch 33 / 100, step 24/80, loss = 0.0821\n",
      "epoch 33 / 100, step 25/80, loss = 0.0784\n",
      "epoch 33 / 100, step 26/80, loss = 0.0820\n",
      "epoch 33 / 100, step 27/80, loss = 0.0907\n",
      "epoch 33 / 100, step 28/80, loss = 0.0799\n",
      "epoch 33 / 100, step 29/80, loss = 0.0789\n",
      "epoch 33 / 100, step 30/80, loss = 0.0953\n",
      "epoch 33 / 100, step 31/80, loss = 0.0800\n",
      "epoch 33 / 100, step 32/80, loss = 0.0839\n",
      "epoch 33 / 100, step 33/80, loss = 0.0851\n",
      "epoch 33 / 100, step 34/80, loss = 0.0878\n",
      "epoch 33 / 100, step 35/80, loss = 0.0819\n",
      "epoch 33 / 100, step 36/80, loss = 0.0823\n",
      "epoch 33 / 100, step 37/80, loss = 0.0820\n",
      "epoch 33 / 100, step 38/80, loss = 0.0839\n",
      "epoch 33 / 100, step 39/80, loss = 0.0832\n",
      "epoch 33 / 100, step 40/80, loss = 0.0894\n",
      "epoch 33 / 100, step 41/80, loss = 0.0891\n",
      "epoch 33 / 100, step 42/80, loss = 0.0790\n",
      "epoch 33 / 100, step 43/80, loss = 0.0794\n",
      "epoch 33 / 100, step 44/80, loss = 0.0896\n",
      "epoch 33 / 100, step 45/80, loss = 0.0825\n",
      "epoch 33 / 100, step 46/80, loss = 0.0919\n",
      "epoch 33 / 100, step 47/80, loss = 0.0838\n",
      "epoch 33 / 100, step 48/80, loss = 0.0788\n",
      "epoch 33 / 100, step 49/80, loss = 0.0850\n",
      "epoch 33 / 100, step 50/80, loss = 0.0807\n",
      "epoch 33 / 100, step 51/80, loss = 0.0813\n",
      "epoch 33 / 100, step 52/80, loss = 0.0870\n",
      "epoch 33 / 100, step 53/80, loss = 0.0850\n",
      "epoch 33 / 100, step 54/80, loss = 0.0817\n",
      "epoch 33 / 100, step 55/80, loss = 0.0818\n",
      "epoch 33 / 100, step 56/80, loss = 0.0828\n",
      "epoch 33 / 100, step 57/80, loss = 0.0824\n",
      "epoch 33 / 100, step 58/80, loss = 0.0833\n",
      "epoch 33 / 100, step 59/80, loss = 0.0837\n",
      "epoch 33 / 100, step 60/80, loss = 0.0783\n",
      "epoch 33 / 100, step 61/80, loss = 0.0813\n",
      "epoch 33 / 100, step 62/80, loss = 0.0866\n",
      "epoch 33 / 100, step 63/80, loss = 0.0886\n",
      "epoch 33 / 100, step 64/80, loss = 0.0809\n",
      "epoch 33 / 100, step 65/80, loss = 0.0892\n",
      "epoch 33 / 100, step 66/80, loss = 0.0845\n",
      "epoch 33 / 100, step 67/80, loss = 0.0810\n",
      "epoch 33 / 100, step 68/80, loss = 0.0824\n",
      "epoch 33 / 100, step 69/80, loss = 0.0850\n",
      "epoch 33 / 100, step 70/80, loss = 0.0827\n",
      "epoch 33 / 100, step 71/80, loss = 0.0815\n",
      "epoch 33 / 100, step 72/80, loss = 0.0895\n",
      "epoch 33 / 100, step 73/80, loss = 0.0846\n",
      "epoch 33 / 100, step 74/80, loss = 0.0847\n",
      "epoch 33 / 100, step 75/80, loss = 0.0932\n",
      "epoch 33 / 100, step 76/80, loss = 0.0820\n",
      "epoch 33 / 100, step 77/80, loss = 0.0811\n",
      "epoch 33 / 100, step 78/80, loss = 0.0832\n",
      "epoch 33 / 100, step 79/80, loss = 0.0825\n",
      "epoch 33 / 100, step 80/80, loss = 0.0833\n",
      "epoch 34 / 100, step 1/80, loss = 0.0806\n",
      "epoch 34 / 100, step 2/80, loss = 0.0754\n",
      "epoch 34 / 100, step 3/80, loss = 0.0874\n",
      "epoch 34 / 100, step 4/80, loss = 0.0823\n",
      "epoch 34 / 100, step 5/80, loss = 0.0769\n",
      "epoch 34 / 100, step 6/80, loss = 0.0868\n",
      "epoch 34 / 100, step 7/80, loss = 0.0830\n",
      "epoch 34 / 100, step 8/80, loss = 0.0782\n",
      "epoch 34 / 100, step 9/80, loss = 0.0863\n",
      "epoch 34 / 100, step 10/80, loss = 0.0824\n",
      "epoch 34 / 100, step 11/80, loss = 0.0869\n",
      "epoch 34 / 100, step 12/80, loss = 0.0847\n",
      "epoch 34 / 100, step 13/80, loss = 0.0853\n",
      "epoch 34 / 100, step 14/80, loss = 0.0812\n",
      "epoch 34 / 100, step 15/80, loss = 0.0818\n",
      "epoch 34 / 100, step 16/80, loss = 0.0803\n",
      "epoch 34 / 100, step 17/80, loss = 0.0827\n",
      "epoch 34 / 100, step 18/80, loss = 0.0873\n",
      "epoch 34 / 100, step 19/80, loss = 0.0801\n",
      "epoch 34 / 100, step 20/80, loss = 0.0784\n",
      "epoch 34 / 100, step 21/80, loss = 0.0779\n",
      "epoch 34 / 100, step 22/80, loss = 0.0824\n",
      "epoch 34 / 100, step 23/80, loss = 0.0782\n",
      "epoch 34 / 100, step 24/80, loss = 0.0784\n",
      "epoch 34 / 100, step 25/80, loss = 0.0794\n",
      "epoch 34 / 100, step 26/80, loss = 0.0754\n",
      "epoch 34 / 100, step 27/80, loss = 0.0838\n",
      "epoch 34 / 100, step 28/80, loss = 0.0845\n",
      "epoch 34 / 100, step 29/80, loss = 0.0793\n",
      "epoch 34 / 100, step 30/80, loss = 0.0819\n",
      "epoch 34 / 100, step 31/80, loss = 0.0812\n",
      "epoch 34 / 100, step 32/80, loss = 0.0775\n",
      "epoch 34 / 100, step 33/80, loss = 0.0798\n",
      "epoch 34 / 100, step 34/80, loss = 0.0804\n",
      "epoch 34 / 100, step 35/80, loss = 0.0826\n",
      "epoch 34 / 100, step 36/80, loss = 0.0849\n",
      "epoch 34 / 100, step 37/80, loss = 0.0837\n",
      "epoch 34 / 100, step 38/80, loss = 0.0830\n",
      "epoch 34 / 100, step 39/80, loss = 0.0796\n",
      "epoch 34 / 100, step 40/80, loss = 0.0783\n",
      "epoch 34 / 100, step 41/80, loss = 0.0829\n",
      "epoch 34 / 100, step 42/80, loss = 0.0809\n",
      "epoch 34 / 100, step 43/80, loss = 0.0794\n",
      "epoch 34 / 100, step 44/80, loss = 0.0833\n",
      "epoch 34 / 100, step 45/80, loss = 0.0780\n",
      "epoch 34 / 100, step 46/80, loss = 0.0794\n",
      "epoch 34 / 100, step 47/80, loss = 0.0888\n",
      "epoch 34 / 100, step 48/80, loss = 0.0839\n",
      "epoch 34 / 100, step 49/80, loss = 0.0810\n",
      "epoch 34 / 100, step 50/80, loss = 0.0800\n",
      "epoch 34 / 100, step 51/80, loss = 0.0888\n",
      "epoch 34 / 100, step 52/80, loss = 0.0866\n",
      "epoch 34 / 100, step 53/80, loss = 0.0820\n",
      "epoch 34 / 100, step 54/80, loss = 0.0763\n",
      "epoch 34 / 100, step 55/80, loss = 0.0812\n",
      "epoch 34 / 100, step 56/80, loss = 0.0822\n",
      "epoch 34 / 100, step 57/80, loss = 0.0783\n",
      "epoch 34 / 100, step 58/80, loss = 0.0796\n",
      "epoch 34 / 100, step 59/80, loss = 0.0782\n",
      "epoch 34 / 100, step 60/80, loss = 0.0753\n",
      "epoch 34 / 100, step 61/80, loss = 0.0822\n",
      "epoch 34 / 100, step 62/80, loss = 0.0833\n",
      "epoch 34 / 100, step 63/80, loss = 0.0822\n",
      "epoch 34 / 100, step 64/80, loss = 0.0779\n",
      "epoch 34 / 100, step 65/80, loss = 0.0797\n",
      "epoch 34 / 100, step 66/80, loss = 0.0840\n",
      "epoch 34 / 100, step 67/80, loss = 0.0789\n",
      "epoch 34 / 100, step 68/80, loss = 0.0814\n",
      "epoch 34 / 100, step 69/80, loss = 0.0816\n",
      "epoch 34 / 100, step 70/80, loss = 0.0761\n",
      "epoch 34 / 100, step 71/80, loss = 0.0763\n",
      "epoch 34 / 100, step 72/80, loss = 0.0834\n",
      "epoch 34 / 100, step 73/80, loss = 0.0838\n",
      "epoch 34 / 100, step 74/80, loss = 0.0841\n",
      "epoch 34 / 100, step 75/80, loss = 0.0796\n",
      "epoch 34 / 100, step 76/80, loss = 0.0796\n",
      "epoch 34 / 100, step 77/80, loss = 0.0751\n",
      "epoch 34 / 100, step 78/80, loss = 0.0793\n",
      "epoch 34 / 100, step 79/80, loss = 0.0786\n",
      "epoch 34 / 100, step 80/80, loss = 0.0825\n",
      "epoch 35 / 100, step 1/80, loss = 0.0780\n",
      "epoch 35 / 100, step 2/80, loss = 0.0755\n",
      "epoch 35 / 100, step 3/80, loss = 0.0749\n",
      "epoch 35 / 100, step 4/80, loss = 0.0787\n",
      "epoch 35 / 100, step 5/80, loss = 0.0795\n",
      "epoch 35 / 100, step 6/80, loss = 0.0815\n",
      "epoch 35 / 100, step 7/80, loss = 0.0742\n",
      "epoch 35 / 100, step 8/80, loss = 0.0750\n",
      "epoch 35 / 100, step 9/80, loss = 0.0888\n",
      "epoch 35 / 100, step 10/80, loss = 0.0775\n",
      "epoch 35 / 100, step 11/80, loss = 0.0772\n",
      "epoch 35 / 100, step 12/80, loss = 0.0809\n",
      "epoch 35 / 100, step 13/80, loss = 0.0782\n",
      "epoch 35 / 100, step 14/80, loss = 0.0792\n",
      "epoch 35 / 100, step 15/80, loss = 0.0833\n",
      "epoch 35 / 100, step 16/80, loss = 0.0795\n",
      "epoch 35 / 100, step 17/80, loss = 0.0807\n",
      "epoch 35 / 100, step 18/80, loss = 0.0837\n",
      "epoch 35 / 100, step 19/80, loss = 0.0793\n",
      "epoch 35 / 100, step 20/80, loss = 0.0770\n",
      "epoch 35 / 100, step 21/80, loss = 0.0798\n",
      "epoch 35 / 100, step 22/80, loss = 0.0771\n",
      "epoch 35 / 100, step 23/80, loss = 0.0792\n",
      "epoch 35 / 100, step 24/80, loss = 0.0778\n",
      "epoch 35 / 100, step 25/80, loss = 0.0742\n",
      "epoch 35 / 100, step 26/80, loss = 0.0795\n",
      "epoch 35 / 100, step 27/80, loss = 0.0785\n",
      "epoch 35 / 100, step 28/80, loss = 0.0762\n",
      "epoch 35 / 100, step 29/80, loss = 0.0788\n",
      "epoch 35 / 100, step 30/80, loss = 0.0760\n",
      "epoch 35 / 100, step 31/80, loss = 0.0796\n",
      "epoch 35 / 100, step 32/80, loss = 0.0796\n",
      "epoch 35 / 100, step 33/80, loss = 0.0803\n",
      "epoch 35 / 100, step 34/80, loss = 0.0797\n",
      "epoch 35 / 100, step 35/80, loss = 0.0768\n",
      "epoch 35 / 100, step 36/80, loss = 0.0789\n",
      "epoch 35 / 100, step 37/80, loss = 0.0783\n",
      "epoch 35 / 100, step 38/80, loss = 0.0782\n",
      "epoch 35 / 100, step 39/80, loss = 0.0787\n",
      "epoch 35 / 100, step 40/80, loss = 0.0766\n",
      "epoch 35 / 100, step 41/80, loss = 0.0755\n",
      "epoch 35 / 100, step 42/80, loss = 0.0770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 / 100, step 43/80, loss = 0.0801\n",
      "epoch 35 / 100, step 44/80, loss = 0.0776\n",
      "epoch 35 / 100, step 45/80, loss = 0.0772\n",
      "epoch 35 / 100, step 46/80, loss = 0.0772\n",
      "epoch 35 / 100, step 47/80, loss = 0.0756\n",
      "epoch 35 / 100, step 48/80, loss = 0.0931\n",
      "epoch 35 / 100, step 49/80, loss = 0.0834\n",
      "epoch 35 / 100, step 50/80, loss = 0.0830\n",
      "epoch 35 / 100, step 51/80, loss = 0.0785\n",
      "epoch 35 / 100, step 52/80, loss = 0.0799\n",
      "epoch 35 / 100, step 53/80, loss = 0.0812\n",
      "epoch 35 / 100, step 54/80, loss = 0.0807\n",
      "epoch 35 / 100, step 55/80, loss = 0.0784\n",
      "epoch 35 / 100, step 56/80, loss = 0.0792\n",
      "epoch 35 / 100, step 57/80, loss = 0.0791\n",
      "epoch 35 / 100, step 58/80, loss = 0.0730\n",
      "epoch 35 / 100, step 59/80, loss = 0.0789\n",
      "epoch 35 / 100, step 60/80, loss = 0.0808\n",
      "epoch 35 / 100, step 61/80, loss = 0.0791\n",
      "epoch 35 / 100, step 62/80, loss = 0.0761\n",
      "epoch 35 / 100, step 63/80, loss = 0.0785\n",
      "epoch 35 / 100, step 64/80, loss = 0.0812\n",
      "epoch 35 / 100, step 65/80, loss = 0.0775\n",
      "epoch 35 / 100, step 66/80, loss = 0.0794\n",
      "epoch 35 / 100, step 67/80, loss = 0.0762\n",
      "epoch 35 / 100, step 68/80, loss = 0.0784\n",
      "epoch 35 / 100, step 69/80, loss = 0.0797\n",
      "epoch 35 / 100, step 70/80, loss = 0.0729\n",
      "epoch 35 / 100, step 71/80, loss = 0.0806\n",
      "epoch 35 / 100, step 72/80, loss = 0.0800\n",
      "epoch 35 / 100, step 73/80, loss = 0.0767\n",
      "epoch 35 / 100, step 74/80, loss = 0.0757\n",
      "epoch 35 / 100, step 75/80, loss = 0.0777\n",
      "epoch 35 / 100, step 76/80, loss = 0.0725\n",
      "epoch 35 / 100, step 77/80, loss = 0.0807\n",
      "epoch 35 / 100, step 78/80, loss = 0.0776\n",
      "epoch 35 / 100, step 79/80, loss = 0.0831\n",
      "epoch 35 / 100, step 80/80, loss = 0.0872\n",
      "epoch 36 / 100, step 1/80, loss = 0.0744\n",
      "epoch 36 / 100, step 2/80, loss = 0.0728\n",
      "epoch 36 / 100, step 3/80, loss = 0.0733\n",
      "epoch 36 / 100, step 4/80, loss = 0.0750\n",
      "epoch 36 / 100, step 5/80, loss = 0.0766\n",
      "epoch 36 / 100, step 6/80, loss = 0.0759\n",
      "epoch 36 / 100, step 7/80, loss = 0.0762\n",
      "epoch 36 / 100, step 8/80, loss = 0.0688\n",
      "epoch 36 / 100, step 9/80, loss = 0.0733\n",
      "epoch 36 / 100, step 10/80, loss = 0.0793\n",
      "epoch 36 / 100, step 11/80, loss = 0.0787\n",
      "epoch 36 / 100, step 12/80, loss = 0.0808\n",
      "epoch 36 / 100, step 13/80, loss = 0.0739\n",
      "epoch 36 / 100, step 14/80, loss = 0.0810\n",
      "epoch 36 / 100, step 15/80, loss = 0.0745\n",
      "epoch 36 / 100, step 16/80, loss = 0.0729\n",
      "epoch 36 / 100, step 17/80, loss = 0.0760\n",
      "epoch 36 / 100, step 18/80, loss = 0.0729\n",
      "epoch 36 / 100, step 19/80, loss = 0.0765\n",
      "epoch 36 / 100, step 20/80, loss = 0.0756\n",
      "epoch 36 / 100, step 21/80, loss = 0.0770\n",
      "epoch 36 / 100, step 22/80, loss = 0.0782\n",
      "epoch 36 / 100, step 23/80, loss = 0.0751\n",
      "epoch 36 / 100, step 24/80, loss = 0.0748\n",
      "epoch 36 / 100, step 25/80, loss = 0.0772\n",
      "epoch 36 / 100, step 26/80, loss = 0.0800\n",
      "epoch 36 / 100, step 27/80, loss = 0.0794\n",
      "epoch 36 / 100, step 28/80, loss = 0.0762\n",
      "epoch 36 / 100, step 29/80, loss = 0.0770\n",
      "epoch 36 / 100, step 30/80, loss = 0.0781\n",
      "epoch 36 / 100, step 31/80, loss = 0.0736\n",
      "epoch 36 / 100, step 32/80, loss = 0.0762\n",
      "epoch 36 / 100, step 33/80, loss = 0.0804\n",
      "epoch 36 / 100, step 34/80, loss = 0.0777\n",
      "epoch 36 / 100, step 35/80, loss = 0.0767\n",
      "epoch 36 / 100, step 36/80, loss = 0.0767\n",
      "epoch 36 / 100, step 37/80, loss = 0.0747\n",
      "epoch 36 / 100, step 38/80, loss = 0.0772\n",
      "epoch 36 / 100, step 39/80, loss = 0.0748\n",
      "epoch 36 / 100, step 40/80, loss = 0.0712\n",
      "epoch 36 / 100, step 41/80, loss = 0.0719\n",
      "epoch 36 / 100, step 42/80, loss = 0.0803\n",
      "epoch 36 / 100, step 43/80, loss = 0.0770\n",
      "epoch 36 / 100, step 44/80, loss = 0.0741\n",
      "epoch 36 / 100, step 45/80, loss = 0.0818\n",
      "epoch 36 / 100, step 46/80, loss = 0.0705\n",
      "epoch 36 / 100, step 47/80, loss = 0.0749\n",
      "epoch 36 / 100, step 48/80, loss = 0.0759\n",
      "epoch 36 / 100, step 49/80, loss = 0.0739\n",
      "epoch 36 / 100, step 50/80, loss = 0.0755\n",
      "epoch 36 / 100, step 51/80, loss = 0.0779\n",
      "epoch 36 / 100, step 52/80, loss = 0.0686\n",
      "epoch 36 / 100, step 53/80, loss = 0.0774\n",
      "epoch 36 / 100, step 54/80, loss = 0.0809\n",
      "epoch 36 / 100, step 55/80, loss = 0.0796\n",
      "epoch 36 / 100, step 56/80, loss = 0.0844\n",
      "epoch 36 / 100, step 57/80, loss = 0.0747\n",
      "epoch 36 / 100, step 58/80, loss = 0.0783\n",
      "epoch 36 / 100, step 59/80, loss = 0.0765\n",
      "epoch 36 / 100, step 60/80, loss = 0.0764\n",
      "epoch 36 / 100, step 61/80, loss = 0.0771\n",
      "epoch 36 / 100, step 62/80, loss = 0.0758\n",
      "epoch 36 / 100, step 63/80, loss = 0.0759\n",
      "epoch 36 / 100, step 64/80, loss = 0.0734\n",
      "epoch 36 / 100, step 65/80, loss = 0.0755\n",
      "epoch 36 / 100, step 66/80, loss = 0.0695\n",
      "epoch 36 / 100, step 67/80, loss = 0.0847\n",
      "epoch 36 / 100, step 68/80, loss = 0.0714\n",
      "epoch 36 / 100, step 69/80, loss = 0.0745\n",
      "epoch 36 / 100, step 70/80, loss = 0.0778\n",
      "epoch 36 / 100, step 71/80, loss = 0.0748\n",
      "epoch 36 / 100, step 72/80, loss = 0.0803\n",
      "epoch 36 / 100, step 73/80, loss = 0.0777\n",
      "epoch 36 / 100, step 74/80, loss = 0.0745\n",
      "epoch 36 / 100, step 75/80, loss = 0.0774\n",
      "epoch 36 / 100, step 76/80, loss = 0.0704\n",
      "epoch 36 / 100, step 77/80, loss = 0.0772\n",
      "epoch 36 / 100, step 78/80, loss = 0.0784\n",
      "epoch 36 / 100, step 79/80, loss = 0.0767\n",
      "epoch 36 / 100, step 80/80, loss = 0.0771\n",
      "epoch 37 / 100, step 1/80, loss = 0.0741\n",
      "epoch 37 / 100, step 2/80, loss = 0.0701\n",
      "epoch 37 / 100, step 3/80, loss = 0.0745\n",
      "epoch 37 / 100, step 4/80, loss = 0.0759\n",
      "epoch 37 / 100, step 5/80, loss = 0.0738\n",
      "epoch 37 / 100, step 6/80, loss = 0.0752\n",
      "epoch 37 / 100, step 7/80, loss = 0.0716\n",
      "epoch 37 / 100, step 8/80, loss = 0.0743\n",
      "epoch 37 / 100, step 9/80, loss = 0.0697\n",
      "epoch 37 / 100, step 10/80, loss = 0.0697\n",
      "epoch 37 / 100, step 11/80, loss = 0.0692\n",
      "epoch 37 / 100, step 12/80, loss = 0.0815\n",
      "epoch 37 / 100, step 13/80, loss = 0.0766\n",
      "epoch 37 / 100, step 14/80, loss = 0.0710\n",
      "epoch 37 / 100, step 15/80, loss = 0.0803\n",
      "epoch 37 / 100, step 16/80, loss = 0.0694\n",
      "epoch 37 / 100, step 17/80, loss = 0.0699\n",
      "epoch 37 / 100, step 18/80, loss = 0.0722\n",
      "epoch 37 / 100, step 19/80, loss = 0.0813\n",
      "epoch 37 / 100, step 20/80, loss = 0.0696\n",
      "epoch 37 / 100, step 21/80, loss = 0.0731\n",
      "epoch 37 / 100, step 22/80, loss = 0.0749\n",
      "epoch 37 / 100, step 23/80, loss = 0.0728\n",
      "epoch 37 / 100, step 24/80, loss = 0.0763\n",
      "epoch 37 / 100, step 25/80, loss = 0.0684\n",
      "epoch 37 / 100, step 26/80, loss = 0.0703\n",
      "epoch 37 / 100, step 27/80, loss = 0.0745\n",
      "epoch 37 / 100, step 28/80, loss = 0.0727\n",
      "epoch 37 / 100, step 29/80, loss = 0.0704\n",
      "epoch 37 / 100, step 30/80, loss = 0.0727\n",
      "epoch 37 / 100, step 31/80, loss = 0.0731\n",
      "epoch 37 / 100, step 32/80, loss = 0.0778\n",
      "epoch 37 / 100, step 33/80, loss = 0.0704\n",
      "epoch 37 / 100, step 34/80, loss = 0.0768\n",
      "epoch 37 / 100, step 35/80, loss = 0.0740\n",
      "epoch 37 / 100, step 36/80, loss = 0.0725\n",
      "epoch 37 / 100, step 37/80, loss = 0.0699\n",
      "epoch 37 / 100, step 38/80, loss = 0.0730\n",
      "epoch 37 / 100, step 39/80, loss = 0.0757\n",
      "epoch 37 / 100, step 40/80, loss = 0.0697\n",
      "epoch 37 / 100, step 41/80, loss = 0.0727\n",
      "epoch 37 / 100, step 42/80, loss = 0.0794\n",
      "epoch 37 / 100, step 43/80, loss = 0.0758\n",
      "epoch 37 / 100, step 44/80, loss = 0.0749\n",
      "epoch 37 / 100, step 45/80, loss = 0.0900\n",
      "epoch 37 / 100, step 46/80, loss = 0.0714\n",
      "epoch 37 / 100, step 47/80, loss = 0.0719\n",
      "epoch 37 / 100, step 48/80, loss = 0.0698\n",
      "epoch 37 / 100, step 49/80, loss = 0.0711\n",
      "epoch 37 / 100, step 50/80, loss = 0.0709\n",
      "epoch 37 / 100, step 51/80, loss = 0.0706\n",
      "epoch 37 / 100, step 52/80, loss = 0.0721\n",
      "epoch 37 / 100, step 53/80, loss = 0.0743\n",
      "epoch 37 / 100, step 54/80, loss = 0.0780\n",
      "epoch 37 / 100, step 55/80, loss = 0.0807\n",
      "epoch 37 / 100, step 56/80, loss = 0.0704\n",
      "epoch 37 / 100, step 57/80, loss = 0.0794\n",
      "epoch 37 / 100, step 58/80, loss = 0.0712\n",
      "epoch 37 / 100, step 59/80, loss = 0.0813\n",
      "epoch 37 / 100, step 60/80, loss = 0.0747\n",
      "epoch 37 / 100, step 61/80, loss = 0.0783\n",
      "epoch 37 / 100, step 62/80, loss = 0.0725\n",
      "epoch 37 / 100, step 63/80, loss = 0.0794\n",
      "epoch 37 / 100, step 64/80, loss = 0.0727\n",
      "epoch 37 / 100, step 65/80, loss = 0.0814\n",
      "epoch 37 / 100, step 66/80, loss = 0.0739\n",
      "epoch 37 / 100, step 67/80, loss = 0.0747\n",
      "epoch 37 / 100, step 68/80, loss = 0.0725\n",
      "epoch 37 / 100, step 69/80, loss = 0.0775\n",
      "epoch 37 / 100, step 70/80, loss = 0.0785\n",
      "epoch 37 / 100, step 71/80, loss = 0.0725\n",
      "epoch 37 / 100, step 72/80, loss = 0.0712\n",
      "epoch 37 / 100, step 73/80, loss = 0.0730\n",
      "epoch 37 / 100, step 74/80, loss = 0.0749\n",
      "epoch 37 / 100, step 75/80, loss = 0.0728\n",
      "epoch 37 / 100, step 76/80, loss = 0.0744\n",
      "epoch 37 / 100, step 77/80, loss = 0.0730\n",
      "epoch 37 / 100, step 78/80, loss = 0.0733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 / 100, step 79/80, loss = 0.0738\n",
      "epoch 37 / 100, step 80/80, loss = 0.0750\n",
      "epoch 38 / 100, step 1/80, loss = 0.0725\n",
      "epoch 38 / 100, step 2/80, loss = 0.0698\n",
      "epoch 38 / 100, step 3/80, loss = 0.0721\n",
      "epoch 38 / 100, step 4/80, loss = 0.0723\n",
      "epoch 38 / 100, step 5/80, loss = 0.0716\n",
      "epoch 38 / 100, step 6/80, loss = 0.0742\n",
      "epoch 38 / 100, step 7/80, loss = 0.0666\n",
      "epoch 38 / 100, step 8/80, loss = 0.0673\n",
      "epoch 38 / 100, step 9/80, loss = 0.0805\n",
      "epoch 38 / 100, step 10/80, loss = 0.0682\n",
      "epoch 38 / 100, step 11/80, loss = 0.0740\n",
      "epoch 38 / 100, step 12/80, loss = 0.0725\n",
      "epoch 38 / 100, step 13/80, loss = 0.0793\n",
      "epoch 38 / 100, step 14/80, loss = 0.0703\n",
      "epoch 38 / 100, step 15/80, loss = 0.0748\n",
      "epoch 38 / 100, step 16/80, loss = 0.0773\n",
      "epoch 38 / 100, step 17/80, loss = 0.0714\n",
      "epoch 38 / 100, step 18/80, loss = 0.0682\n",
      "epoch 38 / 100, step 19/80, loss = 0.0726\n",
      "epoch 38 / 100, step 20/80, loss = 0.0721\n",
      "epoch 38 / 100, step 21/80, loss = 0.0724\n",
      "epoch 38 / 100, step 22/80, loss = 0.0692\n",
      "epoch 38 / 100, step 23/80, loss = 0.0711\n",
      "epoch 38 / 100, step 24/80, loss = 0.0687\n",
      "epoch 38 / 100, step 25/80, loss = 0.0701\n",
      "epoch 38 / 100, step 26/80, loss = 0.0704\n",
      "epoch 38 / 100, step 27/80, loss = 0.0678\n",
      "epoch 38 / 100, step 28/80, loss = 0.0743\n",
      "epoch 38 / 100, step 29/80, loss = 0.0787\n",
      "epoch 38 / 100, step 30/80, loss = 0.0766\n",
      "epoch 38 / 100, step 31/80, loss = 0.0701\n",
      "epoch 38 / 100, step 32/80, loss = 0.0760\n",
      "epoch 38 / 100, step 33/80, loss = 0.0716\n",
      "epoch 38 / 100, step 34/80, loss = 0.0697\n",
      "epoch 38 / 100, step 35/80, loss = 0.0721\n",
      "epoch 38 / 100, step 36/80, loss = 0.0731\n",
      "epoch 38 / 100, step 37/80, loss = 0.0664\n",
      "epoch 38 / 100, step 38/80, loss = 0.0761\n",
      "epoch 38 / 100, step 39/80, loss = 0.0747\n",
      "epoch 38 / 100, step 40/80, loss = 0.0687\n",
      "epoch 38 / 100, step 41/80, loss = 0.0686\n",
      "epoch 38 / 100, step 42/80, loss = 0.0682\n",
      "epoch 38 / 100, step 43/80, loss = 0.0808\n",
      "epoch 38 / 100, step 44/80, loss = 0.0685\n",
      "epoch 38 / 100, step 45/80, loss = 0.0690\n",
      "epoch 38 / 100, step 46/80, loss = 0.0722\n",
      "epoch 38 / 100, step 47/80, loss = 0.0745\n",
      "epoch 38 / 100, step 48/80, loss = 0.0680\n",
      "epoch 38 / 100, step 49/80, loss = 0.0668\n",
      "epoch 38 / 100, step 50/80, loss = 0.0716\n",
      "epoch 38 / 100, step 51/80, loss = 0.0773\n",
      "epoch 38 / 100, step 52/80, loss = 0.0801\n",
      "epoch 38 / 100, step 53/80, loss = 0.0725\n",
      "epoch 38 / 100, step 54/80, loss = 0.0722\n",
      "epoch 38 / 100, step 55/80, loss = 0.0722\n",
      "epoch 38 / 100, step 56/80, loss = 0.0696\n",
      "epoch 38 / 100, step 57/80, loss = 0.0702\n",
      "epoch 38 / 100, step 58/80, loss = 0.0736\n",
      "epoch 38 / 100, step 59/80, loss = 0.0704\n",
      "epoch 38 / 100, step 60/80, loss = 0.0767\n",
      "epoch 38 / 100, step 61/80, loss = 0.0720\n",
      "epoch 38 / 100, step 62/80, loss = 0.0769\n",
      "epoch 38 / 100, step 63/80, loss = 0.0761\n",
      "epoch 38 / 100, step 64/80, loss = 0.0715\n",
      "epoch 38 / 100, step 65/80, loss = 0.0814\n",
      "epoch 38 / 100, step 66/80, loss = 0.0724\n",
      "epoch 38 / 100, step 67/80, loss = 0.0667\n",
      "epoch 38 / 100, step 68/80, loss = 0.0751\n",
      "epoch 38 / 100, step 69/80, loss = 0.0736\n",
      "epoch 38 / 100, step 70/80, loss = 0.0727\n",
      "epoch 38 / 100, step 71/80, loss = 0.0731\n",
      "epoch 38 / 100, step 72/80, loss = 0.0697\n",
      "epoch 38 / 100, step 73/80, loss = 0.0714\n",
      "epoch 38 / 100, step 74/80, loss = 0.0688\n",
      "epoch 38 / 100, step 75/80, loss = 0.0727\n",
      "epoch 38 / 100, step 76/80, loss = 0.0751\n",
      "epoch 38 / 100, step 77/80, loss = 0.0698\n",
      "epoch 38 / 100, step 78/80, loss = 0.0679\n",
      "epoch 38 / 100, step 79/80, loss = 0.0680\n",
      "epoch 38 / 100, step 80/80, loss = 0.0724\n",
      "epoch 39 / 100, step 1/80, loss = 0.0712\n",
      "epoch 39 / 100, step 2/80, loss = 0.0721\n",
      "epoch 39 / 100, step 3/80, loss = 0.0699\n",
      "epoch 39 / 100, step 4/80, loss = 0.0699\n",
      "epoch 39 / 100, step 5/80, loss = 0.0748\n",
      "epoch 39 / 100, step 6/80, loss = 0.0698\n",
      "epoch 39 / 100, step 7/80, loss = 0.0688\n",
      "epoch 39 / 100, step 8/80, loss = 0.0698\n",
      "epoch 39 / 100, step 9/80, loss = 0.0701\n",
      "epoch 39 / 100, step 10/80, loss = 0.0696\n",
      "epoch 39 / 100, step 11/80, loss = 0.0667\n",
      "epoch 39 / 100, step 12/80, loss = 0.0723\n",
      "epoch 39 / 100, step 13/80, loss = 0.0655\n",
      "epoch 39 / 100, step 14/80, loss = 0.0655\n",
      "epoch 39 / 100, step 15/80, loss = 0.0667\n",
      "epoch 39 / 100, step 16/80, loss = 0.0751\n",
      "epoch 39 / 100, step 17/80, loss = 0.0745\n",
      "epoch 39 / 100, step 18/80, loss = 0.0687\n",
      "epoch 39 / 100, step 19/80, loss = 0.0708\n",
      "epoch 39 / 100, step 20/80, loss = 0.0720\n",
      "epoch 39 / 100, step 21/80, loss = 0.0691\n",
      "epoch 39 / 100, step 22/80, loss = 0.0708\n",
      "epoch 39 / 100, step 23/80, loss = 0.0747\n",
      "epoch 39 / 100, step 24/80, loss = 0.0686\n",
      "epoch 39 / 100, step 25/80, loss = 0.0670\n",
      "epoch 39 / 100, step 26/80, loss = 0.0671\n",
      "epoch 39 / 100, step 27/80, loss = 0.0645\n",
      "epoch 39 / 100, step 28/80, loss = 0.0750\n",
      "epoch 39 / 100, step 29/80, loss = 0.0743\n",
      "epoch 39 / 100, step 30/80, loss = 0.0740\n",
      "epoch 39 / 100, step 31/80, loss = 0.0680\n",
      "epoch 39 / 100, step 32/80, loss = 0.0677\n",
      "epoch 39 / 100, step 33/80, loss = 0.0669\n",
      "epoch 39 / 100, step 34/80, loss = 0.0682\n",
      "epoch 39 / 100, step 35/80, loss = 0.0637\n",
      "epoch 39 / 100, step 36/80, loss = 0.0658\n",
      "epoch 39 / 100, step 37/80, loss = 0.0706\n",
      "epoch 39 / 100, step 38/80, loss = 0.0689\n",
      "epoch 39 / 100, step 39/80, loss = 0.0704\n",
      "epoch 39 / 100, step 40/80, loss = 0.0726\n",
      "epoch 39 / 100, step 41/80, loss = 0.0660\n",
      "epoch 39 / 100, step 42/80, loss = 0.0690\n",
      "epoch 39 / 100, step 43/80, loss = 0.0685\n",
      "epoch 39 / 100, step 44/80, loss = 0.0710\n",
      "epoch 39 / 100, step 45/80, loss = 0.0667\n",
      "epoch 39 / 100, step 46/80, loss = 0.0679\n",
      "epoch 39 / 100, step 47/80, loss = 0.0665\n",
      "epoch 39 / 100, step 48/80, loss = 0.0707\n",
      "epoch 39 / 100, step 49/80, loss = 0.0694\n",
      "epoch 39 / 100, step 50/80, loss = 0.0694\n",
      "epoch 39 / 100, step 51/80, loss = 0.0666\n",
      "epoch 39 / 100, step 52/80, loss = 0.0775\n",
      "epoch 39 / 100, step 53/80, loss = 0.0681\n",
      "epoch 39 / 100, step 54/80, loss = 0.0722\n",
      "epoch 39 / 100, step 55/80, loss = 0.0690\n",
      "epoch 39 / 100, step 56/80, loss = 0.0666\n",
      "epoch 39 / 100, step 57/80, loss = 0.0682\n",
      "epoch 39 / 100, step 58/80, loss = 0.0709\n",
      "epoch 39 / 100, step 59/80, loss = 0.0707\n",
      "epoch 39 / 100, step 60/80, loss = 0.0717\n",
      "epoch 39 / 100, step 61/80, loss = 0.0658\n",
      "epoch 39 / 100, step 62/80, loss = 0.0664\n",
      "epoch 39 / 100, step 63/80, loss = 0.0712\n",
      "epoch 39 / 100, step 64/80, loss = 0.0698\n",
      "epoch 39 / 100, step 65/80, loss = 0.0721\n",
      "epoch 39 / 100, step 66/80, loss = 0.0684\n",
      "epoch 39 / 100, step 67/80, loss = 0.0678\n",
      "epoch 39 / 100, step 68/80, loss = 0.0676\n",
      "epoch 39 / 100, step 69/80, loss = 0.0666\n",
      "epoch 39 / 100, step 70/80, loss = 0.0699\n",
      "epoch 39 / 100, step 71/80, loss = 0.0773\n",
      "epoch 39 / 100, step 72/80, loss = 0.0776\n",
      "epoch 39 / 100, step 73/80, loss = 0.0718\n",
      "epoch 39 / 100, step 74/80, loss = 0.0713\n",
      "epoch 39 / 100, step 75/80, loss = 0.0689\n",
      "epoch 39 / 100, step 76/80, loss = 0.0711\n",
      "epoch 39 / 100, step 77/80, loss = 0.0664\n",
      "epoch 39 / 100, step 78/80, loss = 0.0704\n",
      "epoch 39 / 100, step 79/80, loss = 0.0698\n",
      "epoch 39 / 100, step 80/80, loss = 0.0717\n",
      "epoch 40 / 100, step 1/80, loss = 0.0710\n",
      "epoch 40 / 100, step 2/80, loss = 0.0685\n",
      "epoch 40 / 100, step 3/80, loss = 0.0643\n",
      "epoch 40 / 100, step 4/80, loss = 0.0707\n",
      "epoch 40 / 100, step 5/80, loss = 0.0673\n",
      "epoch 40 / 100, step 6/80, loss = 0.0775\n",
      "epoch 40 / 100, step 7/80, loss = 0.0695\n",
      "epoch 40 / 100, step 8/80, loss = 0.0669\n",
      "epoch 40 / 100, step 9/80, loss = 0.0667\n",
      "epoch 40 / 100, step 10/80, loss = 0.0709\n",
      "epoch 40 / 100, step 11/80, loss = 0.0706\n",
      "epoch 40 / 100, step 12/80, loss = 0.0688\n",
      "epoch 40 / 100, step 13/80, loss = 0.0702\n",
      "epoch 40 / 100, step 14/80, loss = 0.0705\n",
      "epoch 40 / 100, step 15/80, loss = 0.0654\n",
      "epoch 40 / 100, step 16/80, loss = 0.0629\n",
      "epoch 40 / 100, step 17/80, loss = 0.0663\n",
      "epoch 40 / 100, step 18/80, loss = 0.0681\n",
      "epoch 40 / 100, step 19/80, loss = 0.0674\n",
      "epoch 40 / 100, step 20/80, loss = 0.0639\n",
      "epoch 40 / 100, step 21/80, loss = 0.0651\n",
      "epoch 40 / 100, step 22/80, loss = 0.0762\n",
      "epoch 40 / 100, step 23/80, loss = 0.0661\n",
      "epoch 40 / 100, step 24/80, loss = 0.0661\n",
      "epoch 40 / 100, step 25/80, loss = 0.0734\n",
      "epoch 40 / 100, step 26/80, loss = 0.0713\n",
      "epoch 40 / 100, step 27/80, loss = 0.0662\n",
      "epoch 40 / 100, step 28/80, loss = 0.0670\n",
      "epoch 40 / 100, step 29/80, loss = 0.0658\n",
      "epoch 40 / 100, step 30/80, loss = 0.0660\n",
      "epoch 40 / 100, step 31/80, loss = 0.0714\n",
      "epoch 40 / 100, step 32/80, loss = 0.0676\n",
      "epoch 40 / 100, step 33/80, loss = 0.0700\n",
      "epoch 40 / 100, step 34/80, loss = 0.0674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 / 100, step 35/80, loss = 0.0621\n",
      "epoch 40 / 100, step 36/80, loss = 0.0693\n",
      "epoch 40 / 100, step 37/80, loss = 0.0714\n",
      "epoch 40 / 100, step 38/80, loss = 0.0644\n",
      "epoch 40 / 100, step 39/80, loss = 0.0693\n",
      "epoch 40 / 100, step 40/80, loss = 0.0684\n",
      "epoch 40 / 100, step 41/80, loss = 0.0684\n",
      "epoch 40 / 100, step 42/80, loss = 0.0651\n",
      "epoch 40 / 100, step 43/80, loss = 0.0649\n",
      "epoch 40 / 100, step 44/80, loss = 0.0744\n",
      "epoch 40 / 100, step 45/80, loss = 0.0628\n",
      "epoch 40 / 100, step 46/80, loss = 0.0717\n",
      "epoch 40 / 100, step 47/80, loss = 0.0694\n",
      "epoch 40 / 100, step 48/80, loss = 0.0674\n",
      "epoch 40 / 100, step 49/80, loss = 0.0662\n",
      "epoch 40 / 100, step 50/80, loss = 0.0746\n",
      "epoch 40 / 100, step 51/80, loss = 0.0660\n",
      "epoch 40 / 100, step 52/80, loss = 0.0672\n",
      "epoch 40 / 100, step 53/80, loss = 0.0658\n",
      "epoch 40 / 100, step 54/80, loss = 0.0796\n",
      "epoch 40 / 100, step 55/80, loss = 0.0682\n",
      "epoch 40 / 100, step 56/80, loss = 0.0656\n",
      "epoch 40 / 100, step 57/80, loss = 0.0700\n",
      "epoch 40 / 100, step 58/80, loss = 0.0688\n",
      "epoch 40 / 100, step 59/80, loss = 0.0679\n",
      "epoch 40 / 100, step 60/80, loss = 0.0707\n",
      "epoch 40 / 100, step 61/80, loss = 0.0684\n",
      "epoch 40 / 100, step 62/80, loss = 0.0684\n",
      "epoch 40 / 100, step 63/80, loss = 0.0661\n",
      "epoch 40 / 100, step 64/80, loss = 0.0705\n",
      "epoch 40 / 100, step 65/80, loss = 0.0653\n",
      "epoch 40 / 100, step 66/80, loss = 0.0664\n",
      "epoch 40 / 100, step 67/80, loss = 0.0708\n",
      "epoch 40 / 100, step 68/80, loss = 0.0644\n",
      "epoch 40 / 100, step 69/80, loss = 0.0685\n",
      "epoch 40 / 100, step 70/80, loss = 0.0630\n",
      "epoch 40 / 100, step 71/80, loss = 0.0667\n",
      "epoch 40 / 100, step 72/80, loss = 0.0726\n",
      "epoch 40 / 100, step 73/80, loss = 0.0636\n",
      "epoch 40 / 100, step 74/80, loss = 0.0718\n",
      "epoch 40 / 100, step 75/80, loss = 0.0684\n",
      "epoch 40 / 100, step 76/80, loss = 0.0663\n",
      "epoch 40 / 100, step 77/80, loss = 0.0678\n",
      "epoch 40 / 100, step 78/80, loss = 0.0645\n",
      "epoch 40 / 100, step 79/80, loss = 0.0704\n",
      "epoch 40 / 100, step 80/80, loss = 0.0688\n",
      "epoch 41 / 100, step 1/80, loss = 0.0653\n",
      "epoch 41 / 100, step 2/80, loss = 0.0676\n",
      "epoch 41 / 100, step 3/80, loss = 0.0650\n",
      "epoch 41 / 100, step 4/80, loss = 0.0671\n",
      "epoch 41 / 100, step 5/80, loss = 0.0638\n",
      "epoch 41 / 100, step 6/80, loss = 0.0661\n",
      "epoch 41 / 100, step 7/80, loss = 0.0653\n",
      "epoch 41 / 100, step 8/80, loss = 0.0647\n",
      "epoch 41 / 100, step 9/80, loss = 0.0672\n",
      "epoch 41 / 100, step 10/80, loss = 0.0617\n",
      "epoch 41 / 100, step 11/80, loss = 0.0698\n",
      "epoch 41 / 100, step 12/80, loss = 0.0641\n",
      "epoch 41 / 100, step 13/80, loss = 0.0674\n",
      "epoch 41 / 100, step 14/80, loss = 0.0676\n",
      "epoch 41 / 100, step 15/80, loss = 0.0673\n",
      "epoch 41 / 100, step 16/80, loss = 0.0705\n",
      "epoch 41 / 100, step 17/80, loss = 0.0683\n",
      "epoch 41 / 100, step 18/80, loss = 0.0654\n",
      "epoch 41 / 100, step 19/80, loss = 0.0661\n",
      "epoch 41 / 100, step 20/80, loss = 0.0677\n",
      "epoch 41 / 100, step 21/80, loss = 0.0629\n",
      "epoch 41 / 100, step 22/80, loss = 0.0674\n",
      "epoch 41 / 100, step 23/80, loss = 0.0672\n",
      "epoch 41 / 100, step 24/80, loss = 0.0652\n",
      "epoch 41 / 100, step 25/80, loss = 0.0637\n",
      "epoch 41 / 100, step 26/80, loss = 0.0679\n",
      "epoch 41 / 100, step 27/80, loss = 0.0679\n",
      "epoch 41 / 100, step 28/80, loss = 0.0679\n",
      "epoch 41 / 100, step 29/80, loss = 0.0616\n",
      "epoch 41 / 100, step 30/80, loss = 0.0680\n",
      "epoch 41 / 100, step 31/80, loss = 0.0676\n",
      "epoch 41 / 100, step 32/80, loss = 0.0704\n",
      "epoch 41 / 100, step 33/80, loss = 0.0666\n",
      "epoch 41 / 100, step 34/80, loss = 0.0629\n",
      "epoch 41 / 100, step 35/80, loss = 0.0687\n",
      "epoch 41 / 100, step 36/80, loss = 0.0700\n",
      "epoch 41 / 100, step 37/80, loss = 0.0744\n",
      "epoch 41 / 100, step 38/80, loss = 0.0597\n",
      "epoch 41 / 100, step 39/80, loss = 0.0660\n",
      "epoch 41 / 100, step 40/80, loss = 0.0657\n",
      "epoch 41 / 100, step 41/80, loss = 0.0717\n",
      "epoch 41 / 100, step 42/80, loss = 0.0679\n",
      "epoch 41 / 100, step 43/80, loss = 0.0696\n",
      "epoch 41 / 100, step 44/80, loss = 0.0671\n",
      "epoch 41 / 100, step 45/80, loss = 0.0651\n",
      "epoch 41 / 100, step 46/80, loss = 0.0664\n",
      "epoch 41 / 100, step 47/80, loss = 0.0663\n",
      "epoch 41 / 100, step 48/80, loss = 0.0704\n",
      "epoch 41 / 100, step 49/80, loss = 0.0624\n",
      "epoch 41 / 100, step 50/80, loss = 0.0665\n",
      "epoch 41 / 100, step 51/80, loss = 0.0633\n",
      "epoch 41 / 100, step 52/80, loss = 0.0687\n",
      "epoch 41 / 100, step 53/80, loss = 0.0663\n",
      "epoch 41 / 100, step 54/80, loss = 0.0678\n",
      "epoch 41 / 100, step 55/80, loss = 0.0643\n",
      "epoch 41 / 100, step 56/80, loss = 0.0621\n",
      "epoch 41 / 100, step 57/80, loss = 0.0713\n",
      "epoch 41 / 100, step 58/80, loss = 0.0690\n",
      "epoch 41 / 100, step 59/80, loss = 0.0685\n",
      "epoch 41 / 100, step 60/80, loss = 0.0646\n",
      "epoch 41 / 100, step 61/80, loss = 0.0633\n",
      "epoch 41 / 100, step 62/80, loss = 0.0647\n",
      "epoch 41 / 100, step 63/80, loss = 0.0606\n",
      "epoch 41 / 100, step 64/80, loss = 0.0608\n",
      "epoch 41 / 100, step 65/80, loss = 0.0695\n",
      "epoch 41 / 100, step 66/80, loss = 0.0675\n",
      "epoch 41 / 100, step 67/80, loss = 0.0759\n",
      "epoch 41 / 100, step 68/80, loss = 0.0682\n",
      "epoch 41 / 100, step 69/80, loss = 0.0639\n",
      "epoch 41 / 100, step 70/80, loss = 0.0672\n",
      "epoch 41 / 100, step 71/80, loss = 0.0620\n",
      "epoch 41 / 100, step 72/80, loss = 0.0669\n",
      "epoch 41 / 100, step 73/80, loss = 0.0657\n",
      "epoch 41 / 100, step 74/80, loss = 0.0660\n",
      "epoch 41 / 100, step 75/80, loss = 0.0631\n",
      "epoch 41 / 100, step 76/80, loss = 0.0654\n",
      "epoch 41 / 100, step 77/80, loss = 0.0623\n",
      "epoch 41 / 100, step 78/80, loss = 0.0651\n",
      "epoch 41 / 100, step 79/80, loss = 0.0644\n",
      "epoch 41 / 100, step 80/80, loss = 0.0676\n",
      "epoch 42 / 100, step 1/80, loss = 0.0698\n",
      "epoch 42 / 100, step 2/80, loss = 0.0623\n",
      "epoch 42 / 100, step 3/80, loss = 0.0635\n",
      "epoch 42 / 100, step 4/80, loss = 0.0654\n",
      "epoch 42 / 100, step 5/80, loss = 0.0619\n",
      "epoch 42 / 100, step 6/80, loss = 0.0622\n",
      "epoch 42 / 100, step 7/80, loss = 0.0640\n",
      "epoch 42 / 100, step 8/80, loss = 0.0698\n",
      "epoch 42 / 100, step 9/80, loss = 0.0629\n",
      "epoch 42 / 100, step 10/80, loss = 0.0635\n",
      "epoch 42 / 100, step 11/80, loss = 0.0647\n",
      "epoch 42 / 100, step 12/80, loss = 0.0637\n",
      "epoch 42 / 100, step 13/80, loss = 0.0623\n",
      "epoch 42 / 100, step 14/80, loss = 0.0644\n",
      "epoch 42 / 100, step 15/80, loss = 0.0644\n",
      "epoch 42 / 100, step 16/80, loss = 0.0630\n",
      "epoch 42 / 100, step 17/80, loss = 0.0655\n",
      "epoch 42 / 100, step 18/80, loss = 0.0623\n",
      "epoch 42 / 100, step 19/80, loss = 0.0609\n",
      "epoch 42 / 100, step 20/80, loss = 0.0618\n",
      "epoch 42 / 100, step 21/80, loss = 0.0630\n",
      "epoch 42 / 100, step 22/80, loss = 0.0607\n",
      "epoch 42 / 100, step 23/80, loss = 0.0641\n",
      "epoch 42 / 100, step 24/80, loss = 0.0611\n",
      "epoch 42 / 100, step 25/80, loss = 0.0661\n",
      "epoch 42 / 100, step 26/80, loss = 0.0638\n",
      "epoch 42 / 100, step 27/80, loss = 0.0613\n",
      "epoch 42 / 100, step 28/80, loss = 0.0646\n",
      "epoch 42 / 100, step 29/80, loss = 0.0641\n",
      "epoch 42 / 100, step 30/80, loss = 0.0634\n",
      "epoch 42 / 100, step 31/80, loss = 0.0605\n",
      "epoch 42 / 100, step 32/80, loss = 0.0640\n",
      "epoch 42 / 100, step 33/80, loss = 0.0622\n",
      "epoch 42 / 100, step 34/80, loss = 0.0609\n",
      "epoch 42 / 100, step 35/80, loss = 0.0645\n",
      "epoch 42 / 100, step 36/80, loss = 0.0715\n",
      "epoch 42 / 100, step 37/80, loss = 0.0613\n",
      "epoch 42 / 100, step 38/80, loss = 0.0598\n",
      "epoch 42 / 100, step 39/80, loss = 0.0633\n",
      "epoch 42 / 100, step 40/80, loss = 0.0585\n",
      "epoch 42 / 100, step 41/80, loss = 0.0698\n",
      "epoch 42 / 100, step 42/80, loss = 0.0609\n",
      "epoch 42 / 100, step 43/80, loss = 0.0642\n",
      "epoch 42 / 100, step 44/80, loss = 0.0678\n",
      "epoch 42 / 100, step 45/80, loss = 0.0692\n",
      "epoch 42 / 100, step 46/80, loss = 0.0619\n",
      "epoch 42 / 100, step 47/80, loss = 0.0614\n",
      "epoch 42 / 100, step 48/80, loss = 0.0619\n",
      "epoch 42 / 100, step 49/80, loss = 0.0678\n",
      "epoch 42 / 100, step 50/80, loss = 0.0652\n",
      "epoch 42 / 100, step 51/80, loss = 0.0659\n",
      "epoch 42 / 100, step 52/80, loss = 0.0679\n",
      "epoch 42 / 100, step 53/80, loss = 0.0701\n",
      "epoch 42 / 100, step 54/80, loss = 0.0648\n",
      "epoch 42 / 100, step 55/80, loss = 0.0691\n",
      "epoch 42 / 100, step 56/80, loss = 0.0650\n",
      "epoch 42 / 100, step 57/80, loss = 0.0675\n",
      "epoch 42 / 100, step 58/80, loss = 0.0666\n",
      "epoch 42 / 100, step 59/80, loss = 0.0650\n",
      "epoch 42 / 100, step 60/80, loss = 0.0609\n",
      "epoch 42 / 100, step 61/80, loss = 0.0607\n",
      "epoch 42 / 100, step 62/80, loss = 0.0589\n",
      "epoch 42 / 100, step 63/80, loss = 0.0609\n",
      "epoch 42 / 100, step 64/80, loss = 0.0641\n",
      "epoch 42 / 100, step 65/80, loss = 0.0672\n",
      "epoch 42 / 100, step 66/80, loss = 0.0689\n",
      "epoch 42 / 100, step 67/80, loss = 0.0624\n",
      "epoch 42 / 100, step 68/80, loss = 0.0692\n",
      "epoch 42 / 100, step 69/80, loss = 0.0633\n",
      "epoch 42 / 100, step 70/80, loss = 0.0639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 / 100, step 71/80, loss = 0.0636\n",
      "epoch 42 / 100, step 72/80, loss = 0.0643\n",
      "epoch 42 / 100, step 73/80, loss = 0.0604\n",
      "epoch 42 / 100, step 74/80, loss = 0.0660\n",
      "epoch 42 / 100, step 75/80, loss = 0.0663\n",
      "epoch 42 / 100, step 76/80, loss = 0.0675\n",
      "epoch 42 / 100, step 77/80, loss = 0.0728\n",
      "epoch 42 / 100, step 78/80, loss = 0.0668\n",
      "epoch 42 / 100, step 79/80, loss = 0.0615\n",
      "epoch 42 / 100, step 80/80, loss = 0.0633\n",
      "epoch 43 / 100, step 1/80, loss = 0.0588\n",
      "epoch 43 / 100, step 2/80, loss = 0.0635\n",
      "epoch 43 / 100, step 3/80, loss = 0.0642\n",
      "epoch 43 / 100, step 4/80, loss = 0.0582\n",
      "epoch 43 / 100, step 5/80, loss = 0.0654\n",
      "epoch 43 / 100, step 6/80, loss = 0.0574\n",
      "epoch 43 / 100, step 7/80, loss = 0.0664\n",
      "epoch 43 / 100, step 8/80, loss = 0.0668\n",
      "epoch 43 / 100, step 9/80, loss = 0.0600\n",
      "epoch 43 / 100, step 10/80, loss = 0.0642\n",
      "epoch 43 / 100, step 11/80, loss = 0.0642\n",
      "epoch 43 / 100, step 12/80, loss = 0.0637\n",
      "epoch 43 / 100, step 13/80, loss = 0.0595\n",
      "epoch 43 / 100, step 14/80, loss = 0.0610\n",
      "epoch 43 / 100, step 15/80, loss = 0.0671\n",
      "epoch 43 / 100, step 16/80, loss = 0.0665\n",
      "epoch 43 / 100, step 17/80, loss = 0.0607\n",
      "epoch 43 / 100, step 18/80, loss = 0.0647\n",
      "epoch 43 / 100, step 19/80, loss = 0.0591\n",
      "epoch 43 / 100, step 20/80, loss = 0.0639\n",
      "epoch 43 / 100, step 21/80, loss = 0.0646\n",
      "epoch 43 / 100, step 22/80, loss = 0.0647\n",
      "epoch 43 / 100, step 23/80, loss = 0.0594\n",
      "epoch 43 / 100, step 24/80, loss = 0.0614\n",
      "epoch 43 / 100, step 25/80, loss = 0.0655\n",
      "epoch 43 / 100, step 26/80, loss = 0.0675\n",
      "epoch 43 / 100, step 27/80, loss = 0.0626\n",
      "epoch 43 / 100, step 28/80, loss = 0.0662\n",
      "epoch 43 / 100, step 29/80, loss = 0.0608\n",
      "epoch 43 / 100, step 30/80, loss = 0.0582\n",
      "epoch 43 / 100, step 31/80, loss = 0.0653\n",
      "epoch 43 / 100, step 32/80, loss = 0.0652\n",
      "epoch 43 / 100, step 33/80, loss = 0.0611\n",
      "epoch 43 / 100, step 34/80, loss = 0.0652\n",
      "epoch 43 / 100, step 35/80, loss = 0.0602\n",
      "epoch 43 / 100, step 36/80, loss = 0.0629\n",
      "epoch 43 / 100, step 37/80, loss = 0.0598\n",
      "epoch 43 / 100, step 38/80, loss = 0.0639\n",
      "epoch 43 / 100, step 39/80, loss = 0.0609\n",
      "epoch 43 / 100, step 40/80, loss = 0.0607\n",
      "epoch 43 / 100, step 41/80, loss = 0.0700\n",
      "epoch 43 / 100, step 42/80, loss = 0.0639\n",
      "epoch 43 / 100, step 43/80, loss = 0.0585\n",
      "epoch 43 / 100, step 44/80, loss = 0.0616\n",
      "epoch 43 / 100, step 45/80, loss = 0.0637\n",
      "epoch 43 / 100, step 46/80, loss = 0.0586\n",
      "epoch 43 / 100, step 47/80, loss = 0.0618\n",
      "epoch 43 / 100, step 48/80, loss = 0.0585\n",
      "epoch 43 / 100, step 49/80, loss = 0.0671\n",
      "epoch 43 / 100, step 50/80, loss = 0.0634\n",
      "epoch 43 / 100, step 51/80, loss = 0.0631\n",
      "epoch 43 / 100, step 52/80, loss = 0.0625\n",
      "epoch 43 / 100, step 53/80, loss = 0.0611\n",
      "epoch 43 / 100, step 54/80, loss = 0.0642\n",
      "epoch 43 / 100, step 55/80, loss = 0.0697\n",
      "epoch 43 / 100, step 56/80, loss = 0.0627\n",
      "epoch 43 / 100, step 57/80, loss = 0.0626\n",
      "epoch 43 / 100, step 58/80, loss = 0.0637\n",
      "epoch 43 / 100, step 59/80, loss = 0.0651\n",
      "epoch 43 / 100, step 60/80, loss = 0.0596\n",
      "epoch 43 / 100, step 61/80, loss = 0.0657\n",
      "epoch 43 / 100, step 62/80, loss = 0.0633\n",
      "epoch 43 / 100, step 63/80, loss = 0.0623\n",
      "epoch 43 / 100, step 64/80, loss = 0.0625\n",
      "epoch 43 / 100, step 65/80, loss = 0.0642\n",
      "epoch 43 / 100, step 66/80, loss = 0.0635\n",
      "epoch 43 / 100, step 67/80, loss = 0.0617\n",
      "epoch 43 / 100, step 68/80, loss = 0.0678\n",
      "epoch 43 / 100, step 69/80, loss = 0.0605\n",
      "epoch 43 / 100, step 70/80, loss = 0.0660\n",
      "epoch 43 / 100, step 71/80, loss = 0.0655\n",
      "epoch 43 / 100, step 72/80, loss = 0.0608\n",
      "epoch 43 / 100, step 73/80, loss = 0.0620\n",
      "epoch 43 / 100, step 74/80, loss = 0.0638\n",
      "epoch 43 / 100, step 75/80, loss = 0.0626\n",
      "epoch 43 / 100, step 76/80, loss = 0.0591\n",
      "epoch 43 / 100, step 77/80, loss = 0.0613\n",
      "epoch 43 / 100, step 78/80, loss = 0.0612\n",
      "epoch 43 / 100, step 79/80, loss = 0.0629\n",
      "epoch 43 / 100, step 80/80, loss = 0.0646\n",
      "epoch 44 / 100, step 1/80, loss = 0.0571\n",
      "epoch 44 / 100, step 2/80, loss = 0.0607\n",
      "epoch 44 / 100, step 3/80, loss = 0.0600\n",
      "epoch 44 / 100, step 4/80, loss = 0.0586\n",
      "epoch 44 / 100, step 5/80, loss = 0.0652\n",
      "epoch 44 / 100, step 6/80, loss = 0.0581\n",
      "epoch 44 / 100, step 7/80, loss = 0.0570\n",
      "epoch 44 / 100, step 8/80, loss = 0.0615\n",
      "epoch 44 / 100, step 9/80, loss = 0.0665\n",
      "epoch 44 / 100, step 10/80, loss = 0.0559\n",
      "epoch 44 / 100, step 11/80, loss = 0.0622\n",
      "epoch 44 / 100, step 12/80, loss = 0.0652\n",
      "epoch 44 / 100, step 13/80, loss = 0.0597\n",
      "epoch 44 / 100, step 14/80, loss = 0.0629\n",
      "epoch 44 / 100, step 15/80, loss = 0.0617\n",
      "epoch 44 / 100, step 16/80, loss = 0.0589\n",
      "epoch 44 / 100, step 17/80, loss = 0.0593\n",
      "epoch 44 / 100, step 18/80, loss = 0.0624\n",
      "epoch 44 / 100, step 19/80, loss = 0.0654\n",
      "epoch 44 / 100, step 20/80, loss = 0.0558\n",
      "epoch 44 / 100, step 21/80, loss = 0.0613\n",
      "epoch 44 / 100, step 22/80, loss = 0.0608\n",
      "epoch 44 / 100, step 23/80, loss = 0.0607\n",
      "epoch 44 / 100, step 24/80, loss = 0.0606\n",
      "epoch 44 / 100, step 25/80, loss = 0.0603\n",
      "epoch 44 / 100, step 26/80, loss = 0.0610\n",
      "epoch 44 / 100, step 27/80, loss = 0.0602\n",
      "epoch 44 / 100, step 28/80, loss = 0.0607\n",
      "epoch 44 / 100, step 29/80, loss = 0.0597\n",
      "epoch 44 / 100, step 30/80, loss = 0.0619\n",
      "epoch 44 / 100, step 31/80, loss = 0.0611\n",
      "epoch 44 / 100, step 32/80, loss = 0.0610\n",
      "epoch 44 / 100, step 33/80, loss = 0.0592\n",
      "epoch 44 / 100, step 34/80, loss = 0.0641\n",
      "epoch 44 / 100, step 35/80, loss = 0.0653\n",
      "epoch 44 / 100, step 36/80, loss = 0.0620\n",
      "epoch 44 / 100, step 37/80, loss = 0.0601\n",
      "epoch 44 / 100, step 38/80, loss = 0.0619\n",
      "epoch 44 / 100, step 39/80, loss = 0.0652\n",
      "epoch 44 / 100, step 40/80, loss = 0.0630\n",
      "epoch 44 / 100, step 41/80, loss = 0.0617\n",
      "epoch 44 / 100, step 42/80, loss = 0.0592\n",
      "epoch 44 / 100, step 43/80, loss = 0.0676\n",
      "epoch 44 / 100, step 44/80, loss = 0.0648\n",
      "epoch 44 / 100, step 45/80, loss = 0.0579\n",
      "epoch 44 / 100, step 46/80, loss = 0.0608\n",
      "epoch 44 / 100, step 47/80, loss = 0.0595\n",
      "epoch 44 / 100, step 48/80, loss = 0.0617\n",
      "epoch 44 / 100, step 49/80, loss = 0.0638\n",
      "epoch 44 / 100, step 50/80, loss = 0.0633\n",
      "epoch 44 / 100, step 51/80, loss = 0.0576\n",
      "epoch 44 / 100, step 52/80, loss = 0.0621\n",
      "epoch 44 / 100, step 53/80, loss = 0.0653\n",
      "epoch 44 / 100, step 54/80, loss = 0.0643\n",
      "epoch 44 / 100, step 55/80, loss = 0.0599\n",
      "epoch 44 / 100, step 56/80, loss = 0.0572\n",
      "epoch 44 / 100, step 57/80, loss = 0.0572\n",
      "epoch 44 / 100, step 58/80, loss = 0.0597\n",
      "epoch 44 / 100, step 59/80, loss = 0.0569\n",
      "epoch 44 / 100, step 60/80, loss = 0.0631\n",
      "epoch 44 / 100, step 61/80, loss = 0.0634\n",
      "epoch 44 / 100, step 62/80, loss = 0.0610\n",
      "epoch 44 / 100, step 63/80, loss = 0.0570\n",
      "epoch 44 / 100, step 64/80, loss = 0.0582\n",
      "epoch 44 / 100, step 65/80, loss = 0.0604\n",
      "epoch 44 / 100, step 66/80, loss = 0.0614\n",
      "epoch 44 / 100, step 67/80, loss = 0.0615\n",
      "epoch 44 / 100, step 68/80, loss = 0.0584\n",
      "epoch 44 / 100, step 69/80, loss = 0.0550\n",
      "epoch 44 / 100, step 70/80, loss = 0.0619\n",
      "epoch 44 / 100, step 71/80, loss = 0.0606\n",
      "epoch 44 / 100, step 72/80, loss = 0.0592\n",
      "epoch 44 / 100, step 73/80, loss = 0.0619\n",
      "epoch 44 / 100, step 74/80, loss = 0.0589\n",
      "epoch 44 / 100, step 75/80, loss = 0.0596\n",
      "epoch 44 / 100, step 76/80, loss = 0.0683\n",
      "epoch 44 / 100, step 77/80, loss = 0.0670\n",
      "epoch 44 / 100, step 78/80, loss = 0.0605\n",
      "epoch 44 / 100, step 79/80, loss = 0.0550\n",
      "epoch 44 / 100, step 80/80, loss = 0.0636\n",
      "epoch 45 / 100, step 1/80, loss = 0.0644\n",
      "epoch 45 / 100, step 2/80, loss = 0.0554\n",
      "epoch 45 / 100, step 3/80, loss = 0.0617\n",
      "epoch 45 / 100, step 4/80, loss = 0.0595\n",
      "epoch 45 / 100, step 5/80, loss = 0.0560\n",
      "epoch 45 / 100, step 6/80, loss = 0.0606\n",
      "epoch 45 / 100, step 7/80, loss = 0.0574\n",
      "epoch 45 / 100, step 8/80, loss = 0.0542\n",
      "epoch 45 / 100, step 9/80, loss = 0.0594\n",
      "epoch 45 / 100, step 10/80, loss = 0.0591\n",
      "epoch 45 / 100, step 11/80, loss = 0.0597\n",
      "epoch 45 / 100, step 12/80, loss = 0.0575\n",
      "epoch 45 / 100, step 13/80, loss = 0.0547\n",
      "epoch 45 / 100, step 14/80, loss = 0.0615\n",
      "epoch 45 / 100, step 15/80, loss = 0.0603\n",
      "epoch 45 / 100, step 16/80, loss = 0.0606\n",
      "epoch 45 / 100, step 17/80, loss = 0.0557\n",
      "epoch 45 / 100, step 18/80, loss = 0.0601\n",
      "epoch 45 / 100, step 19/80, loss = 0.0591\n",
      "epoch 45 / 100, step 20/80, loss = 0.0593\n",
      "epoch 45 / 100, step 21/80, loss = 0.0623\n",
      "epoch 45 / 100, step 22/80, loss = 0.0569\n",
      "epoch 45 / 100, step 23/80, loss = 0.0652\n",
      "epoch 45 / 100, step 24/80, loss = 0.0584\n",
      "epoch 45 / 100, step 25/80, loss = 0.0581\n",
      "epoch 45 / 100, step 26/80, loss = 0.0577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45 / 100, step 27/80, loss = 0.0621\n",
      "epoch 45 / 100, step 28/80, loss = 0.0588\n",
      "epoch 45 / 100, step 29/80, loss = 0.0612\n",
      "epoch 45 / 100, step 30/80, loss = 0.0594\n",
      "epoch 45 / 100, step 31/80, loss = 0.0618\n",
      "epoch 45 / 100, step 32/80, loss = 0.0579\n",
      "epoch 45 / 100, step 33/80, loss = 0.0594\n",
      "epoch 45 / 100, step 34/80, loss = 0.0632\n",
      "epoch 45 / 100, step 35/80, loss = 0.0585\n",
      "epoch 45 / 100, step 36/80, loss = 0.0594\n",
      "epoch 45 / 100, step 37/80, loss = 0.0604\n",
      "epoch 45 / 100, step 38/80, loss = 0.0584\n",
      "epoch 45 / 100, step 39/80, loss = 0.0602\n",
      "epoch 45 / 100, step 40/80, loss = 0.0612\n",
      "epoch 45 / 100, step 41/80, loss = 0.0561\n",
      "epoch 45 / 100, step 42/80, loss = 0.0539\n",
      "epoch 45 / 100, step 43/80, loss = 0.0584\n",
      "epoch 45 / 100, step 44/80, loss = 0.0597\n",
      "epoch 45 / 100, step 45/80, loss = 0.0553\n",
      "epoch 45 / 100, step 46/80, loss = 0.0611\n",
      "epoch 45 / 100, step 47/80, loss = 0.0634\n",
      "epoch 45 / 100, step 48/80, loss = 0.0586\n",
      "epoch 45 / 100, step 49/80, loss = 0.0567\n",
      "epoch 45 / 100, step 50/80, loss = 0.0613\n",
      "epoch 45 / 100, step 51/80, loss = 0.0635\n",
      "epoch 45 / 100, step 52/80, loss = 0.0585\n",
      "epoch 45 / 100, step 53/80, loss = 0.0610\n",
      "epoch 45 / 100, step 54/80, loss = 0.0580\n",
      "epoch 45 / 100, step 55/80, loss = 0.0534\n",
      "epoch 45 / 100, step 56/80, loss = 0.0576\n",
      "epoch 45 / 100, step 57/80, loss = 0.0611\n",
      "epoch 45 / 100, step 58/80, loss = 0.0581\n",
      "epoch 45 / 100, step 59/80, loss = 0.0553\n",
      "epoch 45 / 100, step 60/80, loss = 0.0604\n",
      "epoch 45 / 100, step 61/80, loss = 0.0631\n",
      "epoch 45 / 100, step 62/80, loss = 0.0618\n",
      "epoch 45 / 100, step 63/80, loss = 0.0602\n",
      "epoch 45 / 100, step 64/80, loss = 0.0626\n",
      "epoch 45 / 100, step 65/80, loss = 0.0570\n",
      "epoch 45 / 100, step 66/80, loss = 0.0561\n",
      "epoch 45 / 100, step 67/80, loss = 0.0592\n",
      "epoch 45 / 100, step 68/80, loss = 0.0584\n",
      "epoch 45 / 100, step 69/80, loss = 0.0581\n",
      "epoch 45 / 100, step 70/80, loss = 0.0563\n",
      "epoch 45 / 100, step 71/80, loss = 0.0716\n",
      "epoch 45 / 100, step 72/80, loss = 0.0599\n",
      "epoch 45 / 100, step 73/80, loss = 0.0619\n",
      "epoch 45 / 100, step 74/80, loss = 0.0611\n",
      "epoch 45 / 100, step 75/80, loss = 0.0581\n",
      "epoch 45 / 100, step 76/80, loss = 0.0575\n",
      "epoch 45 / 100, step 77/80, loss = 0.0601\n",
      "epoch 45 / 100, step 78/80, loss = 0.0614\n",
      "epoch 45 / 100, step 79/80, loss = 0.0558\n",
      "epoch 45 / 100, step 80/80, loss = 0.0555\n",
      "epoch 46 / 100, step 1/80, loss = 0.0551\n",
      "epoch 46 / 100, step 2/80, loss = 0.0531\n",
      "epoch 46 / 100, step 3/80, loss = 0.0569\n",
      "epoch 46 / 100, step 4/80, loss = 0.0561\n",
      "epoch 46 / 100, step 5/80, loss = 0.0542\n",
      "epoch 46 / 100, step 6/80, loss = 0.0572\n",
      "epoch 46 / 100, step 7/80, loss = 0.0576\n",
      "epoch 46 / 100, step 8/80, loss = 0.0609\n",
      "epoch 46 / 100, step 9/80, loss = 0.0573\n",
      "epoch 46 / 100, step 10/80, loss = 0.0553\n",
      "epoch 46 / 100, step 11/80, loss = 0.0597\n",
      "epoch 46 / 100, step 12/80, loss = 0.0583\n",
      "epoch 46 / 100, step 13/80, loss = 0.0622\n",
      "epoch 46 / 100, step 14/80, loss = 0.0561\n",
      "epoch 46 / 100, step 15/80, loss = 0.0572\n",
      "epoch 46 / 100, step 16/80, loss = 0.0567\n",
      "epoch 46 / 100, step 17/80, loss = 0.0570\n",
      "epoch 46 / 100, step 18/80, loss = 0.0550\n",
      "epoch 46 / 100, step 19/80, loss = 0.0618\n",
      "epoch 46 / 100, step 20/80, loss = 0.0557\n",
      "epoch 46 / 100, step 21/80, loss = 0.0538\n",
      "epoch 46 / 100, step 22/80, loss = 0.0582\n",
      "epoch 46 / 100, step 23/80, loss = 0.0572\n",
      "epoch 46 / 100, step 24/80, loss = 0.0558\n",
      "epoch 46 / 100, step 25/80, loss = 0.0598\n",
      "epoch 46 / 100, step 26/80, loss = 0.0603\n",
      "epoch 46 / 100, step 27/80, loss = 0.0595\n",
      "epoch 46 / 100, step 28/80, loss = 0.0577\n",
      "epoch 46 / 100, step 29/80, loss = 0.0551\n",
      "epoch 46 / 100, step 30/80, loss = 0.0632\n",
      "epoch 46 / 100, step 31/80, loss = 0.0599\n",
      "epoch 46 / 100, step 32/80, loss = 0.0559\n",
      "epoch 46 / 100, step 33/80, loss = 0.0587\n",
      "epoch 46 / 100, step 34/80, loss = 0.0578\n",
      "epoch 46 / 100, step 35/80, loss = 0.0581\n",
      "epoch 46 / 100, step 36/80, loss = 0.0595\n",
      "epoch 46 / 100, step 37/80, loss = 0.0576\n",
      "epoch 46 / 100, step 38/80, loss = 0.0612\n",
      "epoch 46 / 100, step 39/80, loss = 0.0646\n",
      "epoch 46 / 100, step 40/80, loss = 0.0602\n",
      "epoch 46 / 100, step 41/80, loss = 0.0570\n",
      "epoch 46 / 100, step 42/80, loss = 0.0582\n",
      "epoch 46 / 100, step 43/80, loss = 0.0570\n",
      "epoch 46 / 100, step 44/80, loss = 0.0563\n",
      "epoch 46 / 100, step 45/80, loss = 0.0592\n",
      "epoch 46 / 100, step 46/80, loss = 0.0582\n",
      "epoch 46 / 100, step 47/80, loss = 0.0592\n",
      "epoch 46 / 100, step 48/80, loss = 0.0586\n",
      "epoch 46 / 100, step 49/80, loss = 0.0576\n",
      "epoch 46 / 100, step 50/80, loss = 0.0624\n",
      "epoch 46 / 100, step 51/80, loss = 0.0577\n",
      "epoch 46 / 100, step 52/80, loss = 0.0565\n",
      "epoch 46 / 100, step 53/80, loss = 0.0607\n",
      "epoch 46 / 100, step 54/80, loss = 0.0554\n",
      "epoch 46 / 100, step 55/80, loss = 0.0573\n",
      "epoch 46 / 100, step 56/80, loss = 0.0578\n",
      "epoch 46 / 100, step 57/80, loss = 0.0598\n",
      "epoch 46 / 100, step 58/80, loss = 0.0588\n",
      "epoch 46 / 100, step 59/80, loss = 0.0526\n",
      "epoch 46 / 100, step 60/80, loss = 0.0624\n",
      "epoch 46 / 100, step 61/80, loss = 0.0615\n",
      "epoch 46 / 100, step 62/80, loss = 0.0606\n",
      "epoch 46 / 100, step 63/80, loss = 0.0553\n",
      "epoch 46 / 100, step 64/80, loss = 0.0600\n",
      "epoch 46 / 100, step 65/80, loss = 0.0565\n",
      "epoch 46 / 100, step 66/80, loss = 0.0538\n",
      "epoch 46 / 100, step 67/80, loss = 0.0628\n",
      "epoch 46 / 100, step 68/80, loss = 0.0597\n",
      "epoch 46 / 100, step 69/80, loss = 0.0553\n",
      "epoch 46 / 100, step 70/80, loss = 0.0586\n",
      "epoch 46 / 100, step 71/80, loss = 0.0576\n",
      "epoch 46 / 100, step 72/80, loss = 0.0590\n",
      "epoch 46 / 100, step 73/80, loss = 0.0587\n",
      "epoch 46 / 100, step 74/80, loss = 0.0640\n",
      "epoch 46 / 100, step 75/80, loss = 0.0574\n",
      "epoch 46 / 100, step 76/80, loss = 0.0558\n",
      "epoch 46 / 100, step 77/80, loss = 0.0604\n",
      "epoch 46 / 100, step 78/80, loss = 0.0609\n",
      "epoch 46 / 100, step 79/80, loss = 0.0674\n",
      "epoch 46 / 100, step 80/80, loss = 0.0555\n",
      "epoch 47 / 100, step 1/80, loss = 0.0627\n",
      "epoch 47 / 100, step 2/80, loss = 0.0601\n",
      "epoch 47 / 100, step 3/80, loss = 0.0550\n",
      "epoch 47 / 100, step 4/80, loss = 0.0574\n",
      "epoch 47 / 100, step 5/80, loss = 0.0603\n",
      "epoch 47 / 100, step 6/80, loss = 0.0615\n",
      "epoch 47 / 100, step 7/80, loss = 0.0545\n",
      "epoch 47 / 100, step 8/80, loss = 0.0546\n",
      "epoch 47 / 100, step 9/80, loss = 0.0554\n",
      "epoch 47 / 100, step 10/80, loss = 0.0585\n",
      "epoch 47 / 100, step 11/80, loss = 0.0613\n",
      "epoch 47 / 100, step 12/80, loss = 0.0578\n",
      "epoch 47 / 100, step 13/80, loss = 0.0565\n",
      "epoch 47 / 100, step 14/80, loss = 0.0560\n",
      "epoch 47 / 100, step 15/80, loss = 0.0542\n",
      "epoch 47 / 100, step 16/80, loss = 0.0557\n",
      "epoch 47 / 100, step 17/80, loss = 0.0635\n",
      "epoch 47 / 100, step 18/80, loss = 0.0617\n",
      "epoch 47 / 100, step 19/80, loss = 0.0553\n",
      "epoch 47 / 100, step 20/80, loss = 0.0600\n",
      "epoch 47 / 100, step 21/80, loss = 0.0537\n",
      "epoch 47 / 100, step 22/80, loss = 0.0577\n",
      "epoch 47 / 100, step 23/80, loss = 0.0550\n",
      "epoch 47 / 100, step 24/80, loss = 0.0535\n",
      "epoch 47 / 100, step 25/80, loss = 0.0580\n",
      "epoch 47 / 100, step 26/80, loss = 0.0576\n",
      "epoch 47 / 100, step 27/80, loss = 0.0543\n",
      "epoch 47 / 100, step 28/80, loss = 0.0539\n",
      "epoch 47 / 100, step 29/80, loss = 0.0549\n",
      "epoch 47 / 100, step 30/80, loss = 0.0587\n",
      "epoch 47 / 100, step 31/80, loss = 0.0556\n",
      "epoch 47 / 100, step 32/80, loss = 0.0556\n",
      "epoch 47 / 100, step 33/80, loss = 0.0595\n",
      "epoch 47 / 100, step 34/80, loss = 0.0565\n",
      "epoch 47 / 100, step 35/80, loss = 0.0551\n",
      "epoch 47 / 100, step 36/80, loss = 0.0578\n",
      "epoch 47 / 100, step 37/80, loss = 0.0558\n",
      "epoch 47 / 100, step 38/80, loss = 0.0543\n",
      "epoch 47 / 100, step 39/80, loss = 0.0521\n",
      "epoch 47 / 100, step 40/80, loss = 0.0580\n",
      "epoch 47 / 100, step 41/80, loss = 0.0535\n",
      "epoch 47 / 100, step 42/80, loss = 0.0562\n",
      "epoch 47 / 100, step 43/80, loss = 0.0578\n",
      "epoch 47 / 100, step 44/80, loss = 0.0563\n",
      "epoch 47 / 100, step 45/80, loss = 0.0575\n",
      "epoch 47 / 100, step 46/80, loss = 0.0586\n",
      "epoch 47 / 100, step 47/80, loss = 0.0569\n",
      "epoch 47 / 100, step 48/80, loss = 0.0587\n",
      "epoch 47 / 100, step 49/80, loss = 0.0579\n",
      "epoch 47 / 100, step 50/80, loss = 0.0556\n",
      "epoch 47 / 100, step 51/80, loss = 0.0566\n",
      "epoch 47 / 100, step 52/80, loss = 0.0565\n",
      "epoch 47 / 100, step 53/80, loss = 0.0561\n",
      "epoch 47 / 100, step 54/80, loss = 0.0571\n",
      "epoch 47 / 100, step 55/80, loss = 0.0518\n",
      "epoch 47 / 100, step 56/80, loss = 0.0561\n",
      "epoch 47 / 100, step 57/80, loss = 0.0590\n",
      "epoch 47 / 100, step 58/80, loss = 0.0595\n",
      "epoch 47 / 100, step 59/80, loss = 0.0562\n",
      "epoch 47 / 100, step 60/80, loss = 0.0583\n",
      "epoch 47 / 100, step 61/80, loss = 0.0574\n",
      "epoch 47 / 100, step 62/80, loss = 0.0599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47 / 100, step 63/80, loss = 0.0581\n",
      "epoch 47 / 100, step 64/80, loss = 0.0535\n",
      "epoch 47 / 100, step 65/80, loss = 0.0564\n",
      "epoch 47 / 100, step 66/80, loss = 0.0599\n",
      "epoch 47 / 100, step 67/80, loss = 0.0518\n",
      "epoch 47 / 100, step 68/80, loss = 0.0592\n",
      "epoch 47 / 100, step 69/80, loss = 0.0514\n",
      "epoch 47 / 100, step 70/80, loss = 0.0543\n",
      "epoch 47 / 100, step 71/80, loss = 0.0595\n",
      "epoch 47 / 100, step 72/80, loss = 0.0578\n",
      "epoch 47 / 100, step 73/80, loss = 0.0591\n",
      "epoch 47 / 100, step 74/80, loss = 0.0552\n",
      "epoch 47 / 100, step 75/80, loss = 0.0544\n",
      "epoch 47 / 100, step 76/80, loss = 0.0518\n",
      "epoch 47 / 100, step 77/80, loss = 0.0598\n",
      "epoch 47 / 100, step 78/80, loss = 0.0536\n",
      "epoch 47 / 100, step 79/80, loss = 0.0691\n",
      "epoch 47 / 100, step 80/80, loss = 0.0522\n",
      "epoch 48 / 100, step 1/80, loss = 0.0548\n",
      "epoch 48 / 100, step 2/80, loss = 0.0516\n",
      "epoch 48 / 100, step 3/80, loss = 0.0536\n",
      "epoch 48 / 100, step 4/80, loss = 0.0499\n",
      "epoch 48 / 100, step 5/80, loss = 0.0554\n",
      "epoch 48 / 100, step 6/80, loss = 0.0534\n",
      "epoch 48 / 100, step 7/80, loss = 0.0572\n",
      "epoch 48 / 100, step 8/80, loss = 0.0581\n",
      "epoch 48 / 100, step 9/80, loss = 0.0568\n",
      "epoch 48 / 100, step 10/80, loss = 0.0524\n",
      "epoch 48 / 100, step 11/80, loss = 0.0571\n",
      "epoch 48 / 100, step 12/80, loss = 0.0544\n",
      "epoch 48 / 100, step 13/80, loss = 0.0589\n",
      "epoch 48 / 100, step 14/80, loss = 0.0553\n",
      "epoch 48 / 100, step 15/80, loss = 0.0585\n",
      "epoch 48 / 100, step 16/80, loss = 0.0540\n",
      "epoch 48 / 100, step 17/80, loss = 0.0541\n",
      "epoch 48 / 100, step 18/80, loss = 0.0523\n",
      "epoch 48 / 100, step 19/80, loss = 0.0544\n",
      "epoch 48 / 100, step 20/80, loss = 0.0551\n",
      "epoch 48 / 100, step 21/80, loss = 0.0530\n",
      "epoch 48 / 100, step 22/80, loss = 0.0500\n",
      "epoch 48 / 100, step 23/80, loss = 0.0531\n",
      "epoch 48 / 100, step 24/80, loss = 0.0551\n",
      "epoch 48 / 100, step 25/80, loss = 0.0559\n",
      "epoch 48 / 100, step 26/80, loss = 0.0582\n",
      "epoch 48 / 100, step 27/80, loss = 0.0550\n",
      "epoch 48 / 100, step 28/80, loss = 0.0568\n",
      "epoch 48 / 100, step 29/80, loss = 0.0547\n",
      "epoch 48 / 100, step 30/80, loss = 0.0573\n",
      "epoch 48 / 100, step 31/80, loss = 0.0602\n",
      "epoch 48 / 100, step 32/80, loss = 0.0542\n",
      "epoch 48 / 100, step 33/80, loss = 0.0569\n",
      "epoch 48 / 100, step 34/80, loss = 0.0564\n",
      "epoch 48 / 100, step 35/80, loss = 0.0538\n",
      "epoch 48 / 100, step 36/80, loss = 0.0577\n",
      "epoch 48 / 100, step 37/80, loss = 0.0563\n",
      "epoch 48 / 100, step 38/80, loss = 0.0561\n",
      "epoch 48 / 100, step 39/80, loss = 0.0497\n",
      "epoch 48 / 100, step 40/80, loss = 0.0539\n",
      "epoch 48 / 100, step 41/80, loss = 0.0566\n",
      "epoch 48 / 100, step 42/80, loss = 0.0537\n",
      "epoch 48 / 100, step 43/80, loss = 0.0534\n",
      "epoch 48 / 100, step 44/80, loss = 0.0530\n",
      "epoch 48 / 100, step 45/80, loss = 0.0560\n",
      "epoch 48 / 100, step 46/80, loss = 0.0536\n",
      "epoch 48 / 100, step 47/80, loss = 0.0555\n",
      "epoch 48 / 100, step 48/80, loss = 0.0653\n",
      "epoch 48 / 100, step 49/80, loss = 0.0585\n",
      "epoch 48 / 100, step 50/80, loss = 0.0575\n",
      "epoch 48 / 100, step 51/80, loss = 0.0555\n",
      "epoch 48 / 100, step 52/80, loss = 0.0581\n",
      "epoch 48 / 100, step 53/80, loss = 0.0548\n",
      "epoch 48 / 100, step 54/80, loss = 0.0563\n",
      "epoch 48 / 100, step 55/80, loss = 0.0533\n",
      "epoch 48 / 100, step 56/80, loss = 0.0499\n",
      "epoch 48 / 100, step 57/80, loss = 0.0522\n",
      "epoch 48 / 100, step 58/80, loss = 0.0524\n",
      "epoch 48 / 100, step 59/80, loss = 0.0574\n",
      "epoch 48 / 100, step 60/80, loss = 0.0528\n",
      "epoch 48 / 100, step 61/80, loss = 0.0570\n",
      "epoch 48 / 100, step 62/80, loss = 0.0544\n",
      "epoch 48 / 100, step 63/80, loss = 0.0574\n",
      "epoch 48 / 100, step 64/80, loss = 0.0527\n",
      "epoch 48 / 100, step 65/80, loss = 0.0631\n",
      "epoch 48 / 100, step 66/80, loss = 0.0554\n",
      "epoch 48 / 100, step 67/80, loss = 0.0586\n",
      "epoch 48 / 100, step 68/80, loss = 0.0587\n",
      "epoch 48 / 100, step 69/80, loss = 0.0544\n",
      "epoch 48 / 100, step 70/80, loss = 0.0594\n",
      "epoch 48 / 100, step 71/80, loss = 0.0571\n",
      "epoch 48 / 100, step 72/80, loss = 0.0574\n",
      "epoch 48 / 100, step 73/80, loss = 0.0567\n",
      "epoch 48 / 100, step 74/80, loss = 0.0540\n",
      "epoch 48 / 100, step 75/80, loss = 0.0497\n",
      "epoch 48 / 100, step 76/80, loss = 0.0572\n",
      "epoch 48 / 100, step 77/80, loss = 0.0582\n",
      "epoch 48 / 100, step 78/80, loss = 0.0535\n",
      "epoch 48 / 100, step 79/80, loss = 0.0573\n",
      "epoch 48 / 100, step 80/80, loss = 0.0538\n",
      "epoch 49 / 100, step 1/80, loss = 0.0545\n",
      "epoch 49 / 100, step 2/80, loss = 0.0497\n",
      "epoch 49 / 100, step 3/80, loss = 0.0552\n",
      "epoch 49 / 100, step 4/80, loss = 0.0538\n",
      "epoch 49 / 100, step 5/80, loss = 0.0530\n",
      "epoch 49 / 100, step 6/80, loss = 0.0560\n",
      "epoch 49 / 100, step 7/80, loss = 0.0534\n",
      "epoch 49 / 100, step 8/80, loss = 0.0569\n",
      "epoch 49 / 100, step 9/80, loss = 0.0624\n",
      "epoch 49 / 100, step 10/80, loss = 0.0516\n",
      "epoch 49 / 100, step 11/80, loss = 0.0569\n",
      "epoch 49 / 100, step 12/80, loss = 0.0556\n",
      "epoch 49 / 100, step 13/80, loss = 0.0578\n",
      "epoch 49 / 100, step 14/80, loss = 0.0548\n",
      "epoch 49 / 100, step 15/80, loss = 0.0534\n",
      "epoch 49 / 100, step 16/80, loss = 0.0568\n",
      "epoch 49 / 100, step 17/80, loss = 0.0594\n",
      "epoch 49 / 100, step 18/80, loss = 0.0579\n",
      "epoch 49 / 100, step 19/80, loss = 0.0517\n",
      "epoch 49 / 100, step 20/80, loss = 0.0564\n",
      "epoch 49 / 100, step 21/80, loss = 0.0535\n",
      "epoch 49 / 100, step 22/80, loss = 0.0537\n",
      "epoch 49 / 100, step 23/80, loss = 0.0515\n",
      "epoch 49 / 100, step 24/80, loss = 0.0571\n",
      "epoch 49 / 100, step 25/80, loss = 0.0508\n",
      "epoch 49 / 100, step 26/80, loss = 0.0531\n",
      "epoch 49 / 100, step 27/80, loss = 0.0529\n",
      "epoch 49 / 100, step 28/80, loss = 0.0518\n",
      "epoch 49 / 100, step 29/80, loss = 0.0544\n",
      "epoch 49 / 100, step 30/80, loss = 0.0540\n",
      "epoch 49 / 100, step 31/80, loss = 0.0502\n",
      "epoch 49 / 100, step 32/80, loss = 0.0561\n",
      "epoch 49 / 100, step 33/80, loss = 0.0498\n",
      "epoch 49 / 100, step 34/80, loss = 0.0586\n",
      "epoch 49 / 100, step 35/80, loss = 0.0528\n",
      "epoch 49 / 100, step 36/80, loss = 0.0507\n",
      "epoch 49 / 100, step 37/80, loss = 0.0539\n",
      "epoch 49 / 100, step 38/80, loss = 0.0491\n",
      "epoch 49 / 100, step 39/80, loss = 0.0533\n",
      "epoch 49 / 100, step 40/80, loss = 0.0521\n",
      "epoch 49 / 100, step 41/80, loss = 0.0596\n",
      "epoch 49 / 100, step 42/80, loss = 0.0505\n",
      "epoch 49 / 100, step 43/80, loss = 0.0504\n",
      "epoch 49 / 100, step 44/80, loss = 0.0579\n",
      "epoch 49 / 100, step 45/80, loss = 0.0516\n",
      "epoch 49 / 100, step 46/80, loss = 0.0529\n",
      "epoch 49 / 100, step 47/80, loss = 0.0547\n",
      "epoch 49 / 100, step 48/80, loss = 0.0532\n",
      "epoch 49 / 100, step 49/80, loss = 0.0534\n",
      "epoch 49 / 100, step 50/80, loss = 0.0530\n",
      "epoch 49 / 100, step 51/80, loss = 0.0530\n",
      "epoch 49 / 100, step 52/80, loss = 0.0532\n",
      "epoch 49 / 100, step 53/80, loss = 0.0563\n",
      "epoch 49 / 100, step 54/80, loss = 0.0531\n",
      "epoch 49 / 100, step 55/80, loss = 0.0523\n",
      "epoch 49 / 100, step 56/80, loss = 0.0560\n",
      "epoch 49 / 100, step 57/80, loss = 0.0647\n",
      "epoch 49 / 100, step 58/80, loss = 0.0587\n",
      "epoch 49 / 100, step 59/80, loss = 0.0541\n",
      "epoch 49 / 100, step 60/80, loss = 0.0558\n",
      "epoch 49 / 100, step 61/80, loss = 0.0534\n",
      "epoch 49 / 100, step 62/80, loss = 0.0559\n",
      "epoch 49 / 100, step 63/80, loss = 0.0643\n",
      "epoch 49 / 100, step 64/80, loss = 0.0506\n",
      "epoch 49 / 100, step 65/80, loss = 0.0573\n",
      "epoch 49 / 100, step 66/80, loss = 0.0563\n",
      "epoch 49 / 100, step 67/80, loss = 0.0501\n",
      "epoch 49 / 100, step 68/80, loss = 0.0535\n",
      "epoch 49 / 100, step 69/80, loss = 0.0556\n",
      "epoch 49 / 100, step 70/80, loss = 0.0529\n",
      "epoch 49 / 100, step 71/80, loss = 0.0565\n",
      "epoch 49 / 100, step 72/80, loss = 0.0556\n",
      "epoch 49 / 100, step 73/80, loss = 0.0530\n",
      "epoch 49 / 100, step 74/80, loss = 0.0503\n",
      "epoch 49 / 100, step 75/80, loss = 0.0543\n",
      "epoch 49 / 100, step 76/80, loss = 0.0551\n",
      "epoch 49 / 100, step 77/80, loss = 0.0547\n",
      "epoch 49 / 100, step 78/80, loss = 0.0543\n",
      "epoch 49 / 100, step 79/80, loss = 0.0536\n",
      "epoch 49 / 100, step 80/80, loss = 0.0566\n",
      "epoch 50 / 100, step 1/80, loss = 0.0499\n",
      "epoch 50 / 100, step 2/80, loss = 0.0520\n",
      "epoch 50 / 100, step 3/80, loss = 0.0509\n",
      "epoch 50 / 100, step 4/80, loss = 0.0523\n",
      "epoch 50 / 100, step 5/80, loss = 0.0553\n",
      "epoch 50 / 100, step 6/80, loss = 0.0536\n",
      "epoch 50 / 100, step 7/80, loss = 0.0528\n",
      "epoch 50 / 100, step 8/80, loss = 0.0550\n",
      "epoch 50 / 100, step 9/80, loss = 0.0522\n",
      "epoch 50 / 100, step 10/80, loss = 0.0514\n",
      "epoch 50 / 100, step 11/80, loss = 0.0609\n",
      "epoch 50 / 100, step 12/80, loss = 0.0584\n",
      "epoch 50 / 100, step 13/80, loss = 0.0483\n",
      "epoch 50 / 100, step 14/80, loss = 0.0537\n",
      "epoch 50 / 100, step 15/80, loss = 0.0537\n",
      "epoch 50 / 100, step 16/80, loss = 0.0597\n",
      "epoch 50 / 100, step 17/80, loss = 0.0522\n",
      "epoch 50 / 100, step 18/80, loss = 0.0502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 / 100, step 19/80, loss = 0.0522\n",
      "epoch 50 / 100, step 20/80, loss = 0.0613\n",
      "epoch 50 / 100, step 21/80, loss = 0.0557\n",
      "epoch 50 / 100, step 22/80, loss = 0.0492\n",
      "epoch 50 / 100, step 23/80, loss = 0.0521\n",
      "epoch 50 / 100, step 24/80, loss = 0.0527\n",
      "epoch 50 / 100, step 25/80, loss = 0.0528\n",
      "epoch 50 / 100, step 26/80, loss = 0.0521\n",
      "epoch 50 / 100, step 27/80, loss = 0.0523\n",
      "epoch 50 / 100, step 28/80, loss = 0.0580\n",
      "epoch 50 / 100, step 29/80, loss = 0.0558\n",
      "epoch 50 / 100, step 30/80, loss = 0.0511\n",
      "epoch 50 / 100, step 31/80, loss = 0.0493\n",
      "epoch 50 / 100, step 32/80, loss = 0.0532\n",
      "epoch 50 / 100, step 33/80, loss = 0.0549\n",
      "epoch 50 / 100, step 34/80, loss = 0.0528\n",
      "epoch 50 / 100, step 35/80, loss = 0.0503\n",
      "epoch 50 / 100, step 36/80, loss = 0.0585\n",
      "epoch 50 / 100, step 37/80, loss = 0.0495\n",
      "epoch 50 / 100, step 38/80, loss = 0.0609\n",
      "epoch 50 / 100, step 39/80, loss = 0.0499\n",
      "epoch 50 / 100, step 40/80, loss = 0.0575\n",
      "epoch 50 / 100, step 41/80, loss = 0.0517\n",
      "epoch 50 / 100, step 42/80, loss = 0.0543\n",
      "epoch 50 / 100, step 43/80, loss = 0.0488\n",
      "epoch 50 / 100, step 44/80, loss = 0.0516\n",
      "epoch 50 / 100, step 45/80, loss = 0.0535\n",
      "epoch 50 / 100, step 46/80, loss = 0.0518\n",
      "epoch 50 / 100, step 47/80, loss = 0.0575\n",
      "epoch 50 / 100, step 48/80, loss = 0.0555\n",
      "epoch 50 / 100, step 49/80, loss = 0.0534\n",
      "epoch 50 / 100, step 50/80, loss = 0.0571\n",
      "epoch 50 / 100, step 51/80, loss = 0.0545\n",
      "epoch 50 / 100, step 52/80, loss = 0.0515\n",
      "epoch 50 / 100, step 53/80, loss = 0.0514\n",
      "epoch 50 / 100, step 54/80, loss = 0.0518\n",
      "epoch 50 / 100, step 55/80, loss = 0.0500\n",
      "epoch 50 / 100, step 56/80, loss = 0.0517\n",
      "epoch 50 / 100, step 57/80, loss = 0.0506\n",
      "epoch 50 / 100, step 58/80, loss = 0.0519\n",
      "epoch 50 / 100, step 59/80, loss = 0.0587\n",
      "epoch 50 / 100, step 60/80, loss = 0.0552\n",
      "epoch 50 / 100, step 61/80, loss = 0.0505\n",
      "epoch 50 / 100, step 62/80, loss = 0.0520\n",
      "epoch 50 / 100, step 63/80, loss = 0.0511\n",
      "epoch 50 / 100, step 64/80, loss = 0.0569\n",
      "epoch 50 / 100, step 65/80, loss = 0.0517\n",
      "epoch 50 / 100, step 66/80, loss = 0.0595\n",
      "epoch 50 / 100, step 67/80, loss = 0.0561\n",
      "epoch 50 / 100, step 68/80, loss = 0.0504\n",
      "epoch 50 / 100, step 69/80, loss = 0.0604\n",
      "epoch 50 / 100, step 70/80, loss = 0.0528\n",
      "epoch 50 / 100, step 71/80, loss = 0.0548\n",
      "epoch 50 / 100, step 72/80, loss = 0.0542\n",
      "epoch 50 / 100, step 73/80, loss = 0.0513\n",
      "epoch 50 / 100, step 74/80, loss = 0.0491\n",
      "epoch 50 / 100, step 75/80, loss = 0.0559\n",
      "epoch 50 / 100, step 76/80, loss = 0.0546\n",
      "epoch 50 / 100, step 77/80, loss = 0.0552\n",
      "epoch 50 / 100, step 78/80, loss = 0.0534\n",
      "epoch 50 / 100, step 79/80, loss = 0.0540\n",
      "epoch 50 / 100, step 80/80, loss = 0.0536\n",
      "epoch 51 / 100, step 1/80, loss = 0.0495\n",
      "epoch 51 / 100, step 2/80, loss = 0.0511\n",
      "epoch 51 / 100, step 3/80, loss = 0.0502\n",
      "epoch 51 / 100, step 4/80, loss = 0.0530\n",
      "epoch 51 / 100, step 5/80, loss = 0.0462\n",
      "epoch 51 / 100, step 6/80, loss = 0.0507\n",
      "epoch 51 / 100, step 7/80, loss = 0.0554\n",
      "epoch 51 / 100, step 8/80, loss = 0.0519\n",
      "epoch 51 / 100, step 9/80, loss = 0.0543\n",
      "epoch 51 / 100, step 10/80, loss = 0.0570\n",
      "epoch 51 / 100, step 11/80, loss = 0.0516\n",
      "epoch 51 / 100, step 12/80, loss = 0.0503\n",
      "epoch 51 / 100, step 13/80, loss = 0.0470\n",
      "epoch 51 / 100, step 14/80, loss = 0.0520\n",
      "epoch 51 / 100, step 15/80, loss = 0.0505\n",
      "epoch 51 / 100, step 16/80, loss = 0.0516\n",
      "epoch 51 / 100, step 17/80, loss = 0.0501\n",
      "epoch 51 / 100, step 18/80, loss = 0.0569\n",
      "epoch 51 / 100, step 19/80, loss = 0.0506\n",
      "epoch 51 / 100, step 20/80, loss = 0.0514\n",
      "epoch 51 / 100, step 21/80, loss = 0.0494\n",
      "epoch 51 / 100, step 22/80, loss = 0.0517\n",
      "epoch 51 / 100, step 23/80, loss = 0.0534\n",
      "epoch 51 / 100, step 24/80, loss = 0.0488\n",
      "epoch 51 / 100, step 25/80, loss = 0.0555\n",
      "epoch 51 / 100, step 26/80, loss = 0.0507\n",
      "epoch 51 / 100, step 27/80, loss = 0.0492\n",
      "epoch 51 / 100, step 28/80, loss = 0.0480\n",
      "epoch 51 / 100, step 29/80, loss = 0.0582\n",
      "epoch 51 / 100, step 30/80, loss = 0.0486\n",
      "epoch 51 / 100, step 31/80, loss = 0.0515\n",
      "epoch 51 / 100, step 32/80, loss = 0.0521\n",
      "epoch 51 / 100, step 33/80, loss = 0.0535\n",
      "epoch 51 / 100, step 34/80, loss = 0.0548\n",
      "epoch 51 / 100, step 35/80, loss = 0.0586\n",
      "epoch 51 / 100, step 36/80, loss = 0.0570\n",
      "epoch 51 / 100, step 37/80, loss = 0.0525\n",
      "epoch 51 / 100, step 38/80, loss = 0.0528\n",
      "epoch 51 / 100, step 39/80, loss = 0.0537\n",
      "epoch 51 / 100, step 40/80, loss = 0.0516\n",
      "epoch 51 / 100, step 41/80, loss = 0.0491\n",
      "epoch 51 / 100, step 42/80, loss = 0.0499\n",
      "epoch 51 / 100, step 43/80, loss = 0.0516\n",
      "epoch 51 / 100, step 44/80, loss = 0.0523\n",
      "epoch 51 / 100, step 45/80, loss = 0.0518\n",
      "epoch 51 / 100, step 46/80, loss = 0.0576\n",
      "epoch 51 / 100, step 47/80, loss = 0.0581\n",
      "epoch 51 / 100, step 48/80, loss = 0.0606\n",
      "epoch 51 / 100, step 49/80, loss = 0.0506\n",
      "epoch 51 / 100, step 50/80, loss = 0.0511\n",
      "epoch 51 / 100, step 51/80, loss = 0.0537\n",
      "epoch 51 / 100, step 52/80, loss = 0.0548\n",
      "epoch 51 / 100, step 53/80, loss = 0.0554\n",
      "epoch 51 / 100, step 54/80, loss = 0.0492\n",
      "epoch 51 / 100, step 55/80, loss = 0.0560\n",
      "epoch 51 / 100, step 56/80, loss = 0.0527\n",
      "epoch 51 / 100, step 57/80, loss = 0.0522\n",
      "epoch 51 / 100, step 58/80, loss = 0.0512\n",
      "epoch 51 / 100, step 59/80, loss = 0.0543\n",
      "epoch 51 / 100, step 60/80, loss = 0.0536\n",
      "epoch 51 / 100, step 61/80, loss = 0.0555\n",
      "epoch 51 / 100, step 62/80, loss = 0.0546\n",
      "epoch 51 / 100, step 63/80, loss = 0.0551\n",
      "epoch 51 / 100, step 64/80, loss = 0.0554\n",
      "epoch 51 / 100, step 65/80, loss = 0.0510\n",
      "epoch 51 / 100, step 66/80, loss = 0.0483\n",
      "epoch 51 / 100, step 67/80, loss = 0.0537\n",
      "epoch 51 / 100, step 68/80, loss = 0.0575\n",
      "epoch 51 / 100, step 69/80, loss = 0.0544\n",
      "epoch 51 / 100, step 70/80, loss = 0.0573\n",
      "epoch 51 / 100, step 71/80, loss = 0.0473\n",
      "epoch 51 / 100, step 72/80, loss = 0.0552\n",
      "epoch 51 / 100, step 73/80, loss = 0.0497\n",
      "epoch 51 / 100, step 74/80, loss = 0.0548\n",
      "epoch 51 / 100, step 75/80, loss = 0.0522\n",
      "epoch 51 / 100, step 76/80, loss = 0.0488\n",
      "epoch 51 / 100, step 77/80, loss = 0.0499\n",
      "epoch 51 / 100, step 78/80, loss = 0.0542\n",
      "epoch 51 / 100, step 79/80, loss = 0.0576\n",
      "epoch 51 / 100, step 80/80, loss = 0.0483\n",
      "epoch 52 / 100, step 1/80, loss = 0.0479\n",
      "epoch 52 / 100, step 2/80, loss = 0.0506\n",
      "epoch 52 / 100, step 3/80, loss = 0.0554\n",
      "epoch 52 / 100, step 4/80, loss = 0.0519\n",
      "epoch 52 / 100, step 5/80, loss = 0.0516\n",
      "epoch 52 / 100, step 6/80, loss = 0.0524\n",
      "epoch 52 / 100, step 7/80, loss = 0.0506\n",
      "epoch 52 / 100, step 8/80, loss = 0.0511\n",
      "epoch 52 / 100, step 9/80, loss = 0.0521\n",
      "epoch 52 / 100, step 10/80, loss = 0.0535\n",
      "epoch 52 / 100, step 11/80, loss = 0.0468\n",
      "epoch 52 / 100, step 12/80, loss = 0.0514\n",
      "epoch 52 / 100, step 13/80, loss = 0.0520\n",
      "epoch 52 / 100, step 14/80, loss = 0.0508\n",
      "epoch 52 / 100, step 15/80, loss = 0.0471\n",
      "epoch 52 / 100, step 16/80, loss = 0.0591\n",
      "epoch 52 / 100, step 17/80, loss = 0.0474\n",
      "epoch 52 / 100, step 18/80, loss = 0.0521\n",
      "epoch 52 / 100, step 19/80, loss = 0.0511\n",
      "epoch 52 / 100, step 20/80, loss = 0.0595\n",
      "epoch 52 / 100, step 21/80, loss = 0.0503\n",
      "epoch 52 / 100, step 22/80, loss = 0.0517\n",
      "epoch 52 / 100, step 23/80, loss = 0.0534\n",
      "epoch 52 / 100, step 24/80, loss = 0.0487\n",
      "epoch 52 / 100, step 25/80, loss = 0.0477\n",
      "epoch 52 / 100, step 26/80, loss = 0.0540\n",
      "epoch 52 / 100, step 27/80, loss = 0.0481\n",
      "epoch 52 / 100, step 28/80, loss = 0.0512\n",
      "epoch 52 / 100, step 29/80, loss = 0.0523\n",
      "epoch 52 / 100, step 30/80, loss = 0.0514\n",
      "epoch 52 / 100, step 31/80, loss = 0.0583\n",
      "epoch 52 / 100, step 32/80, loss = 0.0507\n",
      "epoch 52 / 100, step 33/80, loss = 0.0537\n",
      "epoch 52 / 100, step 34/80, loss = 0.0504\n",
      "epoch 52 / 100, step 35/80, loss = 0.0519\n",
      "epoch 52 / 100, step 36/80, loss = 0.0506\n",
      "epoch 52 / 100, step 37/80, loss = 0.0531\n",
      "epoch 52 / 100, step 38/80, loss = 0.0492\n",
      "epoch 52 / 100, step 39/80, loss = 0.0497\n",
      "epoch 52 / 100, step 40/80, loss = 0.0497\n",
      "epoch 52 / 100, step 41/80, loss = 0.0507\n",
      "epoch 52 / 100, step 42/80, loss = 0.0478\n",
      "epoch 52 / 100, step 43/80, loss = 0.0506\n",
      "epoch 52 / 100, step 44/80, loss = 0.0539\n",
      "epoch 52 / 100, step 45/80, loss = 0.0503\n",
      "epoch 52 / 100, step 46/80, loss = 0.0482\n",
      "epoch 52 / 100, step 47/80, loss = 0.0533\n",
      "epoch 52 / 100, step 48/80, loss = 0.0459\n",
      "epoch 52 / 100, step 49/80, loss = 0.0540\n",
      "epoch 52 / 100, step 50/80, loss = 0.0546\n",
      "epoch 52 / 100, step 51/80, loss = 0.0518\n",
      "epoch 52 / 100, step 52/80, loss = 0.0497\n",
      "epoch 52 / 100, step 53/80, loss = 0.0532\n",
      "epoch 52 / 100, step 54/80, loss = 0.0557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52 / 100, step 55/80, loss = 0.0514\n",
      "epoch 52 / 100, step 56/80, loss = 0.0496\n",
      "epoch 52 / 100, step 57/80, loss = 0.0503\n",
      "epoch 52 / 100, step 58/80, loss = 0.0599\n",
      "epoch 52 / 100, step 59/80, loss = 0.0488\n",
      "epoch 52 / 100, step 60/80, loss = 0.0519\n",
      "epoch 52 / 100, step 61/80, loss = 0.0522\n",
      "epoch 52 / 100, step 62/80, loss = 0.0535\n",
      "epoch 52 / 100, step 63/80, loss = 0.0522\n",
      "epoch 52 / 100, step 64/80, loss = 0.0480\n",
      "epoch 52 / 100, step 65/80, loss = 0.0486\n",
      "epoch 52 / 100, step 66/80, loss = 0.0568\n",
      "epoch 52 / 100, step 67/80, loss = 0.0486\n",
      "epoch 52 / 100, step 68/80, loss = 0.0494\n",
      "epoch 52 / 100, step 69/80, loss = 0.0502\n",
      "epoch 52 / 100, step 70/80, loss = 0.0528\n",
      "epoch 52 / 100, step 71/80, loss = 0.0483\n",
      "epoch 52 / 100, step 72/80, loss = 0.0543\n",
      "epoch 52 / 100, step 73/80, loss = 0.0513\n",
      "epoch 52 / 100, step 74/80, loss = 0.0535\n",
      "epoch 52 / 100, step 75/80, loss = 0.0458\n",
      "epoch 52 / 100, step 76/80, loss = 0.0536\n",
      "epoch 52 / 100, step 77/80, loss = 0.0484\n",
      "epoch 52 / 100, step 78/80, loss = 0.0515\n",
      "epoch 52 / 100, step 79/80, loss = 0.0478\n",
      "epoch 52 / 100, step 80/80, loss = 0.0575\n",
      "epoch 53 / 100, step 1/80, loss = 0.0547\n",
      "epoch 53 / 100, step 2/80, loss = 0.0509\n",
      "epoch 53 / 100, step 3/80, loss = 0.0495\n",
      "epoch 53 / 100, step 4/80, loss = 0.0480\n",
      "epoch 53 / 100, step 5/80, loss = 0.0492\n",
      "epoch 53 / 100, step 6/80, loss = 0.0453\n",
      "epoch 53 / 100, step 7/80, loss = 0.0511\n",
      "epoch 53 / 100, step 8/80, loss = 0.0537\n",
      "epoch 53 / 100, step 9/80, loss = 0.0490\n",
      "epoch 53 / 100, step 10/80, loss = 0.0481\n",
      "epoch 53 / 100, step 11/80, loss = 0.0491\n",
      "epoch 53 / 100, step 12/80, loss = 0.0512\n",
      "epoch 53 / 100, step 13/80, loss = 0.0486\n",
      "epoch 53 / 100, step 14/80, loss = 0.0495\n",
      "epoch 53 / 100, step 15/80, loss = 0.0516\n",
      "epoch 53 / 100, step 16/80, loss = 0.0577\n",
      "epoch 53 / 100, step 17/80, loss = 0.0476\n",
      "epoch 53 / 100, step 18/80, loss = 0.0511\n",
      "epoch 53 / 100, step 19/80, loss = 0.0507\n",
      "epoch 53 / 100, step 20/80, loss = 0.0523\n",
      "epoch 53 / 100, step 21/80, loss = 0.0512\n",
      "epoch 53 / 100, step 22/80, loss = 0.0490\n",
      "epoch 53 / 100, step 23/80, loss = 0.0489\n",
      "epoch 53 / 100, step 24/80, loss = 0.0480\n",
      "epoch 53 / 100, step 25/80, loss = 0.0604\n",
      "epoch 53 / 100, step 26/80, loss = 0.0492\n",
      "epoch 53 / 100, step 27/80, loss = 0.0514\n",
      "epoch 53 / 100, step 28/80, loss = 0.0516\n",
      "epoch 53 / 100, step 29/80, loss = 0.0521\n",
      "epoch 53 / 100, step 30/80, loss = 0.0483\n",
      "epoch 53 / 100, step 31/80, loss = 0.0463\n",
      "epoch 53 / 100, step 32/80, loss = 0.0481\n",
      "epoch 53 / 100, step 33/80, loss = 0.0510\n",
      "epoch 53 / 100, step 34/80, loss = 0.0516\n",
      "epoch 53 / 100, step 35/80, loss = 0.0527\n",
      "epoch 53 / 100, step 36/80, loss = 0.0524\n",
      "epoch 53 / 100, step 37/80, loss = 0.0474\n",
      "epoch 53 / 100, step 38/80, loss = 0.0487\n",
      "epoch 53 / 100, step 39/80, loss = 0.0539\n",
      "epoch 53 / 100, step 40/80, loss = 0.0490\n",
      "epoch 53 / 100, step 41/80, loss = 0.0497\n",
      "epoch 53 / 100, step 42/80, loss = 0.0512\n",
      "epoch 53 / 100, step 43/80, loss = 0.0536\n",
      "epoch 53 / 100, step 44/80, loss = 0.0493\n",
      "epoch 53 / 100, step 45/80, loss = 0.0482\n",
      "epoch 53 / 100, step 46/80, loss = 0.0561\n",
      "epoch 53 / 100, step 47/80, loss = 0.0473\n",
      "epoch 53 / 100, step 48/80, loss = 0.0490\n",
      "epoch 53 / 100, step 49/80, loss = 0.0516\n",
      "epoch 53 / 100, step 50/80, loss = 0.0505\n",
      "epoch 53 / 100, step 51/80, loss = 0.0519\n",
      "epoch 53 / 100, step 52/80, loss = 0.0496\n",
      "epoch 53 / 100, step 53/80, loss = 0.0517\n",
      "epoch 53 / 100, step 54/80, loss = 0.0526\n",
      "epoch 53 / 100, step 55/80, loss = 0.0459\n",
      "epoch 53 / 100, step 56/80, loss = 0.0494\n",
      "epoch 53 / 100, step 57/80, loss = 0.0507\n",
      "epoch 53 / 100, step 58/80, loss = 0.0511\n",
      "epoch 53 / 100, step 59/80, loss = 0.0510\n",
      "epoch 53 / 100, step 60/80, loss = 0.0501\n",
      "epoch 53 / 100, step 61/80, loss = 0.0519\n",
      "epoch 53 / 100, step 62/80, loss = 0.0463\n",
      "epoch 53 / 100, step 63/80, loss = 0.0492\n",
      "epoch 53 / 100, step 64/80, loss = 0.0505\n",
      "epoch 53 / 100, step 65/80, loss = 0.0522\n",
      "epoch 53 / 100, step 66/80, loss = 0.0600\n",
      "epoch 53 / 100, step 67/80, loss = 0.0490\n",
      "epoch 53 / 100, step 68/80, loss = 0.0473\n",
      "epoch 53 / 100, step 69/80, loss = 0.0456\n",
      "epoch 53 / 100, step 70/80, loss = 0.0478\n",
      "epoch 53 / 100, step 71/80, loss = 0.0485\n",
      "epoch 53 / 100, step 72/80, loss = 0.0522\n",
      "epoch 53 / 100, step 73/80, loss = 0.0542\n",
      "epoch 53 / 100, step 74/80, loss = 0.0502\n",
      "epoch 53 / 100, step 75/80, loss = 0.0538\n",
      "epoch 53 / 100, step 76/80, loss = 0.0483\n",
      "epoch 53 / 100, step 77/80, loss = 0.0519\n",
      "epoch 53 / 100, step 78/80, loss = 0.0498\n",
      "epoch 53 / 100, step 79/80, loss = 0.0536\n",
      "epoch 53 / 100, step 80/80, loss = 0.0525\n",
      "epoch 54 / 100, step 1/80, loss = 0.0492\n",
      "epoch 54 / 100, step 2/80, loss = 0.0490\n",
      "epoch 54 / 100, step 3/80, loss = 0.0442\n",
      "epoch 54 / 100, step 4/80, loss = 0.0490\n",
      "epoch 54 / 100, step 5/80, loss = 0.0449\n",
      "epoch 54 / 100, step 6/80, loss = 0.0486\n",
      "epoch 54 / 100, step 7/80, loss = 0.0513\n",
      "epoch 54 / 100, step 8/80, loss = 0.0501\n",
      "epoch 54 / 100, step 9/80, loss = 0.0460\n",
      "epoch 54 / 100, step 10/80, loss = 0.0491\n",
      "epoch 54 / 100, step 11/80, loss = 0.0502\n",
      "epoch 54 / 100, step 12/80, loss = 0.0455\n",
      "epoch 54 / 100, step 13/80, loss = 0.0508\n",
      "epoch 54 / 100, step 14/80, loss = 0.0514\n",
      "epoch 54 / 100, step 15/80, loss = 0.0499\n",
      "epoch 54 / 100, step 16/80, loss = 0.0518\n",
      "epoch 54 / 100, step 17/80, loss = 0.0497\n",
      "epoch 54 / 100, step 18/80, loss = 0.0528\n",
      "epoch 54 / 100, step 19/80, loss = 0.0471\n",
      "epoch 54 / 100, step 20/80, loss = 0.0473\n",
      "epoch 54 / 100, step 21/80, loss = 0.0498\n",
      "epoch 54 / 100, step 22/80, loss = 0.0484\n",
      "epoch 54 / 100, step 23/80, loss = 0.0478\n",
      "epoch 54 / 100, step 24/80, loss = 0.0475\n",
      "epoch 54 / 100, step 25/80, loss = 0.0514\n",
      "epoch 54 / 100, step 26/80, loss = 0.0486\n",
      "epoch 54 / 100, step 27/80, loss = 0.0557\n",
      "epoch 54 / 100, step 28/80, loss = 0.0511\n",
      "epoch 54 / 100, step 29/80, loss = 0.0521\n",
      "epoch 54 / 100, step 30/80, loss = 0.0493\n",
      "epoch 54 / 100, step 31/80, loss = 0.0557\n",
      "epoch 54 / 100, step 32/80, loss = 0.0494\n",
      "epoch 54 / 100, step 33/80, loss = 0.0492\n",
      "epoch 54 / 100, step 34/80, loss = 0.0479\n",
      "epoch 54 / 100, step 35/80, loss = 0.0474\n",
      "epoch 54 / 100, step 36/80, loss = 0.0474\n",
      "epoch 54 / 100, step 37/80, loss = 0.0474\n",
      "epoch 54 / 100, step 38/80, loss = 0.0572\n",
      "epoch 54 / 100, step 39/80, loss = 0.0544\n",
      "epoch 54 / 100, step 40/80, loss = 0.0515\n",
      "epoch 54 / 100, step 41/80, loss = 0.0480\n",
      "epoch 54 / 100, step 42/80, loss = 0.0494\n",
      "epoch 54 / 100, step 43/80, loss = 0.0480\n",
      "epoch 54 / 100, step 44/80, loss = 0.0512\n",
      "epoch 54 / 100, step 45/80, loss = 0.0523\n",
      "epoch 54 / 100, step 46/80, loss = 0.0466\n",
      "epoch 54 / 100, step 47/80, loss = 0.0465\n",
      "epoch 54 / 100, step 48/80, loss = 0.0526\n",
      "epoch 54 / 100, step 49/80, loss = 0.0540\n",
      "epoch 54 / 100, step 50/80, loss = 0.0481\n",
      "epoch 54 / 100, step 51/80, loss = 0.0506\n",
      "epoch 54 / 100, step 52/80, loss = 0.0496\n",
      "epoch 54 / 100, step 53/80, loss = 0.0442\n",
      "epoch 54 / 100, step 54/80, loss = 0.0464\n",
      "epoch 54 / 100, step 55/80, loss = 0.0478\n",
      "epoch 54 / 100, step 56/80, loss = 0.0530\n",
      "epoch 54 / 100, step 57/80, loss = 0.0516\n",
      "epoch 54 / 100, step 58/80, loss = 0.0473\n",
      "epoch 54 / 100, step 59/80, loss = 0.0514\n",
      "epoch 54 / 100, step 60/80, loss = 0.0594\n",
      "epoch 54 / 100, step 61/80, loss = 0.0477\n",
      "epoch 54 / 100, step 62/80, loss = 0.0542\n",
      "epoch 54 / 100, step 63/80, loss = 0.0479\n",
      "epoch 54 / 100, step 64/80, loss = 0.0491\n",
      "epoch 54 / 100, step 65/80, loss = 0.0523\n",
      "epoch 54 / 100, step 66/80, loss = 0.0486\n",
      "epoch 54 / 100, step 67/80, loss = 0.0485\n",
      "epoch 54 / 100, step 68/80, loss = 0.0508\n",
      "epoch 54 / 100, step 69/80, loss = 0.0505\n",
      "epoch 54 / 100, step 70/80, loss = 0.0486\n",
      "epoch 54 / 100, step 71/80, loss = 0.0465\n",
      "epoch 54 / 100, step 72/80, loss = 0.0500\n",
      "epoch 54 / 100, step 73/80, loss = 0.0450\n",
      "epoch 54 / 100, step 74/80, loss = 0.0507\n",
      "epoch 54 / 100, step 75/80, loss = 0.0514\n",
      "epoch 54 / 100, step 76/80, loss = 0.0523\n",
      "epoch 54 / 100, step 77/80, loss = 0.0443\n",
      "epoch 54 / 100, step 78/80, loss = 0.0482\n",
      "epoch 54 / 100, step 79/80, loss = 0.0491\n",
      "epoch 54 / 100, step 80/80, loss = 0.0488\n",
      "epoch 55 / 100, step 1/80, loss = 0.0480\n",
      "epoch 55 / 100, step 2/80, loss = 0.0455\n",
      "epoch 55 / 100, step 3/80, loss = 0.0496\n",
      "epoch 55 / 100, step 4/80, loss = 0.0479\n",
      "epoch 55 / 100, step 5/80, loss = 0.0482\n",
      "epoch 55 / 100, step 6/80, loss = 0.0502\n",
      "epoch 55 / 100, step 7/80, loss = 0.0488\n",
      "epoch 55 / 100, step 8/80, loss = 0.0474\n",
      "epoch 55 / 100, step 9/80, loss = 0.0476\n",
      "epoch 55 / 100, step 10/80, loss = 0.0544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55 / 100, step 11/80, loss = 0.0492\n",
      "epoch 55 / 100, step 12/80, loss = 0.0489\n",
      "epoch 55 / 100, step 13/80, loss = 0.0449\n",
      "epoch 55 / 100, step 14/80, loss = 0.0497\n",
      "epoch 55 / 100, step 15/80, loss = 0.0525\n",
      "epoch 55 / 100, step 16/80, loss = 0.0466\n",
      "epoch 55 / 100, step 17/80, loss = 0.0462\n",
      "epoch 55 / 100, step 18/80, loss = 0.0488\n",
      "epoch 55 / 100, step 19/80, loss = 0.0486\n",
      "epoch 55 / 100, step 20/80, loss = 0.0516\n",
      "epoch 55 / 100, step 21/80, loss = 0.0516\n",
      "epoch 55 / 100, step 22/80, loss = 0.0433\n",
      "epoch 55 / 100, step 23/80, loss = 0.0481\n",
      "epoch 55 / 100, step 24/80, loss = 0.0468\n",
      "epoch 55 / 100, step 25/80, loss = 0.0469\n",
      "epoch 55 / 100, step 26/80, loss = 0.0464\n",
      "epoch 55 / 100, step 27/80, loss = 0.0486\n",
      "epoch 55 / 100, step 28/80, loss = 0.0482\n",
      "epoch 55 / 100, step 29/80, loss = 0.0489\n",
      "epoch 55 / 100, step 30/80, loss = 0.0490\n",
      "epoch 55 / 100, step 31/80, loss = 0.0485\n",
      "epoch 55 / 100, step 32/80, loss = 0.0486\n",
      "epoch 55 / 100, step 33/80, loss = 0.0513\n",
      "epoch 55 / 100, step 34/80, loss = 0.0458\n",
      "epoch 55 / 100, step 35/80, loss = 0.0468\n",
      "epoch 55 / 100, step 36/80, loss = 0.0465\n",
      "epoch 55 / 100, step 37/80, loss = 0.0460\n",
      "epoch 55 / 100, step 38/80, loss = 0.0523\n",
      "epoch 55 / 100, step 39/80, loss = 0.0500\n",
      "epoch 55 / 100, step 40/80, loss = 0.0503\n",
      "epoch 55 / 100, step 41/80, loss = 0.0455\n",
      "epoch 55 / 100, step 42/80, loss = 0.0500\n",
      "epoch 55 / 100, step 43/80, loss = 0.0509\n",
      "epoch 55 / 100, step 44/80, loss = 0.0519\n",
      "epoch 55 / 100, step 45/80, loss = 0.0474\n",
      "epoch 55 / 100, step 46/80, loss = 0.0486\n",
      "epoch 55 / 100, step 47/80, loss = 0.0474\n",
      "epoch 55 / 100, step 48/80, loss = 0.0504\n",
      "epoch 55 / 100, step 49/80, loss = 0.0517\n",
      "epoch 55 / 100, step 50/80, loss = 0.0491\n",
      "epoch 55 / 100, step 51/80, loss = 0.0493\n",
      "epoch 55 / 100, step 52/80, loss = 0.0502\n",
      "epoch 55 / 100, step 53/80, loss = 0.0505\n",
      "epoch 55 / 100, step 54/80, loss = 0.0455\n",
      "epoch 55 / 100, step 55/80, loss = 0.0496\n",
      "epoch 55 / 100, step 56/80, loss = 0.0483\n",
      "epoch 55 / 100, step 57/80, loss = 0.0474\n",
      "epoch 55 / 100, step 58/80, loss = 0.0517\n",
      "epoch 55 / 100, step 59/80, loss = 0.0484\n",
      "epoch 55 / 100, step 60/80, loss = 0.0503\n",
      "epoch 55 / 100, step 61/80, loss = 0.0464\n",
      "epoch 55 / 100, step 62/80, loss = 0.0493\n",
      "epoch 55 / 100, step 63/80, loss = 0.0514\n",
      "epoch 55 / 100, step 64/80, loss = 0.0460\n",
      "epoch 55 / 100, step 65/80, loss = 0.0530\n",
      "epoch 55 / 100, step 66/80, loss = 0.0520\n",
      "epoch 55 / 100, step 67/80, loss = 0.0523\n",
      "epoch 55 / 100, step 68/80, loss = 0.0444\n",
      "epoch 55 / 100, step 69/80, loss = 0.0451\n",
      "epoch 55 / 100, step 70/80, loss = 0.0488\n",
      "epoch 55 / 100, step 71/80, loss = 0.0479\n",
      "epoch 55 / 100, step 72/80, loss = 0.0486\n",
      "epoch 55 / 100, step 73/80, loss = 0.0514\n",
      "epoch 55 / 100, step 74/80, loss = 0.0471\n",
      "epoch 55 / 100, step 75/80, loss = 0.0477\n",
      "epoch 55 / 100, step 76/80, loss = 0.0469\n",
      "epoch 55 / 100, step 77/80, loss = 0.0482\n",
      "epoch 55 / 100, step 78/80, loss = 0.0503\n",
      "epoch 55 / 100, step 79/80, loss = 0.0508\n",
      "epoch 55 / 100, step 80/80, loss = 0.0563\n",
      "epoch 56 / 100, step 1/80, loss = 0.0458\n",
      "epoch 56 / 100, step 2/80, loss = 0.0491\n",
      "epoch 56 / 100, step 3/80, loss = 0.0493\n",
      "epoch 56 / 100, step 4/80, loss = 0.0485\n",
      "epoch 56 / 100, step 5/80, loss = 0.0461\n",
      "epoch 56 / 100, step 6/80, loss = 0.0506\n",
      "epoch 56 / 100, step 7/80, loss = 0.0489\n",
      "epoch 56 / 100, step 8/80, loss = 0.0442\n",
      "epoch 56 / 100, step 9/80, loss = 0.0474\n",
      "epoch 56 / 100, step 10/80, loss = 0.0460\n",
      "epoch 56 / 100, step 11/80, loss = 0.0465\n",
      "epoch 56 / 100, step 12/80, loss = 0.0596\n",
      "epoch 56 / 100, step 13/80, loss = 0.0479\n",
      "epoch 56 / 100, step 14/80, loss = 0.0437\n",
      "epoch 56 / 100, step 15/80, loss = 0.0463\n",
      "epoch 56 / 100, step 16/80, loss = 0.0477\n",
      "epoch 56 / 100, step 17/80, loss = 0.0491\n",
      "epoch 56 / 100, step 18/80, loss = 0.0467\n",
      "epoch 56 / 100, step 19/80, loss = 0.0470\n",
      "epoch 56 / 100, step 20/80, loss = 0.0437\n",
      "epoch 56 / 100, step 21/80, loss = 0.0535\n",
      "epoch 56 / 100, step 22/80, loss = 0.0460\n",
      "epoch 56 / 100, step 23/80, loss = 0.0524\n",
      "epoch 56 / 100, step 24/80, loss = 0.0456\n",
      "epoch 56 / 100, step 25/80, loss = 0.0449\n",
      "epoch 56 / 100, step 26/80, loss = 0.0492\n",
      "epoch 56 / 100, step 27/80, loss = 0.0454\n",
      "epoch 56 / 100, step 28/80, loss = 0.0463\n",
      "epoch 56 / 100, step 29/80, loss = 0.0491\n",
      "epoch 56 / 100, step 30/80, loss = 0.0475\n",
      "epoch 56 / 100, step 31/80, loss = 0.0482\n",
      "epoch 56 / 100, step 32/80, loss = 0.0467\n",
      "epoch 56 / 100, step 33/80, loss = 0.0479\n",
      "epoch 56 / 100, step 34/80, loss = 0.0492\n",
      "epoch 56 / 100, step 35/80, loss = 0.0499\n",
      "epoch 56 / 100, step 36/80, loss = 0.0436\n",
      "epoch 56 / 100, step 37/80, loss = 0.0483\n",
      "epoch 56 / 100, step 38/80, loss = 0.0464\n",
      "epoch 56 / 100, step 39/80, loss = 0.0496\n",
      "epoch 56 / 100, step 40/80, loss = 0.0439\n",
      "epoch 56 / 100, step 41/80, loss = 0.0498\n",
      "epoch 56 / 100, step 42/80, loss = 0.0475\n",
      "epoch 56 / 100, step 43/80, loss = 0.0449\n",
      "epoch 56 / 100, step 44/80, loss = 0.0525\n",
      "epoch 56 / 100, step 45/80, loss = 0.0469\n",
      "epoch 56 / 100, step 46/80, loss = 0.0480\n",
      "epoch 56 / 100, step 47/80, loss = 0.0501\n",
      "epoch 56 / 100, step 48/80, loss = 0.0487\n",
      "epoch 56 / 100, step 49/80, loss = 0.0474\n",
      "epoch 56 / 100, step 50/80, loss = 0.0493\n",
      "epoch 56 / 100, step 51/80, loss = 0.0490\n",
      "epoch 56 / 100, step 52/80, loss = 0.0455\n",
      "epoch 56 / 100, step 53/80, loss = 0.0444\n",
      "epoch 56 / 100, step 54/80, loss = 0.0507\n",
      "epoch 56 / 100, step 55/80, loss = 0.0475\n",
      "epoch 56 / 100, step 56/80, loss = 0.0561\n",
      "epoch 56 / 100, step 57/80, loss = 0.0488\n",
      "epoch 56 / 100, step 58/80, loss = 0.0539\n",
      "epoch 56 / 100, step 59/80, loss = 0.0495\n",
      "epoch 56 / 100, step 60/80, loss = 0.0526\n",
      "epoch 56 / 100, step 61/80, loss = 0.0493\n",
      "epoch 56 / 100, step 62/80, loss = 0.0457\n",
      "epoch 56 / 100, step 63/80, loss = 0.0464\n",
      "epoch 56 / 100, step 64/80, loss = 0.0443\n",
      "epoch 56 / 100, step 65/80, loss = 0.0451\n",
      "epoch 56 / 100, step 66/80, loss = 0.0432\n",
      "epoch 56 / 100, step 67/80, loss = 0.0513\n",
      "epoch 56 / 100, step 68/80, loss = 0.0464\n",
      "epoch 56 / 100, step 69/80, loss = 0.0486\n",
      "epoch 56 / 100, step 70/80, loss = 0.0461\n",
      "epoch 56 / 100, step 71/80, loss = 0.0479\n",
      "epoch 56 / 100, step 72/80, loss = 0.0451\n",
      "epoch 56 / 100, step 73/80, loss = 0.0439\n",
      "epoch 56 / 100, step 74/80, loss = 0.0509\n",
      "epoch 56 / 100, step 75/80, loss = 0.0449\n",
      "epoch 56 / 100, step 76/80, loss = 0.0481\n",
      "epoch 56 / 100, step 77/80, loss = 0.0481\n",
      "epoch 56 / 100, step 78/80, loss = 0.0482\n",
      "epoch 56 / 100, step 79/80, loss = 0.0500\n",
      "epoch 56 / 100, step 80/80, loss = 0.0483\n",
      "epoch 57 / 100, step 1/80, loss = 0.0444\n",
      "epoch 57 / 100, step 2/80, loss = 0.0473\n",
      "epoch 57 / 100, step 3/80, loss = 0.0483\n",
      "epoch 57 / 100, step 4/80, loss = 0.0432\n",
      "epoch 57 / 100, step 5/80, loss = 0.0427\n",
      "epoch 57 / 100, step 6/80, loss = 0.0484\n",
      "epoch 57 / 100, step 7/80, loss = 0.0502\n",
      "epoch 57 / 100, step 8/80, loss = 0.0425\n",
      "epoch 57 / 100, step 9/80, loss = 0.0447\n",
      "epoch 57 / 100, step 10/80, loss = 0.0441\n",
      "epoch 57 / 100, step 11/80, loss = 0.0490\n",
      "epoch 57 / 100, step 12/80, loss = 0.0474\n",
      "epoch 57 / 100, step 13/80, loss = 0.0496\n",
      "epoch 57 / 100, step 14/80, loss = 0.0452\n",
      "epoch 57 / 100, step 15/80, loss = 0.0492\n",
      "epoch 57 / 100, step 16/80, loss = 0.0455\n",
      "epoch 57 / 100, step 17/80, loss = 0.0476\n",
      "epoch 57 / 100, step 18/80, loss = 0.0442\n",
      "epoch 57 / 100, step 19/80, loss = 0.0442\n",
      "epoch 57 / 100, step 20/80, loss = 0.0457\n",
      "epoch 57 / 100, step 21/80, loss = 0.0500\n",
      "epoch 57 / 100, step 22/80, loss = 0.0432\n",
      "epoch 57 / 100, step 23/80, loss = 0.0433\n",
      "epoch 57 / 100, step 24/80, loss = 0.0455\n",
      "epoch 57 / 100, step 25/80, loss = 0.0448\n",
      "epoch 57 / 100, step 26/80, loss = 0.0451\n",
      "epoch 57 / 100, step 27/80, loss = 0.0452\n",
      "epoch 57 / 100, step 28/80, loss = 0.0483\n",
      "epoch 57 / 100, step 29/80, loss = 0.0473\n",
      "epoch 57 / 100, step 30/80, loss = 0.0430\n",
      "epoch 57 / 100, step 31/80, loss = 0.0404\n",
      "epoch 57 / 100, step 32/80, loss = 0.0461\n",
      "epoch 57 / 100, step 33/80, loss = 0.0470\n",
      "epoch 57 / 100, step 34/80, loss = 0.0457\n",
      "epoch 57 / 100, step 35/80, loss = 0.0446\n",
      "epoch 57 / 100, step 36/80, loss = 0.0452\n",
      "epoch 57 / 100, step 37/80, loss = 0.0503\n",
      "epoch 57 / 100, step 38/80, loss = 0.0510\n",
      "epoch 57 / 100, step 39/80, loss = 0.0418\n",
      "epoch 57 / 100, step 40/80, loss = 0.0456\n",
      "epoch 57 / 100, step 41/80, loss = 0.0453\n",
      "epoch 57 / 100, step 42/80, loss = 0.0487\n",
      "epoch 57 / 100, step 43/80, loss = 0.0453\n",
      "epoch 57 / 100, step 44/80, loss = 0.0489\n",
      "epoch 57 / 100, step 45/80, loss = 0.0457\n",
      "epoch 57 / 100, step 46/80, loss = 0.0475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57 / 100, step 47/80, loss = 0.0478\n",
      "epoch 57 / 100, step 48/80, loss = 0.0516\n",
      "epoch 57 / 100, step 49/80, loss = 0.0437\n",
      "epoch 57 / 100, step 50/80, loss = 0.0497\n",
      "epoch 57 / 100, step 51/80, loss = 0.0436\n",
      "epoch 57 / 100, step 52/80, loss = 0.0442\n",
      "epoch 57 / 100, step 53/80, loss = 0.0489\n",
      "epoch 57 / 100, step 54/80, loss = 0.0504\n",
      "epoch 57 / 100, step 55/80, loss = 0.0445\n",
      "epoch 57 / 100, step 56/80, loss = 0.0458\n",
      "epoch 57 / 100, step 57/80, loss = 0.0473\n",
      "epoch 57 / 100, step 58/80, loss = 0.0484\n",
      "epoch 57 / 100, step 59/80, loss = 0.0485\n",
      "epoch 57 / 100, step 60/80, loss = 0.0469\n",
      "epoch 57 / 100, step 61/80, loss = 0.0467\n",
      "epoch 57 / 100, step 62/80, loss = 0.0480\n",
      "epoch 57 / 100, step 63/80, loss = 0.0495\n",
      "epoch 57 / 100, step 64/80, loss = 0.0498\n",
      "epoch 57 / 100, step 65/80, loss = 0.0546\n",
      "epoch 57 / 100, step 66/80, loss = 0.0479\n",
      "epoch 57 / 100, step 67/80, loss = 0.0483\n",
      "epoch 57 / 100, step 68/80, loss = 0.0445\n",
      "epoch 57 / 100, step 69/80, loss = 0.0456\n",
      "epoch 57 / 100, step 70/80, loss = 0.0488\n",
      "epoch 57 / 100, step 71/80, loss = 0.0459\n",
      "epoch 57 / 100, step 72/80, loss = 0.0463\n",
      "epoch 57 / 100, step 73/80, loss = 0.0512\n",
      "epoch 57 / 100, step 74/80, loss = 0.0486\n",
      "epoch 57 / 100, step 75/80, loss = 0.0451\n",
      "epoch 57 / 100, step 76/80, loss = 0.0478\n",
      "epoch 57 / 100, step 77/80, loss = 0.0444\n",
      "epoch 57 / 100, step 78/80, loss = 0.0489\n",
      "epoch 57 / 100, step 79/80, loss = 0.0490\n",
      "epoch 57 / 100, step 80/80, loss = 0.0484\n",
      "epoch 58 / 100, step 1/80, loss = 0.0444\n",
      "epoch 58 / 100, step 2/80, loss = 0.0461\n",
      "epoch 58 / 100, step 3/80, loss = 0.0451\n",
      "epoch 58 / 100, step 4/80, loss = 0.0430\n",
      "epoch 58 / 100, step 5/80, loss = 0.0469\n",
      "epoch 58 / 100, step 6/80, loss = 0.0423\n",
      "epoch 58 / 100, step 7/80, loss = 0.0477\n",
      "epoch 58 / 100, step 8/80, loss = 0.0491\n",
      "epoch 58 / 100, step 9/80, loss = 0.0476\n",
      "epoch 58 / 100, step 10/80, loss = 0.0451\n",
      "epoch 58 / 100, step 11/80, loss = 0.0452\n",
      "epoch 58 / 100, step 12/80, loss = 0.0436\n",
      "epoch 58 / 100, step 13/80, loss = 0.0467\n",
      "epoch 58 / 100, step 14/80, loss = 0.0436\n",
      "epoch 58 / 100, step 15/80, loss = 0.0456\n",
      "epoch 58 / 100, step 16/80, loss = 0.0441\n",
      "epoch 58 / 100, step 17/80, loss = 0.0428\n",
      "epoch 58 / 100, step 18/80, loss = 0.0423\n",
      "epoch 58 / 100, step 19/80, loss = 0.0492\n",
      "epoch 58 / 100, step 20/80, loss = 0.0476\n",
      "epoch 58 / 100, step 21/80, loss = 0.0532\n",
      "epoch 58 / 100, step 22/80, loss = 0.0455\n",
      "epoch 58 / 100, step 23/80, loss = 0.0492\n",
      "epoch 58 / 100, step 24/80, loss = 0.0477\n",
      "epoch 58 / 100, step 25/80, loss = 0.0462\n",
      "epoch 58 / 100, step 26/80, loss = 0.0460\n",
      "epoch 58 / 100, step 27/80, loss = 0.0488\n",
      "epoch 58 / 100, step 28/80, loss = 0.0414\n",
      "epoch 58 / 100, step 29/80, loss = 0.0454\n",
      "epoch 58 / 100, step 30/80, loss = 0.0460\n",
      "epoch 58 / 100, step 31/80, loss = 0.0447\n",
      "epoch 58 / 100, step 32/80, loss = 0.0413\n",
      "epoch 58 / 100, step 33/80, loss = 0.0511\n",
      "epoch 58 / 100, step 34/80, loss = 0.0455\n",
      "epoch 58 / 100, step 35/80, loss = 0.0446\n",
      "epoch 58 / 100, step 36/80, loss = 0.0455\n",
      "epoch 58 / 100, step 37/80, loss = 0.0452\n",
      "epoch 58 / 100, step 38/80, loss = 0.0458\n",
      "epoch 58 / 100, step 39/80, loss = 0.0449\n",
      "epoch 58 / 100, step 40/80, loss = 0.0455\n",
      "epoch 58 / 100, step 41/80, loss = 0.0421\n",
      "epoch 58 / 100, step 42/80, loss = 0.0428\n",
      "epoch 58 / 100, step 43/80, loss = 0.0410\n",
      "epoch 58 / 100, step 44/80, loss = 0.0462\n",
      "epoch 58 / 100, step 45/80, loss = 0.0438\n",
      "epoch 58 / 100, step 46/80, loss = 0.0407\n",
      "epoch 58 / 100, step 47/80, loss = 0.0437\n",
      "epoch 58 / 100, step 48/80, loss = 0.0484\n",
      "epoch 58 / 100, step 49/80, loss = 0.0505\n",
      "epoch 58 / 100, step 50/80, loss = 0.0454\n",
      "epoch 58 / 100, step 51/80, loss = 0.0465\n",
      "epoch 58 / 100, step 52/80, loss = 0.0440\n",
      "epoch 58 / 100, step 53/80, loss = 0.0492\n",
      "epoch 58 / 100, step 54/80, loss = 0.0477\n",
      "epoch 58 / 100, step 55/80, loss = 0.0501\n",
      "epoch 58 / 100, step 56/80, loss = 0.0423\n",
      "epoch 58 / 100, step 57/80, loss = 0.0503\n",
      "epoch 58 / 100, step 58/80, loss = 0.0473\n",
      "epoch 58 / 100, step 59/80, loss = 0.0477\n",
      "epoch 58 / 100, step 60/80, loss = 0.0454\n",
      "epoch 58 / 100, step 61/80, loss = 0.0411\n",
      "epoch 58 / 100, step 62/80, loss = 0.0460\n",
      "epoch 58 / 100, step 63/80, loss = 0.0462\n",
      "epoch 58 / 100, step 64/80, loss = 0.0429\n",
      "epoch 58 / 100, step 65/80, loss = 0.0441\n",
      "epoch 58 / 100, step 66/80, loss = 0.0461\n",
      "epoch 58 / 100, step 67/80, loss = 0.0516\n",
      "epoch 58 / 100, step 68/80, loss = 0.0435\n",
      "epoch 58 / 100, step 69/80, loss = 0.0482\n",
      "epoch 58 / 100, step 70/80, loss = 0.0458\n",
      "epoch 58 / 100, step 71/80, loss = 0.0521\n",
      "epoch 58 / 100, step 72/80, loss = 0.0435\n",
      "epoch 58 / 100, step 73/80, loss = 0.0460\n",
      "epoch 58 / 100, step 74/80, loss = 0.0505\n",
      "epoch 58 / 100, step 75/80, loss = 0.0494\n",
      "epoch 58 / 100, step 76/80, loss = 0.0490\n",
      "epoch 58 / 100, step 77/80, loss = 0.0524\n",
      "epoch 58 / 100, step 78/80, loss = 0.0459\n",
      "epoch 58 / 100, step 79/80, loss = 0.0438\n",
      "epoch 58 / 100, step 80/80, loss = 0.0427\n",
      "epoch 59 / 100, step 1/80, loss = 0.0491\n",
      "epoch 59 / 100, step 2/80, loss = 0.0472\n",
      "epoch 59 / 100, step 3/80, loss = 0.0433\n",
      "epoch 59 / 100, step 4/80, loss = 0.0461\n",
      "epoch 59 / 100, step 5/80, loss = 0.0423\n",
      "epoch 59 / 100, step 6/80, loss = 0.0469\n",
      "epoch 59 / 100, step 7/80, loss = 0.0459\n",
      "epoch 59 / 100, step 8/80, loss = 0.0466\n",
      "epoch 59 / 100, step 9/80, loss = 0.0431\n",
      "epoch 59 / 100, step 10/80, loss = 0.0459\n",
      "epoch 59 / 100, step 11/80, loss = 0.0449\n",
      "epoch 59 / 100, step 12/80, loss = 0.0410\n",
      "epoch 59 / 100, step 13/80, loss = 0.0446\n",
      "epoch 59 / 100, step 14/80, loss = 0.0428\n",
      "epoch 59 / 100, step 15/80, loss = 0.0464\n",
      "epoch 59 / 100, step 16/80, loss = 0.0422\n",
      "epoch 59 / 100, step 17/80, loss = 0.0428\n",
      "epoch 59 / 100, step 18/80, loss = 0.0466\n",
      "epoch 59 / 100, step 19/80, loss = 0.0444\n",
      "epoch 59 / 100, step 20/80, loss = 0.0430\n",
      "epoch 59 / 100, step 21/80, loss = 0.0436\n",
      "epoch 59 / 100, step 22/80, loss = 0.0434\n",
      "epoch 59 / 100, step 23/80, loss = 0.0450\n",
      "epoch 59 / 100, step 24/80, loss = 0.0451\n",
      "epoch 59 / 100, step 25/80, loss = 0.0467\n",
      "epoch 59 / 100, step 26/80, loss = 0.0443\n",
      "epoch 59 / 100, step 27/80, loss = 0.0480\n",
      "epoch 59 / 100, step 28/80, loss = 0.0484\n",
      "epoch 59 / 100, step 29/80, loss = 0.0463\n",
      "epoch 59 / 100, step 30/80, loss = 0.0415\n",
      "epoch 59 / 100, step 31/80, loss = 0.0446\n",
      "epoch 59 / 100, step 32/80, loss = 0.0456\n",
      "epoch 59 / 100, step 33/80, loss = 0.0469\n",
      "epoch 59 / 100, step 34/80, loss = 0.0412\n",
      "epoch 59 / 100, step 35/80, loss = 0.0409\n",
      "epoch 59 / 100, step 36/80, loss = 0.0438\n",
      "epoch 59 / 100, step 37/80, loss = 0.0415\n",
      "epoch 59 / 100, step 38/80, loss = 0.0432\n",
      "epoch 59 / 100, step 39/80, loss = 0.0471\n",
      "epoch 59 / 100, step 40/80, loss = 0.0438\n",
      "epoch 59 / 100, step 41/80, loss = 0.0457\n",
      "epoch 59 / 100, step 42/80, loss = 0.0550\n",
      "epoch 59 / 100, step 43/80, loss = 0.0418\n",
      "epoch 59 / 100, step 44/80, loss = 0.0435\n",
      "epoch 59 / 100, step 45/80, loss = 0.0440\n",
      "epoch 59 / 100, step 46/80, loss = 0.0405\n",
      "epoch 59 / 100, step 47/80, loss = 0.0426\n",
      "epoch 59 / 100, step 48/80, loss = 0.0421\n",
      "epoch 59 / 100, step 49/80, loss = 0.0424\n",
      "epoch 59 / 100, step 50/80, loss = 0.0444\n",
      "epoch 59 / 100, step 51/80, loss = 0.0438\n",
      "epoch 59 / 100, step 52/80, loss = 0.0461\n",
      "epoch 59 / 100, step 53/80, loss = 0.0483\n",
      "epoch 59 / 100, step 54/80, loss = 0.0430\n",
      "epoch 59 / 100, step 55/80, loss = 0.0535\n",
      "epoch 59 / 100, step 56/80, loss = 0.0466\n",
      "epoch 59 / 100, step 57/80, loss = 0.0473\n",
      "epoch 59 / 100, step 58/80, loss = 0.0439\n",
      "epoch 59 / 100, step 59/80, loss = 0.0476\n",
      "epoch 59 / 100, step 60/80, loss = 0.0433\n",
      "epoch 59 / 100, step 61/80, loss = 0.0470\n",
      "epoch 59 / 100, step 62/80, loss = 0.0465\n",
      "epoch 59 / 100, step 63/80, loss = 0.0462\n",
      "epoch 59 / 100, step 64/80, loss = 0.0500\n",
      "epoch 59 / 100, step 65/80, loss = 0.0422\n",
      "epoch 59 / 100, step 66/80, loss = 0.0418\n",
      "epoch 59 / 100, step 67/80, loss = 0.0434\n",
      "epoch 59 / 100, step 68/80, loss = 0.0465\n",
      "epoch 59 / 100, step 69/80, loss = 0.0420\n",
      "epoch 59 / 100, step 70/80, loss = 0.0534\n",
      "epoch 59 / 100, step 71/80, loss = 0.0440\n",
      "epoch 59 / 100, step 72/80, loss = 0.0455\n",
      "epoch 59 / 100, step 73/80, loss = 0.0482\n",
      "epoch 59 / 100, step 74/80, loss = 0.0484\n",
      "epoch 59 / 100, step 75/80, loss = 0.0433\n",
      "epoch 59 / 100, step 76/80, loss = 0.0482\n",
      "epoch 59 / 100, step 77/80, loss = 0.0493\n",
      "epoch 59 / 100, step 78/80, loss = 0.0480\n",
      "epoch 59 / 100, step 79/80, loss = 0.0447\n",
      "epoch 59 / 100, step 80/80, loss = 0.0517\n",
      "epoch 60 / 100, step 1/80, loss = 0.0507\n",
      "epoch 60 / 100, step 2/80, loss = 0.0443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 / 100, step 3/80, loss = 0.0497\n",
      "epoch 60 / 100, step 4/80, loss = 0.0446\n",
      "epoch 60 / 100, step 5/80, loss = 0.0435\n",
      "epoch 60 / 100, step 6/80, loss = 0.0483\n",
      "epoch 60 / 100, step 7/80, loss = 0.0505\n",
      "epoch 60 / 100, step 8/80, loss = 0.0463\n",
      "epoch 60 / 100, step 9/80, loss = 0.0479\n",
      "epoch 60 / 100, step 10/80, loss = 0.0472\n",
      "epoch 60 / 100, step 11/80, loss = 0.0471\n",
      "epoch 60 / 100, step 12/80, loss = 0.0413\n",
      "epoch 60 / 100, step 13/80, loss = 0.0454\n",
      "epoch 60 / 100, step 14/80, loss = 0.0497\n",
      "epoch 60 / 100, step 15/80, loss = 0.0439\n",
      "epoch 60 / 100, step 16/80, loss = 0.0452\n",
      "epoch 60 / 100, step 17/80, loss = 0.0448\n",
      "epoch 60 / 100, step 18/80, loss = 0.0461\n",
      "epoch 60 / 100, step 19/80, loss = 0.0466\n",
      "epoch 60 / 100, step 20/80, loss = 0.0449\n",
      "epoch 60 / 100, step 21/80, loss = 0.0437\n",
      "epoch 60 / 100, step 22/80, loss = 0.0426\n",
      "epoch 60 / 100, step 23/80, loss = 0.0418\n",
      "epoch 60 / 100, step 24/80, loss = 0.0504\n",
      "epoch 60 / 100, step 25/80, loss = 0.0450\n",
      "epoch 60 / 100, step 26/80, loss = 0.0489\n",
      "epoch 60 / 100, step 27/80, loss = 0.0427\n",
      "epoch 60 / 100, step 28/80, loss = 0.0408\n",
      "epoch 60 / 100, step 29/80, loss = 0.0421\n",
      "epoch 60 / 100, step 30/80, loss = 0.0480\n",
      "epoch 60 / 100, step 31/80, loss = 0.0447\n",
      "epoch 60 / 100, step 32/80, loss = 0.0410\n",
      "epoch 60 / 100, step 33/80, loss = 0.0436\n",
      "epoch 60 / 100, step 34/80, loss = 0.0485\n",
      "epoch 60 / 100, step 35/80, loss = 0.0424\n",
      "epoch 60 / 100, step 36/80, loss = 0.0447\n",
      "epoch 60 / 100, step 37/80, loss = 0.0426\n",
      "epoch 60 / 100, step 38/80, loss = 0.0437\n",
      "epoch 60 / 100, step 39/80, loss = 0.0427\n",
      "epoch 60 / 100, step 40/80, loss = 0.0429\n",
      "epoch 60 / 100, step 41/80, loss = 0.0403\n",
      "epoch 60 / 100, step 42/80, loss = 0.0478\n",
      "epoch 60 / 100, step 43/80, loss = 0.0435\n",
      "epoch 60 / 100, step 44/80, loss = 0.0495\n",
      "epoch 60 / 100, step 45/80, loss = 0.0435\n",
      "epoch 60 / 100, step 46/80, loss = 0.0457\n",
      "epoch 60 / 100, step 47/80, loss = 0.0418\n",
      "epoch 60 / 100, step 48/80, loss = 0.0423\n",
      "epoch 60 / 100, step 49/80, loss = 0.0432\n",
      "epoch 60 / 100, step 50/80, loss = 0.0444\n",
      "epoch 60 / 100, step 51/80, loss = 0.0456\n",
      "epoch 60 / 100, step 52/80, loss = 0.0426\n",
      "epoch 60 / 100, step 53/80, loss = 0.0491\n",
      "epoch 60 / 100, step 54/80, loss = 0.0444\n",
      "epoch 60 / 100, step 55/80, loss = 0.0425\n",
      "epoch 60 / 100, step 56/80, loss = 0.0424\n",
      "epoch 60 / 100, step 57/80, loss = 0.0492\n",
      "epoch 60 / 100, step 58/80, loss = 0.0444\n",
      "epoch 60 / 100, step 59/80, loss = 0.0461\n",
      "epoch 60 / 100, step 60/80, loss = 0.0457\n",
      "epoch 60 / 100, step 61/80, loss = 0.0406\n",
      "epoch 60 / 100, step 62/80, loss = 0.0480\n",
      "epoch 60 / 100, step 63/80, loss = 0.0427\n",
      "epoch 60 / 100, step 64/80, loss = 0.0469\n",
      "epoch 60 / 100, step 65/80, loss = 0.0425\n",
      "epoch 60 / 100, step 66/80, loss = 0.0444\n",
      "epoch 60 / 100, step 67/80, loss = 0.0483\n",
      "epoch 60 / 100, step 68/80, loss = 0.0432\n",
      "epoch 60 / 100, step 69/80, loss = 0.0483\n",
      "epoch 60 / 100, step 70/80, loss = 0.0443\n",
      "epoch 60 / 100, step 71/80, loss = 0.0463\n",
      "epoch 60 / 100, step 72/80, loss = 0.0455\n",
      "epoch 60 / 100, step 73/80, loss = 0.0434\n",
      "epoch 60 / 100, step 74/80, loss = 0.0437\n",
      "epoch 60 / 100, step 75/80, loss = 0.0432\n",
      "epoch 60 / 100, step 76/80, loss = 0.0456\n",
      "epoch 60 / 100, step 77/80, loss = 0.0456\n",
      "epoch 60 / 100, step 78/80, loss = 0.0427\n",
      "epoch 60 / 100, step 79/80, loss = 0.0481\n",
      "epoch 60 / 100, step 80/80, loss = 0.0406\n",
      "epoch 61 / 100, step 1/80, loss = 0.0424\n",
      "epoch 61 / 100, step 2/80, loss = 0.0389\n",
      "epoch 61 / 100, step 3/80, loss = 0.0410\n",
      "epoch 61 / 100, step 4/80, loss = 0.0427\n",
      "epoch 61 / 100, step 5/80, loss = 0.0400\n",
      "epoch 61 / 100, step 6/80, loss = 0.0436\n",
      "epoch 61 / 100, step 7/80, loss = 0.0433\n",
      "epoch 61 / 100, step 8/80, loss = 0.0426\n",
      "epoch 61 / 100, step 9/80, loss = 0.0421\n",
      "epoch 61 / 100, step 10/80, loss = 0.0396\n",
      "epoch 61 / 100, step 11/80, loss = 0.0447\n",
      "epoch 61 / 100, step 12/80, loss = 0.0417\n",
      "epoch 61 / 100, step 13/80, loss = 0.0418\n",
      "epoch 61 / 100, step 14/80, loss = 0.0466\n",
      "epoch 61 / 100, step 15/80, loss = 0.0498\n",
      "epoch 61 / 100, step 16/80, loss = 0.0439\n",
      "epoch 61 / 100, step 17/80, loss = 0.0401\n",
      "epoch 61 / 100, step 18/80, loss = 0.0442\n",
      "epoch 61 / 100, step 19/80, loss = 0.0406\n",
      "epoch 61 / 100, step 20/80, loss = 0.0397\n",
      "epoch 61 / 100, step 21/80, loss = 0.0429\n",
      "epoch 61 / 100, step 22/80, loss = 0.0409\n",
      "epoch 61 / 100, step 23/80, loss = 0.0482\n",
      "epoch 61 / 100, step 24/80, loss = 0.0478\n",
      "epoch 61 / 100, step 25/80, loss = 0.0437\n",
      "epoch 61 / 100, step 26/80, loss = 0.0429\n",
      "epoch 61 / 100, step 27/80, loss = 0.0482\n",
      "epoch 61 / 100, step 28/80, loss = 0.0423\n",
      "epoch 61 / 100, step 29/80, loss = 0.0500\n",
      "epoch 61 / 100, step 30/80, loss = 0.0448\n",
      "epoch 61 / 100, step 31/80, loss = 0.0473\n",
      "epoch 61 / 100, step 32/80, loss = 0.0435\n",
      "epoch 61 / 100, step 33/80, loss = 0.0433\n",
      "epoch 61 / 100, step 34/80, loss = 0.0494\n",
      "epoch 61 / 100, step 35/80, loss = 0.0460\n",
      "epoch 61 / 100, step 36/80, loss = 0.0461\n",
      "epoch 61 / 100, step 37/80, loss = 0.0413\n",
      "epoch 61 / 100, step 38/80, loss = 0.0419\n",
      "epoch 61 / 100, step 39/80, loss = 0.0414\n",
      "epoch 61 / 100, step 40/80, loss = 0.0415\n",
      "epoch 61 / 100, step 41/80, loss = 0.0448\n",
      "epoch 61 / 100, step 42/80, loss = 0.0450\n",
      "epoch 61 / 100, step 43/80, loss = 0.0421\n",
      "epoch 61 / 100, step 44/80, loss = 0.0421\n",
      "epoch 61 / 100, step 45/80, loss = 0.0463\n",
      "epoch 61 / 100, step 46/80, loss = 0.0478\n",
      "epoch 61 / 100, step 47/80, loss = 0.0429\n",
      "epoch 61 / 100, step 48/80, loss = 0.0471\n",
      "epoch 61 / 100, step 49/80, loss = 0.0468\n",
      "epoch 61 / 100, step 50/80, loss = 0.0421\n",
      "epoch 61 / 100, step 51/80, loss = 0.0461\n",
      "epoch 61 / 100, step 52/80, loss = 0.0429\n",
      "epoch 61 / 100, step 53/80, loss = 0.0453\n",
      "epoch 61 / 100, step 54/80, loss = 0.0420\n",
      "epoch 61 / 100, step 55/80, loss = 0.0463\n",
      "epoch 61 / 100, step 56/80, loss = 0.0395\n",
      "epoch 61 / 100, step 57/80, loss = 0.0415\n",
      "epoch 61 / 100, step 58/80, loss = 0.0460\n",
      "epoch 61 / 100, step 59/80, loss = 0.0424\n",
      "epoch 61 / 100, step 60/80, loss = 0.0406\n",
      "epoch 61 / 100, step 61/80, loss = 0.0423\n",
      "epoch 61 / 100, step 62/80, loss = 0.0447\n",
      "epoch 61 / 100, step 63/80, loss = 0.0421\n",
      "epoch 61 / 100, step 64/80, loss = 0.0421\n",
      "epoch 61 / 100, step 65/80, loss = 0.0405\n",
      "epoch 61 / 100, step 66/80, loss = 0.0433\n",
      "epoch 61 / 100, step 67/80, loss = 0.0427\n",
      "epoch 61 / 100, step 68/80, loss = 0.0436\n",
      "epoch 61 / 100, step 69/80, loss = 0.0416\n",
      "epoch 61 / 100, step 70/80, loss = 0.0460\n",
      "epoch 61 / 100, step 71/80, loss = 0.0453\n",
      "epoch 61 / 100, step 72/80, loss = 0.0424\n",
      "epoch 61 / 100, step 73/80, loss = 0.0449\n",
      "epoch 61 / 100, step 74/80, loss = 0.0433\n",
      "epoch 61 / 100, step 75/80, loss = 0.0450\n",
      "epoch 61 / 100, step 76/80, loss = 0.0437\n",
      "epoch 61 / 100, step 77/80, loss = 0.0471\n",
      "epoch 61 / 100, step 78/80, loss = 0.0433\n",
      "epoch 61 / 100, step 79/80, loss = 0.0461\n",
      "epoch 61 / 100, step 80/80, loss = 0.0425\n",
      "epoch 62 / 100, step 1/80, loss = 0.0442\n",
      "epoch 62 / 100, step 2/80, loss = 0.0449\n",
      "epoch 62 / 100, step 3/80, loss = 0.0415\n",
      "epoch 62 / 100, step 4/80, loss = 0.0441\n",
      "epoch 62 / 100, step 5/80, loss = 0.0428\n",
      "epoch 62 / 100, step 6/80, loss = 0.0421\n",
      "epoch 62 / 100, step 7/80, loss = 0.0425\n",
      "epoch 62 / 100, step 8/80, loss = 0.0421\n",
      "epoch 62 / 100, step 9/80, loss = 0.0398\n",
      "epoch 62 / 100, step 10/80, loss = 0.0406\n",
      "epoch 62 / 100, step 11/80, loss = 0.0445\n",
      "epoch 62 / 100, step 12/80, loss = 0.0489\n",
      "epoch 62 / 100, step 13/80, loss = 0.0405\n",
      "epoch 62 / 100, step 14/80, loss = 0.0411\n",
      "epoch 62 / 100, step 15/80, loss = 0.0456\n",
      "epoch 62 / 100, step 16/80, loss = 0.0398\n",
      "epoch 62 / 100, step 17/80, loss = 0.0432\n",
      "epoch 62 / 100, step 18/80, loss = 0.0472\n",
      "epoch 62 / 100, step 19/80, loss = 0.0448\n",
      "epoch 62 / 100, step 20/80, loss = 0.0434\n",
      "epoch 62 / 100, step 21/80, loss = 0.0436\n",
      "epoch 62 / 100, step 22/80, loss = 0.0403\n",
      "epoch 62 / 100, step 23/80, loss = 0.0398\n",
      "epoch 62 / 100, step 24/80, loss = 0.0447\n",
      "epoch 62 / 100, step 25/80, loss = 0.0491\n",
      "epoch 62 / 100, step 26/80, loss = 0.0462\n",
      "epoch 62 / 100, step 27/80, loss = 0.0411\n",
      "epoch 62 / 100, step 28/80, loss = 0.0434\n",
      "epoch 62 / 100, step 29/80, loss = 0.0414\n",
      "epoch 62 / 100, step 30/80, loss = 0.0444\n",
      "epoch 62 / 100, step 31/80, loss = 0.0470\n",
      "epoch 62 / 100, step 32/80, loss = 0.0442\n",
      "epoch 62 / 100, step 33/80, loss = 0.0423\n",
      "epoch 62 / 100, step 34/80, loss = 0.0439\n",
      "epoch 62 / 100, step 35/80, loss = 0.0463\n",
      "epoch 62 / 100, step 36/80, loss = 0.0424\n",
      "epoch 62 / 100, step 37/80, loss = 0.0427\n",
      "epoch 62 / 100, step 38/80, loss = 0.0438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62 / 100, step 39/80, loss = 0.0417\n",
      "epoch 62 / 100, step 40/80, loss = 0.0423\n",
      "epoch 62 / 100, step 41/80, loss = 0.0463\n",
      "epoch 62 / 100, step 42/80, loss = 0.0427\n",
      "epoch 62 / 100, step 43/80, loss = 0.0381\n",
      "epoch 62 / 100, step 44/80, loss = 0.0429\n",
      "epoch 62 / 100, step 45/80, loss = 0.0451\n",
      "epoch 62 / 100, step 46/80, loss = 0.0468\n",
      "epoch 62 / 100, step 47/80, loss = 0.0427\n",
      "epoch 62 / 100, step 48/80, loss = 0.0387\n",
      "epoch 62 / 100, step 49/80, loss = 0.0400\n",
      "epoch 62 / 100, step 50/80, loss = 0.0419\n",
      "epoch 62 / 100, step 51/80, loss = 0.0438\n",
      "epoch 62 / 100, step 52/80, loss = 0.0408\n",
      "epoch 62 / 100, step 53/80, loss = 0.0444\n",
      "epoch 62 / 100, step 54/80, loss = 0.0452\n",
      "epoch 62 / 100, step 55/80, loss = 0.0444\n",
      "epoch 62 / 100, step 56/80, loss = 0.0393\n",
      "epoch 62 / 100, step 57/80, loss = 0.0423\n",
      "epoch 62 / 100, step 58/80, loss = 0.0436\n",
      "epoch 62 / 100, step 59/80, loss = 0.0416\n",
      "epoch 62 / 100, step 60/80, loss = 0.0424\n",
      "epoch 62 / 100, step 61/80, loss = 0.0414\n",
      "epoch 62 / 100, step 62/80, loss = 0.0401\n",
      "epoch 62 / 100, step 63/80, loss = 0.0442\n",
      "epoch 62 / 100, step 64/80, loss = 0.0395\n",
      "epoch 62 / 100, step 65/80, loss = 0.0460\n",
      "epoch 62 / 100, step 66/80, loss = 0.0401\n",
      "epoch 62 / 100, step 67/80, loss = 0.0422\n",
      "epoch 62 / 100, step 68/80, loss = 0.0443\n",
      "epoch 62 / 100, step 69/80, loss = 0.0427\n",
      "epoch 62 / 100, step 70/80, loss = 0.0414\n",
      "epoch 62 / 100, step 71/80, loss = 0.0483\n",
      "epoch 62 / 100, step 72/80, loss = 0.0462\n",
      "epoch 62 / 100, step 73/80, loss = 0.0421\n",
      "epoch 62 / 100, step 74/80, loss = 0.0416\n",
      "epoch 62 / 100, step 75/80, loss = 0.0393\n",
      "epoch 62 / 100, step 76/80, loss = 0.0395\n",
      "epoch 62 / 100, step 77/80, loss = 0.0449\n",
      "epoch 62 / 100, step 78/80, loss = 0.0425\n",
      "epoch 62 / 100, step 79/80, loss = 0.0437\n",
      "epoch 62 / 100, step 80/80, loss = 0.0446\n",
      "epoch 63 / 100, step 1/80, loss = 0.0386\n",
      "epoch 63 / 100, step 2/80, loss = 0.0437\n",
      "epoch 63 / 100, step 3/80, loss = 0.0444\n",
      "epoch 63 / 100, step 4/80, loss = 0.0409\n",
      "epoch 63 / 100, step 5/80, loss = 0.0390\n",
      "epoch 63 / 100, step 6/80, loss = 0.0428\n",
      "epoch 63 / 100, step 7/80, loss = 0.0401\n",
      "epoch 63 / 100, step 8/80, loss = 0.0399\n",
      "epoch 63 / 100, step 9/80, loss = 0.0438\n",
      "epoch 63 / 100, step 10/80, loss = 0.0392\n",
      "epoch 63 / 100, step 11/80, loss = 0.0436\n",
      "epoch 63 / 100, step 12/80, loss = 0.0403\n",
      "epoch 63 / 100, step 13/80, loss = 0.0389\n",
      "epoch 63 / 100, step 14/80, loss = 0.0432\n",
      "epoch 63 / 100, step 15/80, loss = 0.0427\n",
      "epoch 63 / 100, step 16/80, loss = 0.0392\n",
      "epoch 63 / 100, step 17/80, loss = 0.0410\n",
      "epoch 63 / 100, step 18/80, loss = 0.0410\n",
      "epoch 63 / 100, step 19/80, loss = 0.0425\n",
      "epoch 63 / 100, step 20/80, loss = 0.0448\n",
      "epoch 63 / 100, step 21/80, loss = 0.0399\n",
      "epoch 63 / 100, step 22/80, loss = 0.0393\n",
      "epoch 63 / 100, step 23/80, loss = 0.0403\n",
      "epoch 63 / 100, step 24/80, loss = 0.0408\n",
      "epoch 63 / 100, step 25/80, loss = 0.0429\n",
      "epoch 63 / 100, step 26/80, loss = 0.0402\n",
      "epoch 63 / 100, step 27/80, loss = 0.0397\n",
      "epoch 63 / 100, step 28/80, loss = 0.0437\n",
      "epoch 63 / 100, step 29/80, loss = 0.0440\n",
      "epoch 63 / 100, step 30/80, loss = 0.0462\n",
      "epoch 63 / 100, step 31/80, loss = 0.0397\n",
      "epoch 63 / 100, step 32/80, loss = 0.0403\n",
      "epoch 63 / 100, step 33/80, loss = 0.0401\n",
      "epoch 63 / 100, step 34/80, loss = 0.0461\n",
      "epoch 63 / 100, step 35/80, loss = 0.0453\n",
      "epoch 63 / 100, step 36/80, loss = 0.0378\n",
      "epoch 63 / 100, step 37/80, loss = 0.0418\n",
      "epoch 63 / 100, step 38/80, loss = 0.0409\n",
      "epoch 63 / 100, step 39/80, loss = 0.0412\n",
      "epoch 63 / 100, step 40/80, loss = 0.0344\n",
      "epoch 63 / 100, step 41/80, loss = 0.0402\n",
      "epoch 63 / 100, step 42/80, loss = 0.0411\n",
      "epoch 63 / 100, step 43/80, loss = 0.0441\n",
      "epoch 63 / 100, step 44/80, loss = 0.0401\n",
      "epoch 63 / 100, step 45/80, loss = 0.0428\n",
      "epoch 63 / 100, step 46/80, loss = 0.0381\n",
      "epoch 63 / 100, step 47/80, loss = 0.0417\n",
      "epoch 63 / 100, step 48/80, loss = 0.0416\n",
      "epoch 63 / 100, step 49/80, loss = 0.0432\n",
      "epoch 63 / 100, step 50/80, loss = 0.0408\n",
      "epoch 63 / 100, step 51/80, loss = 0.0429\n",
      "epoch 63 / 100, step 52/80, loss = 0.0428\n",
      "epoch 63 / 100, step 53/80, loss = 0.0434\n",
      "epoch 63 / 100, step 54/80, loss = 0.0388\n",
      "epoch 63 / 100, step 55/80, loss = 0.0416\n",
      "epoch 63 / 100, step 56/80, loss = 0.0435\n",
      "epoch 63 / 100, step 57/80, loss = 0.0503\n",
      "epoch 63 / 100, step 58/80, loss = 0.0454\n",
      "epoch 63 / 100, step 59/80, loss = 0.0438\n",
      "epoch 63 / 100, step 60/80, loss = 0.0418\n",
      "epoch 63 / 100, step 61/80, loss = 0.0492\n",
      "epoch 63 / 100, step 62/80, loss = 0.0448\n",
      "epoch 63 / 100, step 63/80, loss = 0.0474\n",
      "epoch 63 / 100, step 64/80, loss = 0.0508\n",
      "epoch 63 / 100, step 65/80, loss = 0.0456\n",
      "epoch 63 / 100, step 66/80, loss = 0.0392\n",
      "epoch 63 / 100, step 67/80, loss = 0.0439\n",
      "epoch 63 / 100, step 68/80, loss = 0.0424\n",
      "epoch 63 / 100, step 69/80, loss = 0.0440\n",
      "epoch 63 / 100, step 70/80, loss = 0.0421\n",
      "epoch 63 / 100, step 71/80, loss = 0.0480\n",
      "epoch 63 / 100, step 72/80, loss = 0.0388\n",
      "epoch 63 / 100, step 73/80, loss = 0.0427\n",
      "epoch 63 / 100, step 74/80, loss = 0.0440\n",
      "epoch 63 / 100, step 75/80, loss = 0.0394\n",
      "epoch 63 / 100, step 76/80, loss = 0.0447\n",
      "epoch 63 / 100, step 77/80, loss = 0.0418\n",
      "epoch 63 / 100, step 78/80, loss = 0.0424\n",
      "epoch 63 / 100, step 79/80, loss = 0.0433\n",
      "epoch 63 / 100, step 80/80, loss = 0.0435\n",
      "epoch 64 / 100, step 1/80, loss = 0.0406\n",
      "epoch 64 / 100, step 2/80, loss = 0.0386\n",
      "epoch 64 / 100, step 3/80, loss = 0.0402\n",
      "epoch 64 / 100, step 4/80, loss = 0.0421\n",
      "epoch 64 / 100, step 5/80, loss = 0.0435\n",
      "epoch 64 / 100, step 6/80, loss = 0.0409\n",
      "epoch 64 / 100, step 7/80, loss = 0.0420\n",
      "epoch 64 / 100, step 8/80, loss = 0.0399\n",
      "epoch 64 / 100, step 9/80, loss = 0.0433\n",
      "epoch 64 / 100, step 10/80, loss = 0.0407\n",
      "epoch 64 / 100, step 11/80, loss = 0.0375\n",
      "epoch 64 / 100, step 12/80, loss = 0.0397\n",
      "epoch 64 / 100, step 13/80, loss = 0.0382\n",
      "epoch 64 / 100, step 14/80, loss = 0.0419\n",
      "epoch 64 / 100, step 15/80, loss = 0.0398\n",
      "epoch 64 / 100, step 16/80, loss = 0.0391\n",
      "epoch 64 / 100, step 17/80, loss = 0.0412\n",
      "epoch 64 / 100, step 18/80, loss = 0.0387\n",
      "epoch 64 / 100, step 19/80, loss = 0.0456\n",
      "epoch 64 / 100, step 20/80, loss = 0.0398\n",
      "epoch 64 / 100, step 21/80, loss = 0.0426\n",
      "epoch 64 / 100, step 22/80, loss = 0.0382\n",
      "epoch 64 / 100, step 23/80, loss = 0.0430\n",
      "epoch 64 / 100, step 24/80, loss = 0.0444\n",
      "epoch 64 / 100, step 25/80, loss = 0.0406\n",
      "epoch 64 / 100, step 26/80, loss = 0.0443\n",
      "epoch 64 / 100, step 27/80, loss = 0.0403\n",
      "epoch 64 / 100, step 28/80, loss = 0.0426\n",
      "epoch 64 / 100, step 29/80, loss = 0.0438\n",
      "epoch 64 / 100, step 30/80, loss = 0.0389\n",
      "epoch 64 / 100, step 31/80, loss = 0.0421\n",
      "epoch 64 / 100, step 32/80, loss = 0.0444\n",
      "epoch 64 / 100, step 33/80, loss = 0.0390\n",
      "epoch 64 / 100, step 34/80, loss = 0.0413\n",
      "epoch 64 / 100, step 35/80, loss = 0.0436\n",
      "epoch 64 / 100, step 36/80, loss = 0.0410\n",
      "epoch 64 / 100, step 37/80, loss = 0.0416\n",
      "epoch 64 / 100, step 38/80, loss = 0.0421\n",
      "epoch 64 / 100, step 39/80, loss = 0.0405\n",
      "epoch 64 / 100, step 40/80, loss = 0.0435\n",
      "epoch 64 / 100, step 41/80, loss = 0.0428\n",
      "epoch 64 / 100, step 42/80, loss = 0.0383\n",
      "epoch 64 / 100, step 43/80, loss = 0.0382\n",
      "epoch 64 / 100, step 44/80, loss = 0.0394\n",
      "epoch 64 / 100, step 45/80, loss = 0.0405\n",
      "epoch 64 / 100, step 46/80, loss = 0.0437\n",
      "epoch 64 / 100, step 47/80, loss = 0.0382\n",
      "epoch 64 / 100, step 48/80, loss = 0.0428\n",
      "epoch 64 / 100, step 49/80, loss = 0.0416\n",
      "epoch 64 / 100, step 50/80, loss = 0.0395\n",
      "epoch 64 / 100, step 51/80, loss = 0.0430\n",
      "epoch 64 / 100, step 52/80, loss = 0.0413\n",
      "epoch 64 / 100, step 53/80, loss = 0.0412\n",
      "epoch 64 / 100, step 54/80, loss = 0.0420\n",
      "epoch 64 / 100, step 55/80, loss = 0.0409\n",
      "epoch 64 / 100, step 56/80, loss = 0.0422\n",
      "epoch 64 / 100, step 57/80, loss = 0.0521\n",
      "epoch 64 / 100, step 58/80, loss = 0.0395\n",
      "epoch 64 / 100, step 59/80, loss = 0.0485\n",
      "epoch 64 / 100, step 60/80, loss = 0.0417\n",
      "epoch 64 / 100, step 61/80, loss = 0.0406\n",
      "epoch 64 / 100, step 62/80, loss = 0.0420\n",
      "epoch 64 / 100, step 63/80, loss = 0.0451\n",
      "epoch 64 / 100, step 64/80, loss = 0.0392\n",
      "epoch 64 / 100, step 65/80, loss = 0.0410\n",
      "epoch 64 / 100, step 66/80, loss = 0.0459\n",
      "epoch 64 / 100, step 67/80, loss = 0.0376\n",
      "epoch 64 / 100, step 68/80, loss = 0.0388\n",
      "epoch 64 / 100, step 69/80, loss = 0.0407\n",
      "epoch 64 / 100, step 70/80, loss = 0.0431\n",
      "epoch 64 / 100, step 71/80, loss = 0.0363\n",
      "epoch 64 / 100, step 72/80, loss = 0.0397\n",
      "epoch 64 / 100, step 73/80, loss = 0.0378\n",
      "epoch 64 / 100, step 74/80, loss = 0.0386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64 / 100, step 75/80, loss = 0.0469\n",
      "epoch 64 / 100, step 76/80, loss = 0.0364\n",
      "epoch 64 / 100, step 77/80, loss = 0.0396\n",
      "epoch 64 / 100, step 78/80, loss = 0.0427\n",
      "epoch 64 / 100, step 79/80, loss = 0.0422\n",
      "epoch 64 / 100, step 80/80, loss = 0.0425\n",
      "epoch 65 / 100, step 1/80, loss = 0.0422\n",
      "epoch 65 / 100, step 2/80, loss = 0.0388\n",
      "epoch 65 / 100, step 3/80, loss = 0.0393\n",
      "epoch 65 / 100, step 4/80, loss = 0.0400\n",
      "epoch 65 / 100, step 5/80, loss = 0.0403\n",
      "epoch 65 / 100, step 6/80, loss = 0.0385\n",
      "epoch 65 / 100, step 7/80, loss = 0.0394\n",
      "epoch 65 / 100, step 8/80, loss = 0.0380\n",
      "epoch 65 / 100, step 9/80, loss = 0.0380\n",
      "epoch 65 / 100, step 10/80, loss = 0.0399\n",
      "epoch 65 / 100, step 11/80, loss = 0.0407\n",
      "epoch 65 / 100, step 12/80, loss = 0.0379\n",
      "epoch 65 / 100, step 13/80, loss = 0.0410\n",
      "epoch 65 / 100, step 14/80, loss = 0.0406\n",
      "epoch 65 / 100, step 15/80, loss = 0.0426\n",
      "epoch 65 / 100, step 16/80, loss = 0.0386\n",
      "epoch 65 / 100, step 17/80, loss = 0.0373\n",
      "epoch 65 / 100, step 18/80, loss = 0.0404\n",
      "epoch 65 / 100, step 19/80, loss = 0.0400\n",
      "epoch 65 / 100, step 20/80, loss = 0.0375\n",
      "epoch 65 / 100, step 21/80, loss = 0.0385\n",
      "epoch 65 / 100, step 22/80, loss = 0.0414\n",
      "epoch 65 / 100, step 23/80, loss = 0.0394\n",
      "epoch 65 / 100, step 24/80, loss = 0.0449\n",
      "epoch 65 / 100, step 25/80, loss = 0.0416\n",
      "epoch 65 / 100, step 26/80, loss = 0.0423\n",
      "epoch 65 / 100, step 27/80, loss = 0.0414\n",
      "epoch 65 / 100, step 28/80, loss = 0.0386\n",
      "epoch 65 / 100, step 29/80, loss = 0.0359\n",
      "epoch 65 / 100, step 30/80, loss = 0.0416\n",
      "epoch 65 / 100, step 31/80, loss = 0.0386\n",
      "epoch 65 / 100, step 32/80, loss = 0.0383\n",
      "epoch 65 / 100, step 33/80, loss = 0.0388\n",
      "epoch 65 / 100, step 34/80, loss = 0.0413\n",
      "epoch 65 / 100, step 35/80, loss = 0.0395\n",
      "epoch 65 / 100, step 36/80, loss = 0.0389\n",
      "epoch 65 / 100, step 37/80, loss = 0.0396\n",
      "epoch 65 / 100, step 38/80, loss = 0.0417\n",
      "epoch 65 / 100, step 39/80, loss = 0.0394\n",
      "epoch 65 / 100, step 40/80, loss = 0.0383\n",
      "epoch 65 / 100, step 41/80, loss = 0.0415\n",
      "epoch 65 / 100, step 42/80, loss = 0.0420\n",
      "epoch 65 / 100, step 43/80, loss = 0.0420\n",
      "epoch 65 / 100, step 44/80, loss = 0.0402\n",
      "epoch 65 / 100, step 45/80, loss = 0.0397\n",
      "epoch 65 / 100, step 46/80, loss = 0.0444\n",
      "epoch 65 / 100, step 47/80, loss = 0.0473\n",
      "epoch 65 / 100, step 48/80, loss = 0.0398\n",
      "epoch 65 / 100, step 49/80, loss = 0.0432\n",
      "epoch 65 / 100, step 50/80, loss = 0.0407\n",
      "epoch 65 / 100, step 51/80, loss = 0.0370\n",
      "epoch 65 / 100, step 52/80, loss = 0.0446\n",
      "epoch 65 / 100, step 53/80, loss = 0.0424\n",
      "epoch 65 / 100, step 54/80, loss = 0.0435\n",
      "epoch 65 / 100, step 55/80, loss = 0.0424\n",
      "epoch 65 / 100, step 56/80, loss = 0.0401\n",
      "epoch 65 / 100, step 57/80, loss = 0.0372\n",
      "epoch 65 / 100, step 58/80, loss = 0.0389\n",
      "epoch 65 / 100, step 59/80, loss = 0.0378\n",
      "epoch 65 / 100, step 60/80, loss = 0.0504\n",
      "epoch 65 / 100, step 61/80, loss = 0.0417\n",
      "epoch 65 / 100, step 62/80, loss = 0.0394\n",
      "epoch 65 / 100, step 63/80, loss = 0.0435\n",
      "epoch 65 / 100, step 64/80, loss = 0.0416\n",
      "epoch 65 / 100, step 65/80, loss = 0.0516\n",
      "epoch 65 / 100, step 66/80, loss = 0.0384\n",
      "epoch 65 / 100, step 67/80, loss = 0.0404\n",
      "epoch 65 / 100, step 68/80, loss = 0.0468\n",
      "epoch 65 / 100, step 69/80, loss = 0.0419\n",
      "epoch 65 / 100, step 70/80, loss = 0.0444\n",
      "epoch 65 / 100, step 71/80, loss = 0.0434\n",
      "epoch 65 / 100, step 72/80, loss = 0.0469\n",
      "epoch 65 / 100, step 73/80, loss = 0.0414\n",
      "epoch 65 / 100, step 74/80, loss = 0.0421\n",
      "epoch 65 / 100, step 75/80, loss = 0.0384\n",
      "epoch 65 / 100, step 76/80, loss = 0.0380\n",
      "epoch 65 / 100, step 77/80, loss = 0.0488\n",
      "epoch 65 / 100, step 78/80, loss = 0.0445\n",
      "epoch 65 / 100, step 79/80, loss = 0.0409\n",
      "epoch 65 / 100, step 80/80, loss = 0.0407\n",
      "epoch 66 / 100, step 1/80, loss = 0.0425\n",
      "epoch 66 / 100, step 2/80, loss = 0.0412\n",
      "epoch 66 / 100, step 3/80, loss = 0.0389\n",
      "epoch 66 / 100, step 4/80, loss = 0.0379\n",
      "epoch 66 / 100, step 5/80, loss = 0.0384\n",
      "epoch 66 / 100, step 6/80, loss = 0.0376\n",
      "epoch 66 / 100, step 7/80, loss = 0.0382\n",
      "epoch 66 / 100, step 8/80, loss = 0.0376\n",
      "epoch 66 / 100, step 9/80, loss = 0.0376\n",
      "epoch 66 / 100, step 10/80, loss = 0.0384\n",
      "epoch 66 / 100, step 11/80, loss = 0.0397\n",
      "epoch 66 / 100, step 12/80, loss = 0.0409\n",
      "epoch 66 / 100, step 13/80, loss = 0.0399\n",
      "epoch 66 / 100, step 14/80, loss = 0.0389\n",
      "epoch 66 / 100, step 15/80, loss = 0.0426\n",
      "epoch 66 / 100, step 16/80, loss = 0.0409\n",
      "epoch 66 / 100, step 17/80, loss = 0.0401\n",
      "epoch 66 / 100, step 18/80, loss = 0.0444\n",
      "epoch 66 / 100, step 19/80, loss = 0.0414\n",
      "epoch 66 / 100, step 20/80, loss = 0.0388\n",
      "epoch 66 / 100, step 21/80, loss = 0.0361\n",
      "epoch 66 / 100, step 22/80, loss = 0.0431\n",
      "epoch 66 / 100, step 23/80, loss = 0.0426\n",
      "epoch 66 / 100, step 24/80, loss = 0.0370\n",
      "epoch 66 / 100, step 25/80, loss = 0.0363\n",
      "epoch 66 / 100, step 26/80, loss = 0.0359\n",
      "epoch 66 / 100, step 27/80, loss = 0.0381\n",
      "epoch 66 / 100, step 28/80, loss = 0.0472\n",
      "epoch 66 / 100, step 29/80, loss = 0.0426\n",
      "epoch 66 / 100, step 30/80, loss = 0.0429\n",
      "epoch 66 / 100, step 31/80, loss = 0.0389\n",
      "epoch 66 / 100, step 32/80, loss = 0.0420\n",
      "epoch 66 / 100, step 33/80, loss = 0.0403\n",
      "epoch 66 / 100, step 34/80, loss = 0.0465\n",
      "epoch 66 / 100, step 35/80, loss = 0.0399\n",
      "epoch 66 / 100, step 36/80, loss = 0.0421\n",
      "epoch 66 / 100, step 37/80, loss = 0.0393\n",
      "epoch 66 / 100, step 38/80, loss = 0.0402\n",
      "epoch 66 / 100, step 39/80, loss = 0.0399\n",
      "epoch 66 / 100, step 40/80, loss = 0.0375\n",
      "epoch 66 / 100, step 41/80, loss = 0.0463\n",
      "epoch 66 / 100, step 42/80, loss = 0.0407\n",
      "epoch 66 / 100, step 43/80, loss = 0.0393\n",
      "epoch 66 / 100, step 44/80, loss = 0.0467\n",
      "epoch 66 / 100, step 45/80, loss = 0.0401\n",
      "epoch 66 / 100, step 46/80, loss = 0.0403\n",
      "epoch 66 / 100, step 47/80, loss = 0.0437\n",
      "epoch 66 / 100, step 48/80, loss = 0.0410\n",
      "epoch 66 / 100, step 49/80, loss = 0.0460\n",
      "epoch 66 / 100, step 50/80, loss = 0.0412\n",
      "epoch 66 / 100, step 51/80, loss = 0.0396\n",
      "epoch 66 / 100, step 52/80, loss = 0.0444\n",
      "epoch 66 / 100, step 53/80, loss = 0.0451\n",
      "epoch 66 / 100, step 54/80, loss = 0.0406\n",
      "epoch 66 / 100, step 55/80, loss = 0.0436\n",
      "epoch 66 / 100, step 56/80, loss = 0.0486\n",
      "epoch 66 / 100, step 57/80, loss = 0.0411\n",
      "epoch 66 / 100, step 58/80, loss = 0.0444\n",
      "epoch 66 / 100, step 59/80, loss = 0.0483\n",
      "epoch 66 / 100, step 60/80, loss = 0.0393\n",
      "epoch 66 / 100, step 61/80, loss = 0.0362\n",
      "epoch 66 / 100, step 62/80, loss = 0.0446\n",
      "epoch 66 / 100, step 63/80, loss = 0.0459\n",
      "epoch 66 / 100, step 64/80, loss = 0.0365\n",
      "epoch 66 / 100, step 65/80, loss = 0.0390\n",
      "epoch 66 / 100, step 66/80, loss = 0.0400\n",
      "epoch 66 / 100, step 67/80, loss = 0.0454\n",
      "epoch 66 / 100, step 68/80, loss = 0.0399\n",
      "epoch 66 / 100, step 69/80, loss = 0.0404\n",
      "epoch 66 / 100, step 70/80, loss = 0.0429\n",
      "epoch 66 / 100, step 71/80, loss = 0.0403\n",
      "epoch 66 / 100, step 72/80, loss = 0.0405\n",
      "epoch 66 / 100, step 73/80, loss = 0.0420\n",
      "epoch 66 / 100, step 74/80, loss = 0.0421\n",
      "epoch 66 / 100, step 75/80, loss = 0.0452\n",
      "epoch 66 / 100, step 76/80, loss = 0.0419\n",
      "epoch 66 / 100, step 77/80, loss = 0.0399\n",
      "epoch 66 / 100, step 78/80, loss = 0.0450\n",
      "epoch 66 / 100, step 79/80, loss = 0.0399\n",
      "epoch 66 / 100, step 80/80, loss = 0.0430\n",
      "epoch 67 / 100, step 1/80, loss = 0.0376\n",
      "epoch 67 / 100, step 2/80, loss = 0.0402\n",
      "epoch 67 / 100, step 3/80, loss = 0.0381\n",
      "epoch 67 / 100, step 4/80, loss = 0.0413\n",
      "epoch 67 / 100, step 5/80, loss = 0.0370\n",
      "epoch 67 / 100, step 6/80, loss = 0.0386\n",
      "epoch 67 / 100, step 7/80, loss = 0.0460\n",
      "epoch 67 / 100, step 8/80, loss = 0.0409\n",
      "epoch 67 / 100, step 9/80, loss = 0.0477\n",
      "epoch 67 / 100, step 10/80, loss = 0.0359\n",
      "epoch 67 / 100, step 11/80, loss = 0.0393\n",
      "epoch 67 / 100, step 12/80, loss = 0.0420\n",
      "epoch 67 / 100, step 13/80, loss = 0.0381\n",
      "epoch 67 / 100, step 14/80, loss = 0.0391\n",
      "epoch 67 / 100, step 15/80, loss = 0.0407\n",
      "epoch 67 / 100, step 16/80, loss = 0.0392\n",
      "epoch 67 / 100, step 17/80, loss = 0.0428\n",
      "epoch 67 / 100, step 18/80, loss = 0.0437\n",
      "epoch 67 / 100, step 19/80, loss = 0.0395\n",
      "epoch 67 / 100, step 20/80, loss = 0.0402\n",
      "epoch 67 / 100, step 21/80, loss = 0.0428\n",
      "epoch 67 / 100, step 22/80, loss = 0.0360\n",
      "epoch 67 / 100, step 23/80, loss = 0.0394\n",
      "epoch 67 / 100, step 24/80, loss = 0.0396\n",
      "epoch 67 / 100, step 25/80, loss = 0.0434\n",
      "epoch 67 / 100, step 26/80, loss = 0.0387\n",
      "epoch 67 / 100, step 27/80, loss = 0.0390\n",
      "epoch 67 / 100, step 28/80, loss = 0.0362\n",
      "epoch 67 / 100, step 29/80, loss = 0.0447\n",
      "epoch 67 / 100, step 30/80, loss = 0.0394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 / 100, step 31/80, loss = 0.0416\n",
      "epoch 67 / 100, step 32/80, loss = 0.0379\n",
      "epoch 67 / 100, step 33/80, loss = 0.0388\n",
      "epoch 67 / 100, step 34/80, loss = 0.0409\n",
      "epoch 67 / 100, step 35/80, loss = 0.0480\n",
      "epoch 67 / 100, step 36/80, loss = 0.0403\n",
      "epoch 67 / 100, step 37/80, loss = 0.0437\n",
      "epoch 67 / 100, step 38/80, loss = 0.0374\n",
      "epoch 67 / 100, step 39/80, loss = 0.0374\n",
      "epoch 67 / 100, step 40/80, loss = 0.0405\n",
      "epoch 67 / 100, step 41/80, loss = 0.0361\n",
      "epoch 67 / 100, step 42/80, loss = 0.0390\n",
      "epoch 67 / 100, step 43/80, loss = 0.0346\n",
      "epoch 67 / 100, step 44/80, loss = 0.0396\n",
      "epoch 67 / 100, step 45/80, loss = 0.0376\n",
      "epoch 67 / 100, step 46/80, loss = 0.0393\n",
      "epoch 67 / 100, step 47/80, loss = 0.0456\n",
      "epoch 67 / 100, step 48/80, loss = 0.0421\n",
      "epoch 67 / 100, step 49/80, loss = 0.0428\n",
      "epoch 67 / 100, step 50/80, loss = 0.0405\n",
      "epoch 67 / 100, step 51/80, loss = 0.0349\n",
      "epoch 67 / 100, step 52/80, loss = 0.0363\n",
      "epoch 67 / 100, step 53/80, loss = 0.0384\n",
      "epoch 67 / 100, step 54/80, loss = 0.0359\n",
      "epoch 67 / 100, step 55/80, loss = 0.0354\n",
      "epoch 67 / 100, step 56/80, loss = 0.0391\n",
      "epoch 67 / 100, step 57/80, loss = 0.0399\n",
      "epoch 67 / 100, step 58/80, loss = 0.0376\n",
      "epoch 67 / 100, step 59/80, loss = 0.0438\n",
      "epoch 67 / 100, step 60/80, loss = 0.0420\n",
      "epoch 67 / 100, step 61/80, loss = 0.0362\n",
      "epoch 67 / 100, step 62/80, loss = 0.0414\n",
      "epoch 67 / 100, step 63/80, loss = 0.0412\n",
      "epoch 67 / 100, step 64/80, loss = 0.0383\n",
      "epoch 67 / 100, step 65/80, loss = 0.0409\n",
      "epoch 67 / 100, step 66/80, loss = 0.0380\n",
      "epoch 67 / 100, step 67/80, loss = 0.0369\n",
      "epoch 67 / 100, step 68/80, loss = 0.0417\n",
      "epoch 67 / 100, step 69/80, loss = 0.0387\n",
      "epoch 67 / 100, step 70/80, loss = 0.0394\n",
      "epoch 67 / 100, step 71/80, loss = 0.0396\n",
      "epoch 67 / 100, step 72/80, loss = 0.0391\n",
      "epoch 67 / 100, step 73/80, loss = 0.0383\n",
      "epoch 67 / 100, step 74/80, loss = 0.0417\n",
      "epoch 67 / 100, step 75/80, loss = 0.0422\n",
      "epoch 67 / 100, step 76/80, loss = 0.0416\n",
      "epoch 67 / 100, step 77/80, loss = 0.0429\n",
      "epoch 67 / 100, step 78/80, loss = 0.0404\n",
      "epoch 67 / 100, step 79/80, loss = 0.0429\n",
      "epoch 67 / 100, step 80/80, loss = 0.0441\n",
      "epoch 68 / 100, step 1/80, loss = 0.0371\n",
      "epoch 68 / 100, step 2/80, loss = 0.0408\n",
      "epoch 68 / 100, step 3/80, loss = 0.0359\n",
      "epoch 68 / 100, step 4/80, loss = 0.0375\n",
      "epoch 68 / 100, step 5/80, loss = 0.0394\n",
      "epoch 68 / 100, step 6/80, loss = 0.0423\n",
      "epoch 68 / 100, step 7/80, loss = 0.0364\n",
      "epoch 68 / 100, step 8/80, loss = 0.0394\n",
      "epoch 68 / 100, step 9/80, loss = 0.0425\n",
      "epoch 68 / 100, step 10/80, loss = 0.0389\n",
      "epoch 68 / 100, step 11/80, loss = 0.0383\n",
      "epoch 68 / 100, step 12/80, loss = 0.0408\n",
      "epoch 68 / 100, step 13/80, loss = 0.0387\n",
      "epoch 68 / 100, step 14/80, loss = 0.0382\n",
      "epoch 68 / 100, step 15/80, loss = 0.0396\n",
      "epoch 68 / 100, step 16/80, loss = 0.0383\n",
      "epoch 68 / 100, step 17/80, loss = 0.0393\n",
      "epoch 68 / 100, step 18/80, loss = 0.0399\n",
      "epoch 68 / 100, step 19/80, loss = 0.0444\n",
      "epoch 68 / 100, step 20/80, loss = 0.0384\n",
      "epoch 68 / 100, step 21/80, loss = 0.0380\n",
      "epoch 68 / 100, step 22/80, loss = 0.0403\n",
      "epoch 68 / 100, step 23/80, loss = 0.0371\n",
      "epoch 68 / 100, step 24/80, loss = 0.0365\n",
      "epoch 68 / 100, step 25/80, loss = 0.0372\n",
      "epoch 68 / 100, step 26/80, loss = 0.0376\n",
      "epoch 68 / 100, step 27/80, loss = 0.0390\n",
      "epoch 68 / 100, step 28/80, loss = 0.0399\n",
      "epoch 68 / 100, step 29/80, loss = 0.0392\n",
      "epoch 68 / 100, step 30/80, loss = 0.0422\n",
      "epoch 68 / 100, step 31/80, loss = 0.0364\n",
      "epoch 68 / 100, step 32/80, loss = 0.0399\n",
      "epoch 68 / 100, step 33/80, loss = 0.0389\n",
      "epoch 68 / 100, step 34/80, loss = 0.0389\n",
      "epoch 68 / 100, step 35/80, loss = 0.0486\n",
      "epoch 68 / 100, step 36/80, loss = 0.0415\n",
      "epoch 68 / 100, step 37/80, loss = 0.0392\n",
      "epoch 68 / 100, step 38/80, loss = 0.0397\n",
      "epoch 68 / 100, step 39/80, loss = 0.0360\n",
      "epoch 68 / 100, step 40/80, loss = 0.0339\n",
      "epoch 68 / 100, step 41/80, loss = 0.0433\n",
      "epoch 68 / 100, step 42/80, loss = 0.0451\n",
      "epoch 68 / 100, step 43/80, loss = 0.0399\n",
      "epoch 68 / 100, step 44/80, loss = 0.0471\n",
      "epoch 68 / 100, step 45/80, loss = 0.0374\n",
      "epoch 68 / 100, step 46/80, loss = 0.0405\n",
      "epoch 68 / 100, step 47/80, loss = 0.0397\n",
      "epoch 68 / 100, step 48/80, loss = 0.0382\n",
      "epoch 68 / 100, step 49/80, loss = 0.0402\n",
      "epoch 68 / 100, step 50/80, loss = 0.0390\n",
      "epoch 68 / 100, step 51/80, loss = 0.0394\n",
      "epoch 68 / 100, step 52/80, loss = 0.0405\n",
      "epoch 68 / 100, step 53/80, loss = 0.0392\n",
      "epoch 68 / 100, step 54/80, loss = 0.0401\n",
      "epoch 68 / 100, step 55/80, loss = 0.0381\n",
      "epoch 68 / 100, step 56/80, loss = 0.0374\n",
      "epoch 68 / 100, step 57/80, loss = 0.0422\n",
      "epoch 68 / 100, step 58/80, loss = 0.0386\n",
      "epoch 68 / 100, step 59/80, loss = 0.0413\n",
      "epoch 68 / 100, step 60/80, loss = 0.0407\n",
      "epoch 68 / 100, step 61/80, loss = 0.0386\n",
      "epoch 68 / 100, step 62/80, loss = 0.0407\n",
      "epoch 68 / 100, step 63/80, loss = 0.0338\n",
      "epoch 68 / 100, step 64/80, loss = 0.0387\n",
      "epoch 68 / 100, step 65/80, loss = 0.0378\n",
      "epoch 68 / 100, step 66/80, loss = 0.0389\n",
      "epoch 68 / 100, step 67/80, loss = 0.0414\n",
      "epoch 68 / 100, step 68/80, loss = 0.0401\n",
      "epoch 68 / 100, step 69/80, loss = 0.0392\n",
      "epoch 68 / 100, step 70/80, loss = 0.0390\n",
      "epoch 68 / 100, step 71/80, loss = 0.0425\n",
      "epoch 68 / 100, step 72/80, loss = 0.0406\n",
      "epoch 68 / 100, step 73/80, loss = 0.0418\n",
      "epoch 68 / 100, step 74/80, loss = 0.0408\n",
      "epoch 68 / 100, step 75/80, loss = 0.0430\n",
      "epoch 68 / 100, step 76/80, loss = 0.0424\n",
      "epoch 68 / 100, step 77/80, loss = 0.0399\n",
      "epoch 68 / 100, step 78/80, loss = 0.0415\n",
      "epoch 68 / 100, step 79/80, loss = 0.0405\n",
      "epoch 68 / 100, step 80/80, loss = 0.0471\n",
      "epoch 69 / 100, step 1/80, loss = 0.0367\n",
      "epoch 69 / 100, step 2/80, loss = 0.0376\n",
      "epoch 69 / 100, step 3/80, loss = 0.0406\n",
      "epoch 69 / 100, step 4/80, loss = 0.0359\n",
      "epoch 69 / 100, step 5/80, loss = 0.0359\n",
      "epoch 69 / 100, step 6/80, loss = 0.0403\n",
      "epoch 69 / 100, step 7/80, loss = 0.0359\n",
      "epoch 69 / 100, step 8/80, loss = 0.0380\n",
      "epoch 69 / 100, step 9/80, loss = 0.0385\n",
      "epoch 69 / 100, step 10/80, loss = 0.0413\n",
      "epoch 69 / 100, step 11/80, loss = 0.0366\n",
      "epoch 69 / 100, step 12/80, loss = 0.0375\n",
      "epoch 69 / 100, step 13/80, loss = 0.0359\n",
      "epoch 69 / 100, step 14/80, loss = 0.0398\n",
      "epoch 69 / 100, step 15/80, loss = 0.0399\n",
      "epoch 69 / 100, step 16/80, loss = 0.0364\n",
      "epoch 69 / 100, step 17/80, loss = 0.0346\n",
      "epoch 69 / 100, step 18/80, loss = 0.0342\n",
      "epoch 69 / 100, step 19/80, loss = 0.0375\n",
      "epoch 69 / 100, step 20/80, loss = 0.0383\n",
      "epoch 69 / 100, step 21/80, loss = 0.0411\n",
      "epoch 69 / 100, step 22/80, loss = 0.0353\n",
      "epoch 69 / 100, step 23/80, loss = 0.0373\n",
      "epoch 69 / 100, step 24/80, loss = 0.0361\n",
      "epoch 69 / 100, step 25/80, loss = 0.0404\n",
      "epoch 69 / 100, step 26/80, loss = 0.0406\n",
      "epoch 69 / 100, step 27/80, loss = 0.0403\n",
      "epoch 69 / 100, step 28/80, loss = 0.0384\n",
      "epoch 69 / 100, step 29/80, loss = 0.0367\n",
      "epoch 69 / 100, step 30/80, loss = 0.0369\n",
      "epoch 69 / 100, step 31/80, loss = 0.0376\n",
      "epoch 69 / 100, step 32/80, loss = 0.0385\n",
      "epoch 69 / 100, step 33/80, loss = 0.0368\n",
      "epoch 69 / 100, step 34/80, loss = 0.0389\n",
      "epoch 69 / 100, step 35/80, loss = 0.0423\n",
      "epoch 69 / 100, step 36/80, loss = 0.0381\n",
      "epoch 69 / 100, step 37/80, loss = 0.0364\n",
      "epoch 69 / 100, step 38/80, loss = 0.0399\n",
      "epoch 69 / 100, step 39/80, loss = 0.0429\n",
      "epoch 69 / 100, step 40/80, loss = 0.0388\n",
      "epoch 69 / 100, step 41/80, loss = 0.0342\n",
      "epoch 69 / 100, step 42/80, loss = 0.0374\n",
      "epoch 69 / 100, step 43/80, loss = 0.0372\n",
      "epoch 69 / 100, step 44/80, loss = 0.0434\n",
      "epoch 69 / 100, step 45/80, loss = 0.0361\n",
      "epoch 69 / 100, step 46/80, loss = 0.0435\n",
      "epoch 69 / 100, step 47/80, loss = 0.0377\n",
      "epoch 69 / 100, step 48/80, loss = 0.0372\n",
      "epoch 69 / 100, step 49/80, loss = 0.0381\n",
      "epoch 69 / 100, step 50/80, loss = 0.0368\n",
      "epoch 69 / 100, step 51/80, loss = 0.0471\n",
      "epoch 69 / 100, step 52/80, loss = 0.0433\n",
      "epoch 69 / 100, step 53/80, loss = 0.0395\n",
      "epoch 69 / 100, step 54/80, loss = 0.0446\n",
      "epoch 69 / 100, step 55/80, loss = 0.0395\n",
      "epoch 69 / 100, step 56/80, loss = 0.0514\n",
      "epoch 69 / 100, step 57/80, loss = 0.0362\n",
      "epoch 69 / 100, step 58/80, loss = 0.0397\n",
      "epoch 69 / 100, step 59/80, loss = 0.0440\n",
      "epoch 69 / 100, step 60/80, loss = 0.0363\n",
      "epoch 69 / 100, step 61/80, loss = 0.0379\n",
      "epoch 69 / 100, step 62/80, loss = 0.0410\n",
      "epoch 69 / 100, step 63/80, loss = 0.0402\n",
      "epoch 69 / 100, step 64/80, loss = 0.0367\n",
      "epoch 69 / 100, step 65/80, loss = 0.0434\n",
      "epoch 69 / 100, step 66/80, loss = 0.0391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69 / 100, step 67/80, loss = 0.0395\n",
      "epoch 69 / 100, step 68/80, loss = 0.0384\n",
      "epoch 69 / 100, step 69/80, loss = 0.0400\n",
      "epoch 69 / 100, step 70/80, loss = 0.0367\n",
      "epoch 69 / 100, step 71/80, loss = 0.0356\n",
      "epoch 69 / 100, step 72/80, loss = 0.0395\n",
      "epoch 69 / 100, step 73/80, loss = 0.0369\n",
      "epoch 69 / 100, step 74/80, loss = 0.0342\n",
      "epoch 69 / 100, step 75/80, loss = 0.0384\n",
      "epoch 69 / 100, step 76/80, loss = 0.0393\n",
      "epoch 69 / 100, step 77/80, loss = 0.0432\n",
      "epoch 69 / 100, step 78/80, loss = 0.0395\n",
      "epoch 69 / 100, step 79/80, loss = 0.0389\n",
      "epoch 69 / 100, step 80/80, loss = 0.0428\n",
      "epoch 70 / 100, step 1/80, loss = 0.0383\n",
      "epoch 70 / 100, step 2/80, loss = 0.0382\n",
      "epoch 70 / 100, step 3/80, loss = 0.0397\n",
      "epoch 70 / 100, step 4/80, loss = 0.0390\n",
      "epoch 70 / 100, step 5/80, loss = 0.0385\n",
      "epoch 70 / 100, step 6/80, loss = 0.0346\n",
      "epoch 70 / 100, step 7/80, loss = 0.0354\n",
      "epoch 70 / 100, step 8/80, loss = 0.0391\n",
      "epoch 70 / 100, step 9/80, loss = 0.0358\n",
      "epoch 70 / 100, step 10/80, loss = 0.0363\n",
      "epoch 70 / 100, step 11/80, loss = 0.0366\n",
      "epoch 70 / 100, step 12/80, loss = 0.0405\n",
      "epoch 70 / 100, step 13/80, loss = 0.0361\n",
      "epoch 70 / 100, step 14/80, loss = 0.0402\n",
      "epoch 70 / 100, step 15/80, loss = 0.0383\n",
      "epoch 70 / 100, step 16/80, loss = 0.0368\n",
      "epoch 70 / 100, step 17/80, loss = 0.0372\n",
      "epoch 70 / 100, step 18/80, loss = 0.0382\n",
      "epoch 70 / 100, step 19/80, loss = 0.0371\n",
      "epoch 70 / 100, step 20/80, loss = 0.0412\n",
      "epoch 70 / 100, step 21/80, loss = 0.0344\n",
      "epoch 70 / 100, step 22/80, loss = 0.0390\n",
      "epoch 70 / 100, step 23/80, loss = 0.0364\n",
      "epoch 70 / 100, step 24/80, loss = 0.0431\n",
      "epoch 70 / 100, step 25/80, loss = 0.0348\n",
      "epoch 70 / 100, step 26/80, loss = 0.0379\n",
      "epoch 70 / 100, step 27/80, loss = 0.0381\n",
      "epoch 70 / 100, step 28/80, loss = 0.0389\n",
      "epoch 70 / 100, step 29/80, loss = 0.0395\n",
      "epoch 70 / 100, step 30/80, loss = 0.0418\n",
      "epoch 70 / 100, step 31/80, loss = 0.0375\n",
      "epoch 70 / 100, step 32/80, loss = 0.0380\n",
      "epoch 70 / 100, step 33/80, loss = 0.0418\n",
      "epoch 70 / 100, step 34/80, loss = 0.0374\n",
      "epoch 70 / 100, step 35/80, loss = 0.0356\n",
      "epoch 70 / 100, step 36/80, loss = 0.0426\n",
      "epoch 70 / 100, step 37/80, loss = 0.0416\n",
      "epoch 70 / 100, step 38/80, loss = 0.0378\n",
      "epoch 70 / 100, step 39/80, loss = 0.0415\n",
      "epoch 70 / 100, step 40/80, loss = 0.0369\n",
      "epoch 70 / 100, step 41/80, loss = 0.0395\n",
      "epoch 70 / 100, step 42/80, loss = 0.0384\n",
      "epoch 70 / 100, step 43/80, loss = 0.0371\n",
      "epoch 70 / 100, step 44/80, loss = 0.0446\n",
      "epoch 70 / 100, step 45/80, loss = 0.0362\n",
      "epoch 70 / 100, step 46/80, loss = 0.0384\n",
      "epoch 70 / 100, step 47/80, loss = 0.0365\n",
      "epoch 70 / 100, step 48/80, loss = 0.0363\n",
      "epoch 70 / 100, step 49/80, loss = 0.0409\n",
      "epoch 70 / 100, step 50/80, loss = 0.0362\n",
      "epoch 70 / 100, step 51/80, loss = 0.0392\n",
      "epoch 70 / 100, step 52/80, loss = 0.0368\n",
      "epoch 70 / 100, step 53/80, loss = 0.0350\n",
      "epoch 70 / 100, step 54/80, loss = 0.0369\n",
      "epoch 70 / 100, step 55/80, loss = 0.0367\n",
      "epoch 70 / 100, step 56/80, loss = 0.0392\n",
      "epoch 70 / 100, step 57/80, loss = 0.0365\n",
      "epoch 70 / 100, step 58/80, loss = 0.0369\n",
      "epoch 70 / 100, step 59/80, loss = 0.0400\n",
      "epoch 70 / 100, step 60/80, loss = 0.0408\n",
      "epoch 70 / 100, step 61/80, loss = 0.0434\n",
      "epoch 70 / 100, step 62/80, loss = 0.0378\n",
      "epoch 70 / 100, step 63/80, loss = 0.0380\n",
      "epoch 70 / 100, step 64/80, loss = 0.0363\n",
      "epoch 70 / 100, step 65/80, loss = 0.0380\n",
      "epoch 70 / 100, step 66/80, loss = 0.0393\n",
      "epoch 70 / 100, step 67/80, loss = 0.0356\n",
      "epoch 70 / 100, step 68/80, loss = 0.0394\n",
      "epoch 70 / 100, step 69/80, loss = 0.0349\n",
      "epoch 70 / 100, step 70/80, loss = 0.0427\n",
      "epoch 70 / 100, step 71/80, loss = 0.0429\n",
      "epoch 70 / 100, step 72/80, loss = 0.0372\n",
      "epoch 70 / 100, step 73/80, loss = 0.0358\n",
      "epoch 70 / 100, step 74/80, loss = 0.0409\n",
      "epoch 70 / 100, step 75/80, loss = 0.0421\n",
      "epoch 70 / 100, step 76/80, loss = 0.0406\n",
      "epoch 70 / 100, step 77/80, loss = 0.0360\n",
      "epoch 70 / 100, step 78/80, loss = 0.0330\n",
      "epoch 70 / 100, step 79/80, loss = 0.0350\n",
      "epoch 70 / 100, step 80/80, loss = 0.0391\n",
      "epoch 71 / 100, step 1/80, loss = 0.0381\n",
      "epoch 71 / 100, step 2/80, loss = 0.0380\n",
      "epoch 71 / 100, step 3/80, loss = 0.0410\n",
      "epoch 71 / 100, step 4/80, loss = 0.0397\n",
      "epoch 71 / 100, step 5/80, loss = 0.0375\n",
      "epoch 71 / 100, step 6/80, loss = 0.0370\n",
      "epoch 71 / 100, step 7/80, loss = 0.0378\n",
      "epoch 71 / 100, step 8/80, loss = 0.0369\n",
      "epoch 71 / 100, step 9/80, loss = 0.0360\n",
      "epoch 71 / 100, step 10/80, loss = 0.0353\n",
      "epoch 71 / 100, step 11/80, loss = 0.0364\n",
      "epoch 71 / 100, step 12/80, loss = 0.0377\n",
      "epoch 71 / 100, step 13/80, loss = 0.0383\n",
      "epoch 71 / 100, step 14/80, loss = 0.0376\n",
      "epoch 71 / 100, step 15/80, loss = 0.0362\n",
      "epoch 71 / 100, step 16/80, loss = 0.0400\n",
      "epoch 71 / 100, step 17/80, loss = 0.0371\n",
      "epoch 71 / 100, step 18/80, loss = 0.0370\n",
      "epoch 71 / 100, step 19/80, loss = 0.0385\n",
      "epoch 71 / 100, step 20/80, loss = 0.0385\n",
      "epoch 71 / 100, step 21/80, loss = 0.0353\n",
      "epoch 71 / 100, step 22/80, loss = 0.0381\n",
      "epoch 71 / 100, step 23/80, loss = 0.0416\n",
      "epoch 71 / 100, step 24/80, loss = 0.0349\n",
      "epoch 71 / 100, step 25/80, loss = 0.0402\n",
      "epoch 71 / 100, step 26/80, loss = 0.0396\n",
      "epoch 71 / 100, step 27/80, loss = 0.0368\n",
      "epoch 71 / 100, step 28/80, loss = 0.0411\n",
      "epoch 71 / 100, step 29/80, loss = 0.0396\n",
      "epoch 71 / 100, step 30/80, loss = 0.0353\n",
      "epoch 71 / 100, step 31/80, loss = 0.0468\n",
      "epoch 71 / 100, step 32/80, loss = 0.0359\n",
      "epoch 71 / 100, step 33/80, loss = 0.0369\n",
      "epoch 71 / 100, step 34/80, loss = 0.0377\n",
      "epoch 71 / 100, step 35/80, loss = 0.0374\n",
      "epoch 71 / 100, step 36/80, loss = 0.0388\n",
      "epoch 71 / 100, step 37/80, loss = 0.0528\n",
      "epoch 71 / 100, step 38/80, loss = 0.0409\n",
      "epoch 71 / 100, step 39/80, loss = 0.0413\n",
      "epoch 71 / 100, step 40/80, loss = 0.0376\n",
      "epoch 71 / 100, step 41/80, loss = 0.0391\n",
      "epoch 71 / 100, step 42/80, loss = 0.0385\n",
      "epoch 71 / 100, step 43/80, loss = 0.0382\n",
      "epoch 71 / 100, step 44/80, loss = 0.0399\n",
      "epoch 71 / 100, step 45/80, loss = 0.0379\n",
      "epoch 71 / 100, step 46/80, loss = 0.0381\n",
      "epoch 71 / 100, step 47/80, loss = 0.0377\n",
      "epoch 71 / 100, step 48/80, loss = 0.0366\n",
      "epoch 71 / 100, step 49/80, loss = 0.0344\n",
      "epoch 71 / 100, step 50/80, loss = 0.0360\n",
      "epoch 71 / 100, step 51/80, loss = 0.0359\n",
      "epoch 71 / 100, step 52/80, loss = 0.0411\n",
      "epoch 71 / 100, step 53/80, loss = 0.0457\n",
      "epoch 71 / 100, step 54/80, loss = 0.0364\n",
      "epoch 71 / 100, step 55/80, loss = 0.0410\n",
      "epoch 71 / 100, step 56/80, loss = 0.0353\n",
      "epoch 71 / 100, step 57/80, loss = 0.0368\n",
      "epoch 71 / 100, step 58/80, loss = 0.0378\n",
      "epoch 71 / 100, step 59/80, loss = 0.0429\n",
      "epoch 71 / 100, step 60/80, loss = 0.0398\n",
      "epoch 71 / 100, step 61/80, loss = 0.0374\n",
      "epoch 71 / 100, step 62/80, loss = 0.0364\n",
      "epoch 71 / 100, step 63/80, loss = 0.0405\n",
      "epoch 71 / 100, step 64/80, loss = 0.0371\n",
      "epoch 71 / 100, step 65/80, loss = 0.0376\n",
      "epoch 71 / 100, step 66/80, loss = 0.0395\n",
      "epoch 71 / 100, step 67/80, loss = 0.0466\n",
      "epoch 71 / 100, step 68/80, loss = 0.0433\n",
      "epoch 71 / 100, step 69/80, loss = 0.0401\n",
      "epoch 71 / 100, step 70/80, loss = 0.0382\n",
      "epoch 71 / 100, step 71/80, loss = 0.0419\n",
      "epoch 71 / 100, step 72/80, loss = 0.0370\n",
      "epoch 71 / 100, step 73/80, loss = 0.0377\n",
      "epoch 71 / 100, step 74/80, loss = 0.0410\n",
      "epoch 71 / 100, step 75/80, loss = 0.0399\n",
      "epoch 71 / 100, step 76/80, loss = 0.0403\n",
      "epoch 71 / 100, step 77/80, loss = 0.0374\n",
      "epoch 71 / 100, step 78/80, loss = 0.0364\n",
      "epoch 71 / 100, step 79/80, loss = 0.0398\n",
      "epoch 71 / 100, step 80/80, loss = 0.0395\n",
      "epoch 72 / 100, step 1/80, loss = 0.0384\n",
      "epoch 72 / 100, step 2/80, loss = 0.0378\n",
      "epoch 72 / 100, step 3/80, loss = 0.0379\n",
      "epoch 72 / 100, step 4/80, loss = 0.0356\n",
      "epoch 72 / 100, step 5/80, loss = 0.0424\n",
      "epoch 72 / 100, step 6/80, loss = 0.0386\n",
      "epoch 72 / 100, step 7/80, loss = 0.0325\n",
      "epoch 72 / 100, step 8/80, loss = 0.0420\n",
      "epoch 72 / 100, step 9/80, loss = 0.0356\n",
      "epoch 72 / 100, step 10/80, loss = 0.0390\n",
      "epoch 72 / 100, step 11/80, loss = 0.0334\n",
      "epoch 72 / 100, step 12/80, loss = 0.0389\n",
      "epoch 72 / 100, step 13/80, loss = 0.0357\n",
      "epoch 72 / 100, step 14/80, loss = 0.0366\n",
      "epoch 72 / 100, step 15/80, loss = 0.0352\n",
      "epoch 72 / 100, step 16/80, loss = 0.0387\n",
      "epoch 72 / 100, step 17/80, loss = 0.0341\n",
      "epoch 72 / 100, step 18/80, loss = 0.0348\n",
      "epoch 72 / 100, step 19/80, loss = 0.0369\n",
      "epoch 72 / 100, step 20/80, loss = 0.0381\n",
      "epoch 72 / 100, step 21/80, loss = 0.0359\n",
      "epoch 72 / 100, step 22/80, loss = 0.0429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72 / 100, step 23/80, loss = 0.0341\n",
      "epoch 72 / 100, step 24/80, loss = 0.0371\n",
      "epoch 72 / 100, step 25/80, loss = 0.0358\n",
      "epoch 72 / 100, step 26/80, loss = 0.0360\n",
      "epoch 72 / 100, step 27/80, loss = 0.0345\n",
      "epoch 72 / 100, step 28/80, loss = 0.0365\n",
      "epoch 72 / 100, step 29/80, loss = 0.0354\n",
      "epoch 72 / 100, step 30/80, loss = 0.0353\n",
      "epoch 72 / 100, step 31/80, loss = 0.0338\n",
      "epoch 72 / 100, step 32/80, loss = 0.0353\n",
      "epoch 72 / 100, step 33/80, loss = 0.0435\n",
      "epoch 72 / 100, step 34/80, loss = 0.0360\n",
      "epoch 72 / 100, step 35/80, loss = 0.0379\n",
      "epoch 72 / 100, step 36/80, loss = 0.0393\n",
      "epoch 72 / 100, step 37/80, loss = 0.0368\n",
      "epoch 72 / 100, step 38/80, loss = 0.0391\n",
      "epoch 72 / 100, step 39/80, loss = 0.0392\n",
      "epoch 72 / 100, step 40/80, loss = 0.0358\n",
      "epoch 72 / 100, step 41/80, loss = 0.0364\n",
      "epoch 72 / 100, step 42/80, loss = 0.0353\n",
      "epoch 72 / 100, step 43/80, loss = 0.0371\n",
      "epoch 72 / 100, step 44/80, loss = 0.0355\n",
      "epoch 72 / 100, step 45/80, loss = 0.0387\n",
      "epoch 72 / 100, step 46/80, loss = 0.0356\n",
      "epoch 72 / 100, step 47/80, loss = 0.0379\n",
      "epoch 72 / 100, step 48/80, loss = 0.0365\n",
      "epoch 72 / 100, step 49/80, loss = 0.0416\n",
      "epoch 72 / 100, step 50/80, loss = 0.0416\n",
      "epoch 72 / 100, step 51/80, loss = 0.0378\n",
      "epoch 72 / 100, step 52/80, loss = 0.0373\n",
      "epoch 72 / 100, step 53/80, loss = 0.0356\n",
      "epoch 72 / 100, step 54/80, loss = 0.0373\n",
      "epoch 72 / 100, step 55/80, loss = 0.0366\n",
      "epoch 72 / 100, step 56/80, loss = 0.0469\n",
      "epoch 72 / 100, step 57/80, loss = 0.0408\n",
      "epoch 72 / 100, step 58/80, loss = 0.0425\n",
      "epoch 72 / 100, step 59/80, loss = 0.0389\n",
      "epoch 72 / 100, step 60/80, loss = 0.0386\n",
      "epoch 72 / 100, step 61/80, loss = 0.0362\n",
      "epoch 72 / 100, step 62/80, loss = 0.0361\n",
      "epoch 72 / 100, step 63/80, loss = 0.0383\n",
      "epoch 72 / 100, step 64/80, loss = 0.0417\n",
      "epoch 72 / 100, step 65/80, loss = 0.0385\n",
      "epoch 72 / 100, step 66/80, loss = 0.0372\n",
      "epoch 72 / 100, step 67/80, loss = 0.0405\n",
      "epoch 72 / 100, step 68/80, loss = 0.0357\n",
      "epoch 72 / 100, step 69/80, loss = 0.0374\n",
      "epoch 72 / 100, step 70/80, loss = 0.0365\n",
      "epoch 72 / 100, step 71/80, loss = 0.0388\n",
      "epoch 72 / 100, step 72/80, loss = 0.0367\n",
      "epoch 72 / 100, step 73/80, loss = 0.0483\n",
      "epoch 72 / 100, step 74/80, loss = 0.0407\n",
      "epoch 72 / 100, step 75/80, loss = 0.0389\n",
      "epoch 72 / 100, step 76/80, loss = 0.0397\n",
      "epoch 72 / 100, step 77/80, loss = 0.0385\n",
      "epoch 72 / 100, step 78/80, loss = 0.0393\n",
      "epoch 72 / 100, step 79/80, loss = 0.0380\n",
      "epoch 72 / 100, step 80/80, loss = 0.0370\n",
      "epoch 73 / 100, step 1/80, loss = 0.0380\n",
      "epoch 73 / 100, step 2/80, loss = 0.0429\n",
      "epoch 73 / 100, step 3/80, loss = 0.0389\n",
      "epoch 73 / 100, step 4/80, loss = 0.0376\n",
      "epoch 73 / 100, step 5/80, loss = 0.0360\n",
      "epoch 73 / 100, step 6/80, loss = 0.0393\n",
      "epoch 73 / 100, step 7/80, loss = 0.0394\n",
      "epoch 73 / 100, step 8/80, loss = 0.0360\n",
      "epoch 73 / 100, step 9/80, loss = 0.0377\n",
      "epoch 73 / 100, step 10/80, loss = 0.0366\n",
      "epoch 73 / 100, step 11/80, loss = 0.0407\n",
      "epoch 73 / 100, step 12/80, loss = 0.0345\n",
      "epoch 73 / 100, step 13/80, loss = 0.0362\n",
      "epoch 73 / 100, step 14/80, loss = 0.0346\n",
      "epoch 73 / 100, step 15/80, loss = 0.0382\n",
      "epoch 73 / 100, step 16/80, loss = 0.0407\n",
      "epoch 73 / 100, step 17/80, loss = 0.0336\n",
      "epoch 73 / 100, step 18/80, loss = 0.0348\n",
      "epoch 73 / 100, step 19/80, loss = 0.0372\n",
      "epoch 73 / 100, step 20/80, loss = 0.0379\n",
      "epoch 73 / 100, step 21/80, loss = 0.0405\n",
      "epoch 73 / 100, step 22/80, loss = 0.0353\n",
      "epoch 73 / 100, step 23/80, loss = 0.0377\n",
      "epoch 73 / 100, step 24/80, loss = 0.0339\n",
      "epoch 73 / 100, step 25/80, loss = 0.0361\n",
      "epoch 73 / 100, step 26/80, loss = 0.0352\n",
      "epoch 73 / 100, step 27/80, loss = 0.0343\n",
      "epoch 73 / 100, step 28/80, loss = 0.0356\n",
      "epoch 73 / 100, step 29/80, loss = 0.0383\n",
      "epoch 73 / 100, step 30/80, loss = 0.0344\n",
      "epoch 73 / 100, step 31/80, loss = 0.0418\n",
      "epoch 73 / 100, step 32/80, loss = 0.0347\n",
      "epoch 73 / 100, step 33/80, loss = 0.0354\n",
      "epoch 73 / 100, step 34/80, loss = 0.0402\n",
      "epoch 73 / 100, step 35/80, loss = 0.0350\n",
      "epoch 73 / 100, step 36/80, loss = 0.0360\n",
      "epoch 73 / 100, step 37/80, loss = 0.0415\n",
      "epoch 73 / 100, step 38/80, loss = 0.0426\n",
      "epoch 73 / 100, step 39/80, loss = 0.0396\n",
      "epoch 73 / 100, step 40/80, loss = 0.0437\n",
      "epoch 73 / 100, step 41/80, loss = 0.0351\n",
      "epoch 73 / 100, step 42/80, loss = 0.0384\n",
      "epoch 73 / 100, step 43/80, loss = 0.0357\n",
      "epoch 73 / 100, step 44/80, loss = 0.0335\n",
      "epoch 73 / 100, step 45/80, loss = 0.0350\n",
      "epoch 73 / 100, step 46/80, loss = 0.0442\n",
      "epoch 73 / 100, step 47/80, loss = 0.0363\n",
      "epoch 73 / 100, step 48/80, loss = 0.0432\n",
      "epoch 73 / 100, step 49/80, loss = 0.0387\n",
      "epoch 73 / 100, step 50/80, loss = 0.0393\n",
      "epoch 73 / 100, step 51/80, loss = 0.0352\n",
      "epoch 73 / 100, step 52/80, loss = 0.0402\n",
      "epoch 73 / 100, step 53/80, loss = 0.0321\n",
      "epoch 73 / 100, step 54/80, loss = 0.0417\n",
      "epoch 73 / 100, step 55/80, loss = 0.0342\n",
      "epoch 73 / 100, step 56/80, loss = 0.0370\n",
      "epoch 73 / 100, step 57/80, loss = 0.0360\n",
      "epoch 73 / 100, step 58/80, loss = 0.0370\n",
      "epoch 73 / 100, step 59/80, loss = 0.0398\n",
      "epoch 73 / 100, step 60/80, loss = 0.0386\n",
      "epoch 73 / 100, step 61/80, loss = 0.0364\n",
      "epoch 73 / 100, step 62/80, loss = 0.0373\n",
      "epoch 73 / 100, step 63/80, loss = 0.0347\n",
      "epoch 73 / 100, step 64/80, loss = 0.0347\n",
      "epoch 73 / 100, step 65/80, loss = 0.0348\n",
      "epoch 73 / 100, step 66/80, loss = 0.0363\n",
      "epoch 73 / 100, step 67/80, loss = 0.0371\n",
      "epoch 73 / 100, step 68/80, loss = 0.0351\n",
      "epoch 73 / 100, step 69/80, loss = 0.0387\n",
      "epoch 73 / 100, step 70/80, loss = 0.0360\n",
      "epoch 73 / 100, step 71/80, loss = 0.0359\n",
      "epoch 73 / 100, step 72/80, loss = 0.0365\n",
      "epoch 73 / 100, step 73/80, loss = 0.0353\n",
      "epoch 73 / 100, step 74/80, loss = 0.0378\n",
      "epoch 73 / 100, step 75/80, loss = 0.0348\n",
      "epoch 73 / 100, step 76/80, loss = 0.0364\n",
      "epoch 73 / 100, step 77/80, loss = 0.0380\n",
      "epoch 73 / 100, step 78/80, loss = 0.0325\n",
      "epoch 73 / 100, step 79/80, loss = 0.0339\n",
      "epoch 73 / 100, step 80/80, loss = 0.0355\n",
      "epoch 74 / 100, step 1/80, loss = 0.0351\n",
      "epoch 74 / 100, step 2/80, loss = 0.0346\n",
      "epoch 74 / 100, step 3/80, loss = 0.0343\n",
      "epoch 74 / 100, step 4/80, loss = 0.0332\n",
      "epoch 74 / 100, step 5/80, loss = 0.0326\n",
      "epoch 74 / 100, step 6/80, loss = 0.0339\n",
      "epoch 74 / 100, step 7/80, loss = 0.0367\n",
      "epoch 74 / 100, step 8/80, loss = 0.0418\n",
      "epoch 74 / 100, step 9/80, loss = 0.0380\n",
      "epoch 74 / 100, step 10/80, loss = 0.0387\n",
      "epoch 74 / 100, step 11/80, loss = 0.0355\n",
      "epoch 74 / 100, step 12/80, loss = 0.0359\n",
      "epoch 74 / 100, step 13/80, loss = 0.0370\n",
      "epoch 74 / 100, step 14/80, loss = 0.0362\n",
      "epoch 74 / 100, step 15/80, loss = 0.0353\n",
      "epoch 74 / 100, step 16/80, loss = 0.0373\n",
      "epoch 74 / 100, step 17/80, loss = 0.0354\n",
      "epoch 74 / 100, step 18/80, loss = 0.0348\n",
      "epoch 74 / 100, step 19/80, loss = 0.0397\n",
      "epoch 74 / 100, step 20/80, loss = 0.0360\n",
      "epoch 74 / 100, step 21/80, loss = 0.0334\n",
      "epoch 74 / 100, step 22/80, loss = 0.0344\n",
      "epoch 74 / 100, step 23/80, loss = 0.0333\n",
      "epoch 74 / 100, step 24/80, loss = 0.0337\n",
      "epoch 74 / 100, step 25/80, loss = 0.0411\n",
      "epoch 74 / 100, step 26/80, loss = 0.0343\n",
      "epoch 74 / 100, step 27/80, loss = 0.0399\n",
      "epoch 74 / 100, step 28/80, loss = 0.0373\n",
      "epoch 74 / 100, step 29/80, loss = 0.0365\n",
      "epoch 74 / 100, step 30/80, loss = 0.0346\n",
      "epoch 74 / 100, step 31/80, loss = 0.0362\n",
      "epoch 74 / 100, step 32/80, loss = 0.0359\n",
      "epoch 74 / 100, step 33/80, loss = 0.0331\n",
      "epoch 74 / 100, step 34/80, loss = 0.0370\n",
      "epoch 74 / 100, step 35/80, loss = 0.0333\n",
      "epoch 74 / 100, step 36/80, loss = 0.0346\n",
      "epoch 74 / 100, step 37/80, loss = 0.0360\n",
      "epoch 74 / 100, step 38/80, loss = 0.0374\n",
      "epoch 74 / 100, step 39/80, loss = 0.0351\n",
      "epoch 74 / 100, step 40/80, loss = 0.0320\n",
      "epoch 74 / 100, step 41/80, loss = 0.0439\n",
      "epoch 74 / 100, step 42/80, loss = 0.0358\n",
      "epoch 74 / 100, step 43/80, loss = 0.0352\n",
      "epoch 74 / 100, step 44/80, loss = 0.0371\n",
      "epoch 74 / 100, step 45/80, loss = 0.0352\n",
      "epoch 74 / 100, step 46/80, loss = 0.0367\n",
      "epoch 74 / 100, step 47/80, loss = 0.0343\n",
      "epoch 74 / 100, step 48/80, loss = 0.0366\n",
      "epoch 74 / 100, step 49/80, loss = 0.0352\n",
      "epoch 74 / 100, step 50/80, loss = 0.0349\n",
      "epoch 74 / 100, step 51/80, loss = 0.0363\n",
      "epoch 74 / 100, step 52/80, loss = 0.0356\n",
      "epoch 74 / 100, step 53/80, loss = 0.0363\n",
      "epoch 74 / 100, step 54/80, loss = 0.0390\n",
      "epoch 74 / 100, step 55/80, loss = 0.0321\n",
      "epoch 74 / 100, step 56/80, loss = 0.0347\n",
      "epoch 74 / 100, step 57/80, loss = 0.0354\n",
      "epoch 74 / 100, step 58/80, loss = 0.0385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74 / 100, step 59/80, loss = 0.0410\n",
      "epoch 74 / 100, step 60/80, loss = 0.0363\n",
      "epoch 74 / 100, step 61/80, loss = 0.0333\n",
      "epoch 74 / 100, step 62/80, loss = 0.0349\n",
      "epoch 74 / 100, step 63/80, loss = 0.0382\n",
      "epoch 74 / 100, step 64/80, loss = 0.0384\n",
      "epoch 74 / 100, step 65/80, loss = 0.0353\n",
      "epoch 74 / 100, step 66/80, loss = 0.0360\n",
      "epoch 74 / 100, step 67/80, loss = 0.0374\n",
      "epoch 74 / 100, step 68/80, loss = 0.0345\n",
      "epoch 74 / 100, step 69/80, loss = 0.0462\n",
      "epoch 74 / 100, step 70/80, loss = 0.0367\n",
      "epoch 74 / 100, step 71/80, loss = 0.0343\n",
      "epoch 74 / 100, step 72/80, loss = 0.0336\n",
      "epoch 74 / 100, step 73/80, loss = 0.0364\n",
      "epoch 74 / 100, step 74/80, loss = 0.0335\n",
      "epoch 74 / 100, step 75/80, loss = 0.0360\n",
      "epoch 74 / 100, step 76/80, loss = 0.0330\n",
      "epoch 74 / 100, step 77/80, loss = 0.0380\n",
      "epoch 74 / 100, step 78/80, loss = 0.0360\n",
      "epoch 74 / 100, step 79/80, loss = 0.0472\n",
      "epoch 74 / 100, step 80/80, loss = 0.0373\n",
      "epoch 75 / 100, step 1/80, loss = 0.0344\n",
      "epoch 75 / 100, step 2/80, loss = 0.0371\n",
      "epoch 75 / 100, step 3/80, loss = 0.0363\n",
      "epoch 75 / 100, step 4/80, loss = 0.0348\n",
      "epoch 75 / 100, step 5/80, loss = 0.0363\n",
      "epoch 75 / 100, step 6/80, loss = 0.0316\n",
      "epoch 75 / 100, step 7/80, loss = 0.0325\n",
      "epoch 75 / 100, step 8/80, loss = 0.0397\n",
      "epoch 75 / 100, step 9/80, loss = 0.0344\n",
      "epoch 75 / 100, step 10/80, loss = 0.0361\n",
      "epoch 75 / 100, step 11/80, loss = 0.0368\n",
      "epoch 75 / 100, step 12/80, loss = 0.0330\n",
      "epoch 75 / 100, step 13/80, loss = 0.0328\n",
      "epoch 75 / 100, step 14/80, loss = 0.0342\n",
      "epoch 75 / 100, step 15/80, loss = 0.0330\n",
      "epoch 75 / 100, step 16/80, loss = 0.0336\n",
      "epoch 75 / 100, step 17/80, loss = 0.0322\n",
      "epoch 75 / 100, step 18/80, loss = 0.0336\n",
      "epoch 75 / 100, step 19/80, loss = 0.0359\n",
      "epoch 75 / 100, step 20/80, loss = 0.0337\n",
      "epoch 75 / 100, step 21/80, loss = 0.0367\n",
      "epoch 75 / 100, step 22/80, loss = 0.0333\n",
      "epoch 75 / 100, step 23/80, loss = 0.0367\n",
      "epoch 75 / 100, step 24/80, loss = 0.0313\n",
      "epoch 75 / 100, step 25/80, loss = 0.0313\n",
      "epoch 75 / 100, step 26/80, loss = 0.0335\n",
      "epoch 75 / 100, step 27/80, loss = 0.0373\n",
      "epoch 75 / 100, step 28/80, loss = 0.0342\n",
      "epoch 75 / 100, step 29/80, loss = 0.0357\n",
      "epoch 75 / 100, step 30/80, loss = 0.0363\n",
      "epoch 75 / 100, step 31/80, loss = 0.0352\n",
      "epoch 75 / 100, step 32/80, loss = 0.0339\n",
      "epoch 75 / 100, step 33/80, loss = 0.0353\n",
      "epoch 75 / 100, step 34/80, loss = 0.0328\n",
      "epoch 75 / 100, step 35/80, loss = 0.0332\n",
      "epoch 75 / 100, step 36/80, loss = 0.0383\n",
      "epoch 75 / 100, step 37/80, loss = 0.0361\n",
      "epoch 75 / 100, step 38/80, loss = 0.0322\n",
      "epoch 75 / 100, step 39/80, loss = 0.0377\n",
      "epoch 75 / 100, step 40/80, loss = 0.0360\n",
      "epoch 75 / 100, step 41/80, loss = 0.0361\n",
      "epoch 75 / 100, step 42/80, loss = 0.0358\n",
      "epoch 75 / 100, step 43/80, loss = 0.0339\n",
      "epoch 75 / 100, step 44/80, loss = 0.0347\n",
      "epoch 75 / 100, step 45/80, loss = 0.0326\n",
      "epoch 75 / 100, step 46/80, loss = 0.0336\n",
      "epoch 75 / 100, step 47/80, loss = 0.0346\n",
      "epoch 75 / 100, step 48/80, loss = 0.0351\n",
      "epoch 75 / 100, step 49/80, loss = 0.0345\n",
      "epoch 75 / 100, step 50/80, loss = 0.0319\n",
      "epoch 75 / 100, step 51/80, loss = 0.0372\n",
      "epoch 75 / 100, step 52/80, loss = 0.0354\n",
      "epoch 75 / 100, step 53/80, loss = 0.0353\n",
      "epoch 75 / 100, step 54/80, loss = 0.0358\n",
      "epoch 75 / 100, step 55/80, loss = 0.0360\n",
      "epoch 75 / 100, step 56/80, loss = 0.0368\n",
      "epoch 75 / 100, step 57/80, loss = 0.0350\n",
      "epoch 75 / 100, step 58/80, loss = 0.0373\n",
      "epoch 75 / 100, step 59/80, loss = 0.0361\n",
      "epoch 75 / 100, step 60/80, loss = 0.0415\n",
      "epoch 75 / 100, step 61/80, loss = 0.0361\n",
      "epoch 75 / 100, step 62/80, loss = 0.0339\n",
      "epoch 75 / 100, step 63/80, loss = 0.0388\n",
      "epoch 75 / 100, step 64/80, loss = 0.0356\n",
      "epoch 75 / 100, step 65/80, loss = 0.0334\n",
      "epoch 75 / 100, step 66/80, loss = 0.0360\n",
      "epoch 75 / 100, step 67/80, loss = 0.0372\n",
      "epoch 75 / 100, step 68/80, loss = 0.0358\n",
      "epoch 75 / 100, step 69/80, loss = 0.0366\n",
      "epoch 75 / 100, step 70/80, loss = 0.0356\n",
      "epoch 75 / 100, step 71/80, loss = 0.0371\n",
      "epoch 75 / 100, step 72/80, loss = 0.0392\n",
      "epoch 75 / 100, step 73/80, loss = 0.0413\n",
      "epoch 75 / 100, step 74/80, loss = 0.0393\n",
      "epoch 75 / 100, step 75/80, loss = 0.0376\n",
      "epoch 75 / 100, step 76/80, loss = 0.0371\n",
      "epoch 75 / 100, step 77/80, loss = 0.0327\n",
      "epoch 75 / 100, step 78/80, loss = 0.0379\n",
      "epoch 75 / 100, step 79/80, loss = 0.0331\n",
      "epoch 75 / 100, step 80/80, loss = 0.0361\n",
      "epoch 76 / 100, step 1/80, loss = 0.0338\n",
      "epoch 76 / 100, step 2/80, loss = 0.0359\n",
      "epoch 76 / 100, step 3/80, loss = 0.0327\n",
      "epoch 76 / 100, step 4/80, loss = 0.0357\n",
      "epoch 76 / 100, step 5/80, loss = 0.0345\n",
      "epoch 76 / 100, step 6/80, loss = 0.0341\n",
      "epoch 76 / 100, step 7/80, loss = 0.0342\n",
      "epoch 76 / 100, step 8/80, loss = 0.0380\n",
      "epoch 76 / 100, step 9/80, loss = 0.0373\n",
      "epoch 76 / 100, step 10/80, loss = 0.0370\n",
      "epoch 76 / 100, step 11/80, loss = 0.0354\n",
      "epoch 76 / 100, step 12/80, loss = 0.0352\n",
      "epoch 76 / 100, step 13/80, loss = 0.0399\n",
      "epoch 76 / 100, step 14/80, loss = 0.0375\n",
      "epoch 76 / 100, step 15/80, loss = 0.0361\n",
      "epoch 76 / 100, step 16/80, loss = 0.0376\n",
      "epoch 76 / 100, step 17/80, loss = 0.0399\n",
      "epoch 76 / 100, step 18/80, loss = 0.0325\n",
      "epoch 76 / 100, step 19/80, loss = 0.0341\n",
      "epoch 76 / 100, step 20/80, loss = 0.0336\n",
      "epoch 76 / 100, step 21/80, loss = 0.0322\n",
      "epoch 76 / 100, step 22/80, loss = 0.0386\n",
      "epoch 76 / 100, step 23/80, loss = 0.0333\n",
      "epoch 76 / 100, step 24/80, loss = 0.0335\n",
      "epoch 76 / 100, step 25/80, loss = 0.0348\n",
      "epoch 76 / 100, step 26/80, loss = 0.0341\n",
      "epoch 76 / 100, step 27/80, loss = 0.0346\n",
      "epoch 76 / 100, step 28/80, loss = 0.0340\n",
      "epoch 76 / 100, step 29/80, loss = 0.0342\n",
      "epoch 76 / 100, step 30/80, loss = 0.0356\n",
      "epoch 76 / 100, step 31/80, loss = 0.0307\n",
      "epoch 76 / 100, step 32/80, loss = 0.0338\n",
      "epoch 76 / 100, step 33/80, loss = 0.0382\n",
      "epoch 76 / 100, step 34/80, loss = 0.0378\n",
      "epoch 76 / 100, step 35/80, loss = 0.0349\n",
      "epoch 76 / 100, step 36/80, loss = 0.0354\n",
      "epoch 76 / 100, step 37/80, loss = 0.0403\n",
      "epoch 76 / 100, step 38/80, loss = 0.0347\n",
      "epoch 76 / 100, step 39/80, loss = 0.0352\n",
      "epoch 76 / 100, step 40/80, loss = 0.0335\n",
      "epoch 76 / 100, step 41/80, loss = 0.0344\n",
      "epoch 76 / 100, step 42/80, loss = 0.0368\n",
      "epoch 76 / 100, step 43/80, loss = 0.0374\n",
      "epoch 76 / 100, step 44/80, loss = 0.0334\n",
      "epoch 76 / 100, step 45/80, loss = 0.0382\n",
      "epoch 76 / 100, step 46/80, loss = 0.0336\n",
      "epoch 76 / 100, step 47/80, loss = 0.0384\n",
      "epoch 76 / 100, step 48/80, loss = 0.0335\n",
      "epoch 76 / 100, step 49/80, loss = 0.0365\n",
      "epoch 76 / 100, step 50/80, loss = 0.0337\n",
      "epoch 76 / 100, step 51/80, loss = 0.0353\n",
      "epoch 76 / 100, step 52/80, loss = 0.0342\n",
      "epoch 76 / 100, step 53/80, loss = 0.0361\n",
      "epoch 76 / 100, step 54/80, loss = 0.0321\n",
      "epoch 76 / 100, step 55/80, loss = 0.0356\n",
      "epoch 76 / 100, step 56/80, loss = 0.0340\n",
      "epoch 76 / 100, step 57/80, loss = 0.0337\n",
      "epoch 76 / 100, step 58/80, loss = 0.0346\n",
      "epoch 76 / 100, step 59/80, loss = 0.0330\n",
      "epoch 76 / 100, step 60/80, loss = 0.0367\n",
      "epoch 76 / 100, step 61/80, loss = 0.0325\n",
      "epoch 76 / 100, step 62/80, loss = 0.0354\n",
      "epoch 76 / 100, step 63/80, loss = 0.0348\n",
      "epoch 76 / 100, step 64/80, loss = 0.0319\n",
      "epoch 76 / 100, step 65/80, loss = 0.0343\n",
      "epoch 76 / 100, step 66/80, loss = 0.0425\n",
      "epoch 76 / 100, step 67/80, loss = 0.0330\n",
      "epoch 76 / 100, step 68/80, loss = 0.0333\n",
      "epoch 76 / 100, step 69/80, loss = 0.0366\n",
      "epoch 76 / 100, step 70/80, loss = 0.0353\n",
      "epoch 76 / 100, step 71/80, loss = 0.0339\n",
      "epoch 76 / 100, step 72/80, loss = 0.0362\n",
      "epoch 76 / 100, step 73/80, loss = 0.0407\n",
      "epoch 76 / 100, step 74/80, loss = 0.0337\n",
      "epoch 76 / 100, step 75/80, loss = 0.0318\n",
      "epoch 76 / 100, step 76/80, loss = 0.0366\n",
      "epoch 76 / 100, step 77/80, loss = 0.0350\n",
      "epoch 76 / 100, step 78/80, loss = 0.0357\n",
      "epoch 76 / 100, step 79/80, loss = 0.0340\n",
      "epoch 76 / 100, step 80/80, loss = 0.0333\n",
      "epoch 77 / 100, step 1/80, loss = 0.0327\n",
      "epoch 77 / 100, step 2/80, loss = 0.0323\n",
      "epoch 77 / 100, step 3/80, loss = 0.0363\n",
      "epoch 77 / 100, step 4/80, loss = 0.0332\n",
      "epoch 77 / 100, step 5/80, loss = 0.0323\n",
      "epoch 77 / 100, step 6/80, loss = 0.0335\n",
      "epoch 77 / 100, step 7/80, loss = 0.0357\n",
      "epoch 77 / 100, step 8/80, loss = 0.0336\n",
      "epoch 77 / 100, step 9/80, loss = 0.0341\n",
      "epoch 77 / 100, step 10/80, loss = 0.0347\n",
      "epoch 77 / 100, step 11/80, loss = 0.0364\n",
      "epoch 77 / 100, step 12/80, loss = 0.0376\n",
      "epoch 77 / 100, step 13/80, loss = 0.0358\n",
      "epoch 77 / 100, step 14/80, loss = 0.0342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77 / 100, step 15/80, loss = 0.0315\n",
      "epoch 77 / 100, step 16/80, loss = 0.0347\n",
      "epoch 77 / 100, step 17/80, loss = 0.0321\n",
      "epoch 77 / 100, step 18/80, loss = 0.0361\n",
      "epoch 77 / 100, step 19/80, loss = 0.0331\n",
      "epoch 77 / 100, step 20/80, loss = 0.0366\n",
      "epoch 77 / 100, step 21/80, loss = 0.0360\n",
      "epoch 77 / 100, step 22/80, loss = 0.0309\n",
      "epoch 77 / 100, step 23/80, loss = 0.0308\n",
      "epoch 77 / 100, step 24/80, loss = 0.0309\n",
      "epoch 77 / 100, step 25/80, loss = 0.0376\n",
      "epoch 77 / 100, step 26/80, loss = 0.0329\n",
      "epoch 77 / 100, step 27/80, loss = 0.0343\n",
      "epoch 77 / 100, step 28/80, loss = 0.0336\n",
      "epoch 77 / 100, step 29/80, loss = 0.0295\n",
      "epoch 77 / 100, step 30/80, loss = 0.0339\n",
      "epoch 77 / 100, step 31/80, loss = 0.0329\n",
      "epoch 77 / 100, step 32/80, loss = 0.0374\n",
      "epoch 77 / 100, step 33/80, loss = 0.0378\n",
      "epoch 77 / 100, step 34/80, loss = 0.0321\n",
      "epoch 77 / 100, step 35/80, loss = 0.0342\n",
      "epoch 77 / 100, step 36/80, loss = 0.0357\n",
      "epoch 77 / 100, step 37/80, loss = 0.0360\n",
      "epoch 77 / 100, step 38/80, loss = 0.0338\n",
      "epoch 77 / 100, step 39/80, loss = 0.0347\n",
      "epoch 77 / 100, step 40/80, loss = 0.0322\n",
      "epoch 77 / 100, step 41/80, loss = 0.0327\n",
      "epoch 77 / 100, step 42/80, loss = 0.0335\n",
      "epoch 77 / 100, step 43/80, loss = 0.0330\n",
      "epoch 77 / 100, step 44/80, loss = 0.0369\n",
      "epoch 77 / 100, step 45/80, loss = 0.0384\n",
      "epoch 77 / 100, step 46/80, loss = 0.0371\n",
      "epoch 77 / 100, step 47/80, loss = 0.0350\n",
      "epoch 77 / 100, step 48/80, loss = 0.0343\n",
      "epoch 77 / 100, step 49/80, loss = 0.0317\n",
      "epoch 77 / 100, step 50/80, loss = 0.0337\n",
      "epoch 77 / 100, step 51/80, loss = 0.0382\n",
      "epoch 77 / 100, step 52/80, loss = 0.0368\n",
      "epoch 77 / 100, step 53/80, loss = 0.0342\n",
      "epoch 77 / 100, step 54/80, loss = 0.0408\n",
      "epoch 77 / 100, step 55/80, loss = 0.0397\n",
      "epoch 77 / 100, step 56/80, loss = 0.0363\n",
      "epoch 77 / 100, step 57/80, loss = 0.0340\n",
      "epoch 77 / 100, step 58/80, loss = 0.0357\n",
      "epoch 77 / 100, step 59/80, loss = 0.0382\n",
      "epoch 77 / 100, step 60/80, loss = 0.0360\n",
      "epoch 77 / 100, step 61/80, loss = 0.0355\n",
      "epoch 77 / 100, step 62/80, loss = 0.0342\n",
      "epoch 77 / 100, step 63/80, loss = 0.0337\n",
      "epoch 77 / 100, step 64/80, loss = 0.0360\n",
      "epoch 77 / 100, step 65/80, loss = 0.0407\n",
      "epoch 77 / 100, step 66/80, loss = 0.0404\n",
      "epoch 77 / 100, step 67/80, loss = 0.0479\n",
      "epoch 77 / 100, step 68/80, loss = 0.0349\n",
      "epoch 77 / 100, step 69/80, loss = 0.0371\n",
      "epoch 77 / 100, step 70/80, loss = 0.0370\n",
      "epoch 77 / 100, step 71/80, loss = 0.0374\n",
      "epoch 77 / 100, step 72/80, loss = 0.0330\n",
      "epoch 77 / 100, step 73/80, loss = 0.0332\n",
      "epoch 77 / 100, step 74/80, loss = 0.0370\n",
      "epoch 77 / 100, step 75/80, loss = 0.0376\n",
      "epoch 77 / 100, step 76/80, loss = 0.0356\n",
      "epoch 77 / 100, step 77/80, loss = 0.0376\n",
      "epoch 77 / 100, step 78/80, loss = 0.0396\n",
      "epoch 77 / 100, step 79/80, loss = 0.0335\n",
      "epoch 77 / 100, step 80/80, loss = 0.0379\n",
      "epoch 78 / 100, step 1/80, loss = 0.0335\n",
      "epoch 78 / 100, step 2/80, loss = 0.0345\n",
      "epoch 78 / 100, step 3/80, loss = 0.0354\n",
      "epoch 78 / 100, step 4/80, loss = 0.0332\n",
      "epoch 78 / 100, step 5/80, loss = 0.0345\n",
      "epoch 78 / 100, step 6/80, loss = 0.0336\n",
      "epoch 78 / 100, step 7/80, loss = 0.0307\n",
      "epoch 78 / 100, step 8/80, loss = 0.0340\n",
      "epoch 78 / 100, step 9/80, loss = 0.0375\n",
      "epoch 78 / 100, step 10/80, loss = 0.0351\n",
      "epoch 78 / 100, step 11/80, loss = 0.0319\n",
      "epoch 78 / 100, step 12/80, loss = 0.0316\n",
      "epoch 78 / 100, step 13/80, loss = 0.0389\n",
      "epoch 78 / 100, step 14/80, loss = 0.0344\n",
      "epoch 78 / 100, step 15/80, loss = 0.0344\n",
      "epoch 78 / 100, step 16/80, loss = 0.0369\n",
      "epoch 78 / 100, step 17/80, loss = 0.0405\n",
      "epoch 78 / 100, step 18/80, loss = 0.0357\n",
      "epoch 78 / 100, step 19/80, loss = 0.0352\n",
      "epoch 78 / 100, step 20/80, loss = 0.0364\n",
      "epoch 78 / 100, step 21/80, loss = 0.0375\n",
      "epoch 78 / 100, step 22/80, loss = 0.0340\n",
      "epoch 78 / 100, step 23/80, loss = 0.0340\n",
      "epoch 78 / 100, step 24/80, loss = 0.0371\n",
      "epoch 78 / 100, step 25/80, loss = 0.0358\n",
      "epoch 78 / 100, step 26/80, loss = 0.0331\n",
      "epoch 78 / 100, step 27/80, loss = 0.0334\n",
      "epoch 78 / 100, step 28/80, loss = 0.0326\n",
      "epoch 78 / 100, step 29/80, loss = 0.0381\n",
      "epoch 78 / 100, step 30/80, loss = 0.0324\n",
      "epoch 78 / 100, step 31/80, loss = 0.0302\n",
      "epoch 78 / 100, step 32/80, loss = 0.0335\n",
      "epoch 78 / 100, step 33/80, loss = 0.0316\n",
      "epoch 78 / 100, step 34/80, loss = 0.0363\n",
      "epoch 78 / 100, step 35/80, loss = 0.0381\n",
      "epoch 78 / 100, step 36/80, loss = 0.0330\n",
      "epoch 78 / 100, step 37/80, loss = 0.0359\n",
      "epoch 78 / 100, step 38/80, loss = 0.0404\n",
      "epoch 78 / 100, step 39/80, loss = 0.0338\n",
      "epoch 78 / 100, step 40/80, loss = 0.0361\n",
      "epoch 78 / 100, step 41/80, loss = 0.0376\n",
      "epoch 78 / 100, step 42/80, loss = 0.0384\n",
      "epoch 78 / 100, step 43/80, loss = 0.0344\n",
      "epoch 78 / 100, step 44/80, loss = 0.0368\n",
      "epoch 78 / 100, step 45/80, loss = 0.0358\n",
      "epoch 78 / 100, step 46/80, loss = 0.0331\n",
      "epoch 78 / 100, step 47/80, loss = 0.0320\n",
      "epoch 78 / 100, step 48/80, loss = 0.0362\n",
      "epoch 78 / 100, step 49/80, loss = 0.0317\n",
      "epoch 78 / 100, step 50/80, loss = 0.0329\n",
      "epoch 78 / 100, step 51/80, loss = 0.0359\n",
      "epoch 78 / 100, step 52/80, loss = 0.0374\n",
      "epoch 78 / 100, step 53/80, loss = 0.0352\n",
      "epoch 78 / 100, step 54/80, loss = 0.0367\n",
      "epoch 78 / 100, step 55/80, loss = 0.0358\n",
      "epoch 78 / 100, step 56/80, loss = 0.0321\n",
      "epoch 78 / 100, step 57/80, loss = 0.0317\n",
      "epoch 78 / 100, step 58/80, loss = 0.0331\n",
      "epoch 78 / 100, step 59/80, loss = 0.0336\n",
      "epoch 78 / 100, step 60/80, loss = 0.0342\n",
      "epoch 78 / 100, step 61/80, loss = 0.0348\n",
      "epoch 78 / 100, step 62/80, loss = 0.0349\n",
      "epoch 78 / 100, step 63/80, loss = 0.0340\n",
      "epoch 78 / 100, step 64/80, loss = 0.0328\n",
      "epoch 78 / 100, step 65/80, loss = 0.0328\n",
      "epoch 78 / 100, step 66/80, loss = 0.0334\n",
      "epoch 78 / 100, step 67/80, loss = 0.0382\n",
      "epoch 78 / 100, step 68/80, loss = 0.0334\n",
      "epoch 78 / 100, step 69/80, loss = 0.0366\n",
      "epoch 78 / 100, step 70/80, loss = 0.0396\n",
      "epoch 78 / 100, step 71/80, loss = 0.0366\n",
      "epoch 78 / 100, step 72/80, loss = 0.0331\n",
      "epoch 78 / 100, step 73/80, loss = 0.0355\n",
      "epoch 78 / 100, step 74/80, loss = 0.0396\n",
      "epoch 78 / 100, step 75/80, loss = 0.0382\n",
      "epoch 78 / 100, step 76/80, loss = 0.0360\n",
      "epoch 78 / 100, step 77/80, loss = 0.0358\n",
      "epoch 78 / 100, step 78/80, loss = 0.0338\n",
      "epoch 78 / 100, step 79/80, loss = 0.0364\n",
      "epoch 78 / 100, step 80/80, loss = 0.0319\n",
      "epoch 79 / 100, step 1/80, loss = 0.0399\n",
      "epoch 79 / 100, step 2/80, loss = 0.0330\n",
      "epoch 79 / 100, step 3/80, loss = 0.0354\n",
      "epoch 79 / 100, step 4/80, loss = 0.0357\n",
      "epoch 79 / 100, step 5/80, loss = 0.0342\n",
      "epoch 79 / 100, step 6/80, loss = 0.0328\n",
      "epoch 79 / 100, step 7/80, loss = 0.0371\n",
      "epoch 79 / 100, step 8/80, loss = 0.0389\n",
      "epoch 79 / 100, step 9/80, loss = 0.0341\n",
      "epoch 79 / 100, step 10/80, loss = 0.0326\n",
      "epoch 79 / 100, step 11/80, loss = 0.0338\n",
      "epoch 79 / 100, step 12/80, loss = 0.0326\n",
      "epoch 79 / 100, step 13/80, loss = 0.0351\n",
      "epoch 79 / 100, step 14/80, loss = 0.0330\n",
      "epoch 79 / 100, step 15/80, loss = 0.0322\n",
      "epoch 79 / 100, step 16/80, loss = 0.0342\n",
      "epoch 79 / 100, step 17/80, loss = 0.0349\n",
      "epoch 79 / 100, step 18/80, loss = 0.0322\n",
      "epoch 79 / 100, step 19/80, loss = 0.0311\n",
      "epoch 79 / 100, step 20/80, loss = 0.0335\n",
      "epoch 79 / 100, step 21/80, loss = 0.0340\n",
      "epoch 79 / 100, step 22/80, loss = 0.0339\n",
      "epoch 79 / 100, step 23/80, loss = 0.0319\n",
      "epoch 79 / 100, step 24/80, loss = 0.0345\n",
      "epoch 79 / 100, step 25/80, loss = 0.0335\n",
      "epoch 79 / 100, step 26/80, loss = 0.0301\n",
      "epoch 79 / 100, step 27/80, loss = 0.0352\n",
      "epoch 79 / 100, step 28/80, loss = 0.0331\n",
      "epoch 79 / 100, step 29/80, loss = 0.0339\n",
      "epoch 79 / 100, step 30/80, loss = 0.0347\n",
      "epoch 79 / 100, step 31/80, loss = 0.0341\n",
      "epoch 79 / 100, step 32/80, loss = 0.0308\n",
      "epoch 79 / 100, step 33/80, loss = 0.0348\n",
      "epoch 79 / 100, step 34/80, loss = 0.0325\n",
      "epoch 79 / 100, step 35/80, loss = 0.0343\n",
      "epoch 79 / 100, step 36/80, loss = 0.0332\n",
      "epoch 79 / 100, step 37/80, loss = 0.0324\n",
      "epoch 79 / 100, step 38/80, loss = 0.0338\n",
      "epoch 79 / 100, step 39/80, loss = 0.0340\n",
      "epoch 79 / 100, step 40/80, loss = 0.0365\n",
      "epoch 79 / 100, step 41/80, loss = 0.0336\n",
      "epoch 79 / 100, step 42/80, loss = 0.0373\n",
      "epoch 79 / 100, step 43/80, loss = 0.0339\n",
      "epoch 79 / 100, step 44/80, loss = 0.0355\n",
      "epoch 79 / 100, step 45/80, loss = 0.0349\n",
      "epoch 79 / 100, step 46/80, loss = 0.0296\n",
      "epoch 79 / 100, step 47/80, loss = 0.0450\n",
      "epoch 79 / 100, step 48/80, loss = 0.0404\n",
      "epoch 79 / 100, step 49/80, loss = 0.0389\n",
      "epoch 79 / 100, step 50/80, loss = 0.0414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79 / 100, step 51/80, loss = 0.0354\n",
      "epoch 79 / 100, step 52/80, loss = 0.0327\n",
      "epoch 79 / 100, step 53/80, loss = 0.0384\n",
      "epoch 79 / 100, step 54/80, loss = 0.0351\n",
      "epoch 79 / 100, step 55/80, loss = 0.0364\n",
      "epoch 79 / 100, step 56/80, loss = 0.0344\n",
      "epoch 79 / 100, step 57/80, loss = 0.0340\n",
      "epoch 79 / 100, step 58/80, loss = 0.0342\n",
      "epoch 79 / 100, step 59/80, loss = 0.0333\n",
      "epoch 79 / 100, step 60/80, loss = 0.0350\n",
      "epoch 79 / 100, step 61/80, loss = 0.0347\n",
      "epoch 79 / 100, step 62/80, loss = 0.0317\n",
      "epoch 79 / 100, step 63/80, loss = 0.0307\n",
      "epoch 79 / 100, step 64/80, loss = 0.0314\n",
      "epoch 79 / 100, step 65/80, loss = 0.0318\n",
      "epoch 79 / 100, step 66/80, loss = 0.0363\n",
      "epoch 79 / 100, step 67/80, loss = 0.0374\n",
      "epoch 79 / 100, step 68/80, loss = 0.0323\n",
      "epoch 79 / 100, step 69/80, loss = 0.0366\n",
      "epoch 79 / 100, step 70/80, loss = 0.0345\n",
      "epoch 79 / 100, step 71/80, loss = 0.0337\n",
      "epoch 79 / 100, step 72/80, loss = 0.0310\n",
      "epoch 79 / 100, step 73/80, loss = 0.0392\n",
      "epoch 79 / 100, step 74/80, loss = 0.0343\n",
      "epoch 79 / 100, step 75/80, loss = 0.0294\n",
      "epoch 79 / 100, step 76/80, loss = 0.0332\n",
      "epoch 79 / 100, step 77/80, loss = 0.0360\n",
      "epoch 79 / 100, step 78/80, loss = 0.0340\n",
      "epoch 79 / 100, step 79/80, loss = 0.0379\n",
      "epoch 79 / 100, step 80/80, loss = 0.0320\n",
      "epoch 80 / 100, step 1/80, loss = 0.0318\n",
      "epoch 80 / 100, step 2/80, loss = 0.0287\n",
      "epoch 80 / 100, step 3/80, loss = 0.0353\n",
      "epoch 80 / 100, step 4/80, loss = 0.0335\n",
      "epoch 80 / 100, step 5/80, loss = 0.0340\n",
      "epoch 80 / 100, step 6/80, loss = 0.0346\n",
      "epoch 80 / 100, step 7/80, loss = 0.0322\n",
      "epoch 80 / 100, step 8/80, loss = 0.0336\n",
      "epoch 80 / 100, step 9/80, loss = 0.0321\n",
      "epoch 80 / 100, step 10/80, loss = 0.0335\n",
      "epoch 80 / 100, step 11/80, loss = 0.0312\n",
      "epoch 80 / 100, step 12/80, loss = 0.0340\n",
      "epoch 80 / 100, step 13/80, loss = 0.0335\n",
      "epoch 80 / 100, step 14/80, loss = 0.0371\n",
      "epoch 80 / 100, step 15/80, loss = 0.0337\n",
      "epoch 80 / 100, step 16/80, loss = 0.0328\n",
      "epoch 80 / 100, step 17/80, loss = 0.0334\n",
      "epoch 80 / 100, step 18/80, loss = 0.0373\n",
      "epoch 80 / 100, step 19/80, loss = 0.0326\n",
      "epoch 80 / 100, step 20/80, loss = 0.0378\n",
      "epoch 80 / 100, step 21/80, loss = 0.0399\n",
      "epoch 80 / 100, step 22/80, loss = 0.0325\n",
      "epoch 80 / 100, step 23/80, loss = 0.0323\n",
      "epoch 80 / 100, step 24/80, loss = 0.0377\n",
      "epoch 80 / 100, step 25/80, loss = 0.0339\n",
      "epoch 80 / 100, step 26/80, loss = 0.0355\n",
      "epoch 80 / 100, step 27/80, loss = 0.0319\n",
      "epoch 80 / 100, step 28/80, loss = 0.0352\n",
      "epoch 80 / 100, step 29/80, loss = 0.0358\n",
      "epoch 80 / 100, step 30/80, loss = 0.0337\n",
      "epoch 80 / 100, step 31/80, loss = 0.0341\n",
      "epoch 80 / 100, step 32/80, loss = 0.0331\n",
      "epoch 80 / 100, step 33/80, loss = 0.0295\n",
      "epoch 80 / 100, step 34/80, loss = 0.0313\n",
      "epoch 80 / 100, step 35/80, loss = 0.0316\n",
      "epoch 80 / 100, step 36/80, loss = 0.0323\n",
      "epoch 80 / 100, step 37/80, loss = 0.0356\n",
      "epoch 80 / 100, step 38/80, loss = 0.0359\n",
      "epoch 80 / 100, step 39/80, loss = 0.0324\n",
      "epoch 80 / 100, step 40/80, loss = 0.0356\n",
      "epoch 80 / 100, step 41/80, loss = 0.0405\n",
      "epoch 80 / 100, step 42/80, loss = 0.0329\n",
      "epoch 80 / 100, step 43/80, loss = 0.0382\n",
      "epoch 80 / 100, step 44/80, loss = 0.0336\n",
      "epoch 80 / 100, step 45/80, loss = 0.0304\n",
      "epoch 80 / 100, step 46/80, loss = 0.0309\n",
      "epoch 80 / 100, step 47/80, loss = 0.0348\n",
      "epoch 80 / 100, step 48/80, loss = 0.0342\n",
      "epoch 80 / 100, step 49/80, loss = 0.0337\n",
      "epoch 80 / 100, step 50/80, loss = 0.0355\n",
      "epoch 80 / 100, step 51/80, loss = 0.0323\n",
      "epoch 80 / 100, step 52/80, loss = 0.0363\n",
      "epoch 80 / 100, step 53/80, loss = 0.0347\n",
      "epoch 80 / 100, step 54/80, loss = 0.0348\n",
      "epoch 80 / 100, step 55/80, loss = 0.0340\n",
      "epoch 80 / 100, step 56/80, loss = 0.0411\n",
      "epoch 80 / 100, step 57/80, loss = 0.0353\n",
      "epoch 80 / 100, step 58/80, loss = 0.0328\n",
      "epoch 80 / 100, step 59/80, loss = 0.0326\n",
      "epoch 80 / 100, step 60/80, loss = 0.0338\n",
      "epoch 80 / 100, step 61/80, loss = 0.0349\n",
      "epoch 80 / 100, step 62/80, loss = 0.0376\n",
      "epoch 80 / 100, step 63/80, loss = 0.0354\n",
      "epoch 80 / 100, step 64/80, loss = 0.0334\n",
      "epoch 80 / 100, step 65/80, loss = 0.0333\n",
      "epoch 80 / 100, step 66/80, loss = 0.0337\n",
      "epoch 80 / 100, step 67/80, loss = 0.0347\n",
      "epoch 80 / 100, step 68/80, loss = 0.0360\n",
      "epoch 80 / 100, step 69/80, loss = 0.0319\n",
      "epoch 80 / 100, step 70/80, loss = 0.0336\n",
      "epoch 80 / 100, step 71/80, loss = 0.0324\n",
      "epoch 80 / 100, step 72/80, loss = 0.0355\n",
      "epoch 80 / 100, step 73/80, loss = 0.0325\n",
      "epoch 80 / 100, step 74/80, loss = 0.0416\n",
      "epoch 80 / 100, step 75/80, loss = 0.0346\n",
      "epoch 80 / 100, step 76/80, loss = 0.0341\n",
      "epoch 80 / 100, step 77/80, loss = 0.0357\n",
      "epoch 80 / 100, step 78/80, loss = 0.0405\n",
      "epoch 80 / 100, step 79/80, loss = 0.0331\n",
      "epoch 80 / 100, step 80/80, loss = 0.0352\n",
      "epoch 81 / 100, step 1/80, loss = 0.0323\n",
      "epoch 81 / 100, step 2/80, loss = 0.0346\n",
      "epoch 81 / 100, step 3/80, loss = 0.0332\n",
      "epoch 81 / 100, step 4/80, loss = 0.0320\n",
      "epoch 81 / 100, step 5/80, loss = 0.0340\n",
      "epoch 81 / 100, step 6/80, loss = 0.0306\n",
      "epoch 81 / 100, step 7/80, loss = 0.0308\n",
      "epoch 81 / 100, step 8/80, loss = 0.0345\n",
      "epoch 81 / 100, step 9/80, loss = 0.0365\n",
      "epoch 81 / 100, step 10/80, loss = 0.0322\n",
      "epoch 81 / 100, step 11/80, loss = 0.0293\n",
      "epoch 81 / 100, step 12/80, loss = 0.0336\n",
      "epoch 81 / 100, step 13/80, loss = 0.0320\n",
      "epoch 81 / 100, step 14/80, loss = 0.0321\n",
      "epoch 81 / 100, step 15/80, loss = 0.0286\n",
      "epoch 81 / 100, step 16/80, loss = 0.0333\n",
      "epoch 81 / 100, step 17/80, loss = 0.0318\n",
      "epoch 81 / 100, step 18/80, loss = 0.0298\n",
      "epoch 81 / 100, step 19/80, loss = 0.0311\n",
      "epoch 81 / 100, step 20/80, loss = 0.0297\n",
      "epoch 81 / 100, step 21/80, loss = 0.0301\n",
      "epoch 81 / 100, step 22/80, loss = 0.0290\n",
      "epoch 81 / 100, step 23/80, loss = 0.0316\n",
      "epoch 81 / 100, step 24/80, loss = 0.0350\n",
      "epoch 81 / 100, step 25/80, loss = 0.0359\n",
      "epoch 81 / 100, step 26/80, loss = 0.0398\n",
      "epoch 81 / 100, step 27/80, loss = 0.0338\n",
      "epoch 81 / 100, step 28/80, loss = 0.0339\n",
      "epoch 81 / 100, step 29/80, loss = 0.0357\n",
      "epoch 81 / 100, step 30/80, loss = 0.0328\n",
      "epoch 81 / 100, step 31/80, loss = 0.0334\n",
      "epoch 81 / 100, step 32/80, loss = 0.0325\n",
      "epoch 81 / 100, step 33/80, loss = 0.0350\n",
      "epoch 81 / 100, step 34/80, loss = 0.0304\n",
      "epoch 81 / 100, step 35/80, loss = 0.0340\n",
      "epoch 81 / 100, step 36/80, loss = 0.0332\n",
      "epoch 81 / 100, step 37/80, loss = 0.0339\n",
      "epoch 81 / 100, step 38/80, loss = 0.0304\n",
      "epoch 81 / 100, step 39/80, loss = 0.0320\n",
      "epoch 81 / 100, step 40/80, loss = 0.0379\n",
      "epoch 81 / 100, step 41/80, loss = 0.0306\n",
      "epoch 81 / 100, step 42/80, loss = 0.0303\n",
      "epoch 81 / 100, step 43/80, loss = 0.0349\n",
      "epoch 81 / 100, step 44/80, loss = 0.0388\n",
      "epoch 81 / 100, step 45/80, loss = 0.0384\n",
      "epoch 81 / 100, step 46/80, loss = 0.0348\n",
      "epoch 81 / 100, step 47/80, loss = 0.0324\n",
      "epoch 81 / 100, step 48/80, loss = 0.0352\n",
      "epoch 81 / 100, step 49/80, loss = 0.0361\n",
      "epoch 81 / 100, step 50/80, loss = 0.0321\n",
      "epoch 81 / 100, step 51/80, loss = 0.0313\n",
      "epoch 81 / 100, step 52/80, loss = 0.0339\n",
      "epoch 81 / 100, step 53/80, loss = 0.0333\n",
      "epoch 81 / 100, step 54/80, loss = 0.0325\n",
      "epoch 81 / 100, step 55/80, loss = 0.0332\n",
      "epoch 81 / 100, step 56/80, loss = 0.0306\n",
      "epoch 81 / 100, step 57/80, loss = 0.0315\n",
      "epoch 81 / 100, step 58/80, loss = 0.0341\n",
      "epoch 81 / 100, step 59/80, loss = 0.0370\n",
      "epoch 81 / 100, step 60/80, loss = 0.0350\n",
      "epoch 81 / 100, step 61/80, loss = 0.0317\n",
      "epoch 81 / 100, step 62/80, loss = 0.0372\n",
      "epoch 81 / 100, step 63/80, loss = 0.0326\n",
      "epoch 81 / 100, step 64/80, loss = 0.0339\n",
      "epoch 81 / 100, step 65/80, loss = 0.0325\n",
      "epoch 81 / 100, step 66/80, loss = 0.0326\n",
      "epoch 81 / 100, step 67/80, loss = 0.0390\n",
      "epoch 81 / 100, step 68/80, loss = 0.0359\n",
      "epoch 81 / 100, step 69/80, loss = 0.0302\n",
      "epoch 81 / 100, step 70/80, loss = 0.0337\n",
      "epoch 81 / 100, step 71/80, loss = 0.0346\n",
      "epoch 81 / 100, step 72/80, loss = 0.0349\n",
      "epoch 81 / 100, step 73/80, loss = 0.0408\n",
      "epoch 81 / 100, step 74/80, loss = 0.0370\n",
      "epoch 81 / 100, step 75/80, loss = 0.0357\n",
      "epoch 81 / 100, step 76/80, loss = 0.0430\n",
      "epoch 81 / 100, step 77/80, loss = 0.0305\n",
      "epoch 81 / 100, step 78/80, loss = 0.0338\n",
      "epoch 81 / 100, step 79/80, loss = 0.0326\n",
      "epoch 81 / 100, step 80/80, loss = 0.0347\n",
      "epoch 82 / 100, step 1/80, loss = 0.0288\n",
      "epoch 82 / 100, step 2/80, loss = 0.0339\n",
      "epoch 82 / 100, step 3/80, loss = 0.0348\n",
      "epoch 82 / 100, step 4/80, loss = 0.0312\n",
      "epoch 82 / 100, step 5/80, loss = 0.0308\n",
      "epoch 82 / 100, step 6/80, loss = 0.0335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82 / 100, step 7/80, loss = 0.0304\n",
      "epoch 82 / 100, step 8/80, loss = 0.0337\n",
      "epoch 82 / 100, step 9/80, loss = 0.0342\n",
      "epoch 82 / 100, step 10/80, loss = 0.0294\n",
      "epoch 82 / 100, step 11/80, loss = 0.0326\n",
      "epoch 82 / 100, step 12/80, loss = 0.0319\n",
      "epoch 82 / 100, step 13/80, loss = 0.0293\n",
      "epoch 82 / 100, step 14/80, loss = 0.0331\n",
      "epoch 82 / 100, step 15/80, loss = 0.0317\n",
      "epoch 82 / 100, step 16/80, loss = 0.0303\n",
      "epoch 82 / 100, step 17/80, loss = 0.0316\n",
      "epoch 82 / 100, step 18/80, loss = 0.0294\n",
      "epoch 82 / 100, step 19/80, loss = 0.0344\n",
      "epoch 82 / 100, step 20/80, loss = 0.0356\n",
      "epoch 82 / 100, step 21/80, loss = 0.0302\n",
      "epoch 82 / 100, step 22/80, loss = 0.0335\n",
      "epoch 82 / 100, step 23/80, loss = 0.0300\n",
      "epoch 82 / 100, step 24/80, loss = 0.0323\n",
      "epoch 82 / 100, step 25/80, loss = 0.0308\n",
      "epoch 82 / 100, step 26/80, loss = 0.0314\n",
      "epoch 82 / 100, step 27/80, loss = 0.0351\n",
      "epoch 82 / 100, step 28/80, loss = 0.0301\n",
      "epoch 82 / 100, step 29/80, loss = 0.0350\n",
      "epoch 82 / 100, step 30/80, loss = 0.0359\n",
      "epoch 82 / 100, step 31/80, loss = 0.0381\n",
      "epoch 82 / 100, step 32/80, loss = 0.0341\n",
      "epoch 82 / 100, step 33/80, loss = 0.0341\n",
      "epoch 82 / 100, step 34/80, loss = 0.0351\n",
      "epoch 82 / 100, step 35/80, loss = 0.0298\n",
      "epoch 82 / 100, step 36/80, loss = 0.0363\n",
      "epoch 82 / 100, step 37/80, loss = 0.0312\n",
      "epoch 82 / 100, step 38/80, loss = 0.0372\n",
      "epoch 82 / 100, step 39/80, loss = 0.0321\n",
      "epoch 82 / 100, step 40/80, loss = 0.0373\n",
      "epoch 82 / 100, step 41/80, loss = 0.0305\n",
      "epoch 82 / 100, step 42/80, loss = 0.0317\n",
      "epoch 82 / 100, step 43/80, loss = 0.0341\n",
      "epoch 82 / 100, step 44/80, loss = 0.0330\n",
      "epoch 82 / 100, step 45/80, loss = 0.0332\n",
      "epoch 82 / 100, step 46/80, loss = 0.0321\n",
      "epoch 82 / 100, step 47/80, loss = 0.0346\n",
      "epoch 82 / 100, step 48/80, loss = 0.0335\n",
      "epoch 82 / 100, step 49/80, loss = 0.0360\n",
      "epoch 82 / 100, step 50/80, loss = 0.0318\n",
      "epoch 82 / 100, step 51/80, loss = 0.0381\n",
      "epoch 82 / 100, step 52/80, loss = 0.0345\n",
      "epoch 82 / 100, step 53/80, loss = 0.0314\n",
      "epoch 82 / 100, step 54/80, loss = 0.0336\n",
      "epoch 82 / 100, step 55/80, loss = 0.0315\n",
      "epoch 82 / 100, step 56/80, loss = 0.0366\n",
      "epoch 82 / 100, step 57/80, loss = 0.0404\n",
      "epoch 82 / 100, step 58/80, loss = 0.0321\n",
      "epoch 82 / 100, step 59/80, loss = 0.0310\n",
      "epoch 82 / 100, step 60/80, loss = 0.0322\n",
      "epoch 82 / 100, step 61/80, loss = 0.0345\n",
      "epoch 82 / 100, step 62/80, loss = 0.0376\n",
      "epoch 82 / 100, step 63/80, loss = 0.0319\n",
      "epoch 82 / 100, step 64/80, loss = 0.0301\n",
      "epoch 82 / 100, step 65/80, loss = 0.0324\n",
      "epoch 82 / 100, step 66/80, loss = 0.0348\n",
      "epoch 82 / 100, step 67/80, loss = 0.0384\n",
      "epoch 82 / 100, step 68/80, loss = 0.0315\n",
      "epoch 82 / 100, step 69/80, loss = 0.0336\n",
      "epoch 82 / 100, step 70/80, loss = 0.0389\n",
      "epoch 82 / 100, step 71/80, loss = 0.0321\n",
      "epoch 82 / 100, step 72/80, loss = 0.0320\n",
      "epoch 82 / 100, step 73/80, loss = 0.0331\n",
      "epoch 82 / 100, step 74/80, loss = 0.0352\n",
      "epoch 82 / 100, step 75/80, loss = 0.0363\n",
      "epoch 82 / 100, step 76/80, loss = 0.0317\n",
      "epoch 82 / 100, step 77/80, loss = 0.0352\n",
      "epoch 82 / 100, step 78/80, loss = 0.0330\n",
      "epoch 82 / 100, step 79/80, loss = 0.0308\n",
      "epoch 82 / 100, step 80/80, loss = 0.0311\n",
      "epoch 83 / 100, step 1/80, loss = 0.0324\n",
      "epoch 83 / 100, step 2/80, loss = 0.0318\n",
      "epoch 83 / 100, step 3/80, loss = 0.0349\n",
      "epoch 83 / 100, step 4/80, loss = 0.0311\n",
      "epoch 83 / 100, step 5/80, loss = 0.0320\n",
      "epoch 83 / 100, step 6/80, loss = 0.0312\n",
      "epoch 83 / 100, step 7/80, loss = 0.0312\n",
      "epoch 83 / 100, step 8/80, loss = 0.0312\n",
      "epoch 83 / 100, step 9/80, loss = 0.0337\n",
      "epoch 83 / 100, step 10/80, loss = 0.0325\n",
      "epoch 83 / 100, step 11/80, loss = 0.0307\n",
      "epoch 83 / 100, step 12/80, loss = 0.0325\n",
      "epoch 83 / 100, step 13/80, loss = 0.0330\n",
      "epoch 83 / 100, step 14/80, loss = 0.0319\n",
      "epoch 83 / 100, step 15/80, loss = 0.0331\n",
      "epoch 83 / 100, step 16/80, loss = 0.0353\n",
      "epoch 83 / 100, step 17/80, loss = 0.0297\n",
      "epoch 83 / 100, step 18/80, loss = 0.0330\n",
      "epoch 83 / 100, step 19/80, loss = 0.0358\n",
      "epoch 83 / 100, step 20/80, loss = 0.0326\n",
      "epoch 83 / 100, step 21/80, loss = 0.0379\n",
      "epoch 83 / 100, step 22/80, loss = 0.0289\n",
      "epoch 83 / 100, step 23/80, loss = 0.0324\n",
      "epoch 83 / 100, step 24/80, loss = 0.0325\n",
      "epoch 83 / 100, step 25/80, loss = 0.0337\n",
      "epoch 83 / 100, step 26/80, loss = 0.0479\n",
      "epoch 83 / 100, step 27/80, loss = 0.0329\n",
      "epoch 83 / 100, step 28/80, loss = 0.0329\n",
      "epoch 83 / 100, step 29/80, loss = 0.0336\n",
      "epoch 83 / 100, step 30/80, loss = 0.0305\n",
      "epoch 83 / 100, step 31/80, loss = 0.0295\n",
      "epoch 83 / 100, step 32/80, loss = 0.0349\n",
      "epoch 83 / 100, step 33/80, loss = 0.0324\n",
      "epoch 83 / 100, step 34/80, loss = 0.0307\n",
      "epoch 83 / 100, step 35/80, loss = 0.0336\n",
      "epoch 83 / 100, step 36/80, loss = 0.0328\n",
      "epoch 83 / 100, step 37/80, loss = 0.0315\n",
      "epoch 83 / 100, step 38/80, loss = 0.0342\n",
      "epoch 83 / 100, step 39/80, loss = 0.0326\n",
      "epoch 83 / 100, step 40/80, loss = 0.0344\n",
      "epoch 83 / 100, step 41/80, loss = 0.0305\n",
      "epoch 83 / 100, step 42/80, loss = 0.0324\n",
      "epoch 83 / 100, step 43/80, loss = 0.0326\n",
      "epoch 83 / 100, step 44/80, loss = 0.0305\n",
      "epoch 83 / 100, step 45/80, loss = 0.0297\n",
      "epoch 83 / 100, step 46/80, loss = 0.0351\n",
      "epoch 83 / 100, step 47/80, loss = 0.0326\n",
      "epoch 83 / 100, step 48/80, loss = 0.0308\n",
      "epoch 83 / 100, step 49/80, loss = 0.0315\n",
      "epoch 83 / 100, step 50/80, loss = 0.0300\n",
      "epoch 83 / 100, step 51/80, loss = 0.0375\n",
      "epoch 83 / 100, step 52/80, loss = 0.0326\n",
      "epoch 83 / 100, step 53/80, loss = 0.0372\n",
      "epoch 83 / 100, step 54/80, loss = 0.0280\n",
      "epoch 83 / 100, step 55/80, loss = 0.0336\n",
      "epoch 83 / 100, step 56/80, loss = 0.0334\n",
      "epoch 83 / 100, step 57/80, loss = 0.0350\n",
      "epoch 83 / 100, step 58/80, loss = 0.0354\n",
      "epoch 83 / 100, step 59/80, loss = 0.0325\n",
      "epoch 83 / 100, step 60/80, loss = 0.0347\n",
      "epoch 83 / 100, step 61/80, loss = 0.0327\n",
      "epoch 83 / 100, step 62/80, loss = 0.0311\n",
      "epoch 83 / 100, step 63/80, loss = 0.0317\n",
      "epoch 83 / 100, step 64/80, loss = 0.0325\n",
      "epoch 83 / 100, step 65/80, loss = 0.0318\n",
      "epoch 83 / 100, step 66/80, loss = 0.0340\n",
      "epoch 83 / 100, step 67/80, loss = 0.0324\n",
      "epoch 83 / 100, step 68/80, loss = 0.0329\n",
      "epoch 83 / 100, step 69/80, loss = 0.0315\n",
      "epoch 83 / 100, step 70/80, loss = 0.0311\n",
      "epoch 83 / 100, step 71/80, loss = 0.0314\n",
      "epoch 83 / 100, step 72/80, loss = 0.0351\n",
      "epoch 83 / 100, step 73/80, loss = 0.0297\n",
      "epoch 83 / 100, step 74/80, loss = 0.0300\n",
      "epoch 83 / 100, step 75/80, loss = 0.0292\n",
      "epoch 83 / 100, step 76/80, loss = 0.0293\n",
      "epoch 83 / 100, step 77/80, loss = 0.0283\n",
      "epoch 83 / 100, step 78/80, loss = 0.0344\n",
      "epoch 83 / 100, step 79/80, loss = 0.0349\n",
      "epoch 83 / 100, step 80/80, loss = 0.0312\n",
      "epoch 84 / 100, step 1/80, loss = 0.0302\n",
      "epoch 84 / 100, step 2/80, loss = 0.0338\n",
      "epoch 84 / 100, step 3/80, loss = 0.0303\n",
      "epoch 84 / 100, step 4/80, loss = 0.0320\n",
      "epoch 84 / 100, step 5/80, loss = 0.0299\n",
      "epoch 84 / 100, step 6/80, loss = 0.0341\n",
      "epoch 84 / 100, step 7/80, loss = 0.0317\n",
      "epoch 84 / 100, step 8/80, loss = 0.0300\n",
      "epoch 84 / 100, step 9/80, loss = 0.0295\n",
      "epoch 84 / 100, step 10/80, loss = 0.0391\n",
      "epoch 84 / 100, step 11/80, loss = 0.0318\n",
      "epoch 84 / 100, step 12/80, loss = 0.0305\n",
      "epoch 84 / 100, step 13/80, loss = 0.0335\n",
      "epoch 84 / 100, step 14/80, loss = 0.0356\n",
      "epoch 84 / 100, step 15/80, loss = 0.0316\n",
      "epoch 84 / 100, step 16/80, loss = 0.0326\n",
      "epoch 84 / 100, step 17/80, loss = 0.0331\n",
      "epoch 84 / 100, step 18/80, loss = 0.0355\n",
      "epoch 84 / 100, step 19/80, loss = 0.0283\n",
      "epoch 84 / 100, step 20/80, loss = 0.0339\n",
      "epoch 84 / 100, step 21/80, loss = 0.0286\n",
      "epoch 84 / 100, step 22/80, loss = 0.0327\n",
      "epoch 84 / 100, step 23/80, loss = 0.0332\n",
      "epoch 84 / 100, step 24/80, loss = 0.0289\n",
      "epoch 84 / 100, step 25/80, loss = 0.0309\n",
      "epoch 84 / 100, step 26/80, loss = 0.0349\n",
      "epoch 84 / 100, step 27/80, loss = 0.0318\n",
      "epoch 84 / 100, step 28/80, loss = 0.0297\n",
      "epoch 84 / 100, step 29/80, loss = 0.0339\n",
      "epoch 84 / 100, step 30/80, loss = 0.0300\n",
      "epoch 84 / 100, step 31/80, loss = 0.0298\n",
      "epoch 84 / 100, step 32/80, loss = 0.0339\n",
      "epoch 84 / 100, step 33/80, loss = 0.0316\n",
      "epoch 84 / 100, step 34/80, loss = 0.0316\n",
      "epoch 84 / 100, step 35/80, loss = 0.0317\n",
      "epoch 84 / 100, step 36/80, loss = 0.0386\n",
      "epoch 84 / 100, step 37/80, loss = 0.0298\n",
      "epoch 84 / 100, step 38/80, loss = 0.0311\n",
      "epoch 84 / 100, step 39/80, loss = 0.0310\n",
      "epoch 84 / 100, step 40/80, loss = 0.0342\n",
      "epoch 84 / 100, step 41/80, loss = 0.0286\n",
      "epoch 84 / 100, step 42/80, loss = 0.0316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84 / 100, step 43/80, loss = 0.0348\n",
      "epoch 84 / 100, step 44/80, loss = 0.0312\n",
      "epoch 84 / 100, step 45/80, loss = 0.0308\n",
      "epoch 84 / 100, step 46/80, loss = 0.0352\n",
      "epoch 84 / 100, step 47/80, loss = 0.0314\n",
      "epoch 84 / 100, step 48/80, loss = 0.0334\n",
      "epoch 84 / 100, step 49/80, loss = 0.0314\n",
      "epoch 84 / 100, step 50/80, loss = 0.0367\n",
      "epoch 84 / 100, step 51/80, loss = 0.0333\n",
      "epoch 84 / 100, step 52/80, loss = 0.0339\n",
      "epoch 84 / 100, step 53/80, loss = 0.0345\n",
      "epoch 84 / 100, step 54/80, loss = 0.0334\n",
      "epoch 84 / 100, step 55/80, loss = 0.0317\n",
      "epoch 84 / 100, step 56/80, loss = 0.0331\n",
      "epoch 84 / 100, step 57/80, loss = 0.0323\n",
      "epoch 84 / 100, step 58/80, loss = 0.0299\n",
      "epoch 84 / 100, step 59/80, loss = 0.0339\n",
      "epoch 84 / 100, step 60/80, loss = 0.0330\n",
      "epoch 84 / 100, step 61/80, loss = 0.0325\n",
      "epoch 84 / 100, step 62/80, loss = 0.0314\n",
      "epoch 84 / 100, step 63/80, loss = 0.0378\n",
      "epoch 84 / 100, step 64/80, loss = 0.0357\n",
      "epoch 84 / 100, step 65/80, loss = 0.0315\n",
      "epoch 84 / 100, step 66/80, loss = 0.0316\n",
      "epoch 84 / 100, step 67/80, loss = 0.0334\n",
      "epoch 84 / 100, step 68/80, loss = 0.0311\n",
      "epoch 84 / 100, step 69/80, loss = 0.0307\n",
      "epoch 84 / 100, step 70/80, loss = 0.0372\n",
      "epoch 84 / 100, step 71/80, loss = 0.0323\n",
      "epoch 84 / 100, step 72/80, loss = 0.0335\n",
      "epoch 84 / 100, step 73/80, loss = 0.0322\n",
      "epoch 84 / 100, step 74/80, loss = 0.0340\n",
      "epoch 84 / 100, step 75/80, loss = 0.0317\n",
      "epoch 84 / 100, step 76/80, loss = 0.0343\n",
      "epoch 84 / 100, step 77/80, loss = 0.0316\n",
      "epoch 84 / 100, step 78/80, loss = 0.0295\n",
      "epoch 84 / 100, step 79/80, loss = 0.0323\n",
      "epoch 84 / 100, step 80/80, loss = 0.0364\n",
      "epoch 85 / 100, step 1/80, loss = 0.0295\n",
      "epoch 85 / 100, step 2/80, loss = 0.0304\n",
      "epoch 85 / 100, step 3/80, loss = 0.0321\n",
      "epoch 85 / 100, step 4/80, loss = 0.0283\n",
      "epoch 85 / 100, step 5/80, loss = 0.0326\n",
      "epoch 85 / 100, step 6/80, loss = 0.0309\n",
      "epoch 85 / 100, step 7/80, loss = 0.0305\n",
      "epoch 85 / 100, step 8/80, loss = 0.0301\n",
      "epoch 85 / 100, step 9/80, loss = 0.0407\n",
      "epoch 85 / 100, step 10/80, loss = 0.0369\n",
      "epoch 85 / 100, step 11/80, loss = 0.0390\n",
      "epoch 85 / 100, step 12/80, loss = 0.0324\n",
      "epoch 85 / 100, step 13/80, loss = 0.0304\n",
      "epoch 85 / 100, step 14/80, loss = 0.0297\n",
      "epoch 85 / 100, step 15/80, loss = 0.0305\n",
      "epoch 85 / 100, step 16/80, loss = 0.0362\n",
      "epoch 85 / 100, step 17/80, loss = 0.0279\n",
      "epoch 85 / 100, step 18/80, loss = 0.0354\n",
      "epoch 85 / 100, step 19/80, loss = 0.0319\n",
      "epoch 85 / 100, step 20/80, loss = 0.0325\n",
      "epoch 85 / 100, step 21/80, loss = 0.0294\n",
      "epoch 85 / 100, step 22/80, loss = 0.0352\n",
      "epoch 85 / 100, step 23/80, loss = 0.0294\n",
      "epoch 85 / 100, step 24/80, loss = 0.0350\n",
      "epoch 85 / 100, step 25/80, loss = 0.0349\n",
      "epoch 85 / 100, step 26/80, loss = 0.0338\n",
      "epoch 85 / 100, step 27/80, loss = 0.0280\n",
      "epoch 85 / 100, step 28/80, loss = 0.0294\n",
      "epoch 85 / 100, step 29/80, loss = 0.0303\n",
      "epoch 85 / 100, step 30/80, loss = 0.0326\n",
      "epoch 85 / 100, step 31/80, loss = 0.0300\n",
      "epoch 85 / 100, step 32/80, loss = 0.0331\n",
      "epoch 85 / 100, step 33/80, loss = 0.0322\n",
      "epoch 85 / 100, step 34/80, loss = 0.0335\n",
      "epoch 85 / 100, step 35/80, loss = 0.0346\n",
      "epoch 85 / 100, step 36/80, loss = 0.0316\n",
      "epoch 85 / 100, step 37/80, loss = 0.0338\n",
      "epoch 85 / 100, step 38/80, loss = 0.0343\n",
      "epoch 85 / 100, step 39/80, loss = 0.0296\n",
      "epoch 85 / 100, step 40/80, loss = 0.0319\n",
      "epoch 85 / 100, step 41/80, loss = 0.0373\n",
      "epoch 85 / 100, step 42/80, loss = 0.0317\n",
      "epoch 85 / 100, step 43/80, loss = 0.0332\n",
      "epoch 85 / 100, step 44/80, loss = 0.0310\n",
      "epoch 85 / 100, step 45/80, loss = 0.0389\n",
      "epoch 85 / 100, step 46/80, loss = 0.0329\n",
      "epoch 85 / 100, step 47/80, loss = 0.0326\n",
      "epoch 85 / 100, step 48/80, loss = 0.0296\n",
      "epoch 85 / 100, step 49/80, loss = 0.0327\n",
      "epoch 85 / 100, step 50/80, loss = 0.0352\n",
      "epoch 85 / 100, step 51/80, loss = 0.0338\n",
      "epoch 85 / 100, step 52/80, loss = 0.0349\n",
      "epoch 85 / 100, step 53/80, loss = 0.0284\n",
      "epoch 85 / 100, step 54/80, loss = 0.0306\n",
      "epoch 85 / 100, step 55/80, loss = 0.0329\n",
      "epoch 85 / 100, step 56/80, loss = 0.0374\n",
      "epoch 85 / 100, step 57/80, loss = 0.0293\n",
      "epoch 85 / 100, step 58/80, loss = 0.0318\n",
      "epoch 85 / 100, step 59/80, loss = 0.0316\n",
      "epoch 85 / 100, step 60/80, loss = 0.0354\n",
      "epoch 85 / 100, step 61/80, loss = 0.0293\n",
      "epoch 85 / 100, step 62/80, loss = 0.0314\n",
      "epoch 85 / 100, step 63/80, loss = 0.0311\n",
      "epoch 85 / 100, step 64/80, loss = 0.0341\n",
      "epoch 85 / 100, step 65/80, loss = 0.0324\n",
      "epoch 85 / 100, step 66/80, loss = 0.0326\n",
      "epoch 85 / 100, step 67/80, loss = 0.0320\n",
      "epoch 85 / 100, step 68/80, loss = 0.0300\n",
      "epoch 85 / 100, step 69/80, loss = 0.0281\n",
      "epoch 85 / 100, step 70/80, loss = 0.0303\n",
      "epoch 85 / 100, step 71/80, loss = 0.0301\n",
      "epoch 85 / 100, step 72/80, loss = 0.0349\n",
      "epoch 85 / 100, step 73/80, loss = 0.0414\n",
      "epoch 85 / 100, step 74/80, loss = 0.0312\n",
      "epoch 85 / 100, step 75/80, loss = 0.0345\n",
      "epoch 85 / 100, step 76/80, loss = 0.0306\n",
      "epoch 85 / 100, step 77/80, loss = 0.0298\n",
      "epoch 85 / 100, step 78/80, loss = 0.0327\n",
      "epoch 85 / 100, step 79/80, loss = 0.0327\n",
      "epoch 85 / 100, step 80/80, loss = 0.0330\n",
      "epoch 86 / 100, step 1/80, loss = 0.0287\n",
      "epoch 86 / 100, step 2/80, loss = 0.0284\n",
      "epoch 86 / 100, step 3/80, loss = 0.0283\n",
      "epoch 86 / 100, step 4/80, loss = 0.0298\n",
      "epoch 86 / 100, step 5/80, loss = 0.0353\n",
      "epoch 86 / 100, step 6/80, loss = 0.0296\n",
      "epoch 86 / 100, step 7/80, loss = 0.0352\n",
      "epoch 86 / 100, step 8/80, loss = 0.0304\n",
      "epoch 86 / 100, step 9/80, loss = 0.0369\n",
      "epoch 86 / 100, step 10/80, loss = 0.0348\n",
      "epoch 86 / 100, step 11/80, loss = 0.0313\n",
      "epoch 86 / 100, step 12/80, loss = 0.0327\n",
      "epoch 86 / 100, step 13/80, loss = 0.0331\n",
      "epoch 86 / 100, step 14/80, loss = 0.0368\n",
      "epoch 86 / 100, step 15/80, loss = 0.0301\n",
      "epoch 86 / 100, step 16/80, loss = 0.0331\n",
      "epoch 86 / 100, step 17/80, loss = 0.0320\n",
      "epoch 86 / 100, step 18/80, loss = 0.0308\n",
      "epoch 86 / 100, step 19/80, loss = 0.0311\n",
      "epoch 86 / 100, step 20/80, loss = 0.0324\n",
      "epoch 86 / 100, step 21/80, loss = 0.0356\n",
      "epoch 86 / 100, step 22/80, loss = 0.0335\n",
      "epoch 86 / 100, step 23/80, loss = 0.0333\n",
      "epoch 86 / 100, step 24/80, loss = 0.0326\n",
      "epoch 86 / 100, step 25/80, loss = 0.0297\n",
      "epoch 86 / 100, step 26/80, loss = 0.0327\n",
      "epoch 86 / 100, step 27/80, loss = 0.0307\n",
      "epoch 86 / 100, step 28/80, loss = 0.0281\n",
      "epoch 86 / 100, step 29/80, loss = 0.0299\n",
      "epoch 86 / 100, step 30/80, loss = 0.0294\n",
      "epoch 86 / 100, step 31/80, loss = 0.0329\n",
      "epoch 86 / 100, step 32/80, loss = 0.0335\n",
      "epoch 86 / 100, step 33/80, loss = 0.0362\n",
      "epoch 86 / 100, step 34/80, loss = 0.0309\n",
      "epoch 86 / 100, step 35/80, loss = 0.0320\n",
      "epoch 86 / 100, step 36/80, loss = 0.0304\n",
      "epoch 86 / 100, step 37/80, loss = 0.0318\n",
      "epoch 86 / 100, step 38/80, loss = 0.0312\n",
      "epoch 86 / 100, step 39/80, loss = 0.0331\n",
      "epoch 86 / 100, step 40/80, loss = 0.0339\n",
      "epoch 86 / 100, step 41/80, loss = 0.0323\n",
      "epoch 86 / 100, step 42/80, loss = 0.0318\n",
      "epoch 86 / 100, step 43/80, loss = 0.0341\n",
      "epoch 86 / 100, step 44/80, loss = 0.0310\n",
      "epoch 86 / 100, step 45/80, loss = 0.0326\n",
      "epoch 86 / 100, step 46/80, loss = 0.0295\n",
      "epoch 86 / 100, step 47/80, loss = 0.0306\n",
      "epoch 86 / 100, step 48/80, loss = 0.0324\n",
      "epoch 86 / 100, step 49/80, loss = 0.0356\n",
      "epoch 86 / 100, step 50/80, loss = 0.0317\n",
      "epoch 86 / 100, step 51/80, loss = 0.0417\n",
      "epoch 86 / 100, step 52/80, loss = 0.0320\n",
      "epoch 86 / 100, step 53/80, loss = 0.0345\n",
      "epoch 86 / 100, step 54/80, loss = 0.0298\n",
      "epoch 86 / 100, step 55/80, loss = 0.0370\n",
      "epoch 86 / 100, step 56/80, loss = 0.0307\n",
      "epoch 86 / 100, step 57/80, loss = 0.0321\n",
      "epoch 86 / 100, step 58/80, loss = 0.0323\n",
      "epoch 86 / 100, step 59/80, loss = 0.0321\n",
      "epoch 86 / 100, step 60/80, loss = 0.0296\n",
      "epoch 86 / 100, step 61/80, loss = 0.0327\n",
      "epoch 86 / 100, step 62/80, loss = 0.0324\n",
      "epoch 86 / 100, step 63/80, loss = 0.0322\n",
      "epoch 86 / 100, step 64/80, loss = 0.0359\n",
      "epoch 86 / 100, step 65/80, loss = 0.0325\n",
      "epoch 86 / 100, step 66/80, loss = 0.0314\n",
      "epoch 86 / 100, step 67/80, loss = 0.0367\n",
      "epoch 86 / 100, step 68/80, loss = 0.0313\n",
      "epoch 86 / 100, step 69/80, loss = 0.0333\n",
      "epoch 86 / 100, step 70/80, loss = 0.0322\n",
      "epoch 86 / 100, step 71/80, loss = 0.0291\n",
      "epoch 86 / 100, step 72/80, loss = 0.0290\n",
      "epoch 86 / 100, step 73/80, loss = 0.0332\n",
      "epoch 86 / 100, step 74/80, loss = 0.0332\n",
      "epoch 86 / 100, step 75/80, loss = 0.0300\n",
      "epoch 86 / 100, step 76/80, loss = 0.0326\n",
      "epoch 86 / 100, step 77/80, loss = 0.0330\n",
      "epoch 86 / 100, step 78/80, loss = 0.0345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86 / 100, step 79/80, loss = 0.0343\n",
      "epoch 86 / 100, step 80/80, loss = 0.0341\n",
      "epoch 87 / 100, step 1/80, loss = 0.0358\n",
      "epoch 87 / 100, step 2/80, loss = 0.0303\n",
      "epoch 87 / 100, step 3/80, loss = 0.0295\n",
      "epoch 87 / 100, step 4/80, loss = 0.0271\n",
      "epoch 87 / 100, step 5/80, loss = 0.0329\n",
      "epoch 87 / 100, step 6/80, loss = 0.0290\n",
      "epoch 87 / 100, step 7/80, loss = 0.0284\n",
      "epoch 87 / 100, step 8/80, loss = 0.0324\n",
      "epoch 87 / 100, step 9/80, loss = 0.0321\n",
      "epoch 87 / 100, step 10/80, loss = 0.0309\n",
      "epoch 87 / 100, step 11/80, loss = 0.0320\n",
      "epoch 87 / 100, step 12/80, loss = 0.0293\n",
      "epoch 87 / 100, step 13/80, loss = 0.0325\n",
      "epoch 87 / 100, step 14/80, loss = 0.0307\n",
      "epoch 87 / 100, step 15/80, loss = 0.0368\n",
      "epoch 87 / 100, step 16/80, loss = 0.0330\n",
      "epoch 87 / 100, step 17/80, loss = 0.0301\n",
      "epoch 87 / 100, step 18/80, loss = 0.0305\n",
      "epoch 87 / 100, step 19/80, loss = 0.0313\n",
      "epoch 87 / 100, step 20/80, loss = 0.0283\n",
      "epoch 87 / 100, step 21/80, loss = 0.0322\n",
      "epoch 87 / 100, step 22/80, loss = 0.0350\n",
      "epoch 87 / 100, step 23/80, loss = 0.0341\n",
      "epoch 87 / 100, step 24/80, loss = 0.0302\n",
      "epoch 87 / 100, step 25/80, loss = 0.0309\n",
      "epoch 87 / 100, step 26/80, loss = 0.0328\n",
      "epoch 87 / 100, step 27/80, loss = 0.0313\n",
      "epoch 87 / 100, step 28/80, loss = 0.0300\n",
      "epoch 87 / 100, step 29/80, loss = 0.0318\n",
      "epoch 87 / 100, step 30/80, loss = 0.0327\n",
      "epoch 87 / 100, step 31/80, loss = 0.0312\n",
      "epoch 87 / 100, step 32/80, loss = 0.0306\n",
      "epoch 87 / 100, step 33/80, loss = 0.0361\n",
      "epoch 87 / 100, step 34/80, loss = 0.0318\n",
      "epoch 87 / 100, step 35/80, loss = 0.0320\n",
      "epoch 87 / 100, step 36/80, loss = 0.0318\n",
      "epoch 87 / 100, step 37/80, loss = 0.0286\n",
      "epoch 87 / 100, step 38/80, loss = 0.0307\n",
      "epoch 87 / 100, step 39/80, loss = 0.0399\n",
      "epoch 87 / 100, step 40/80, loss = 0.0348\n",
      "epoch 87 / 100, step 41/80, loss = 0.0326\n",
      "epoch 87 / 100, step 42/80, loss = 0.0288\n",
      "epoch 87 / 100, step 43/80, loss = 0.0317\n",
      "epoch 87 / 100, step 44/80, loss = 0.0339\n",
      "epoch 87 / 100, step 45/80, loss = 0.0316\n",
      "epoch 87 / 100, step 46/80, loss = 0.0317\n",
      "epoch 87 / 100, step 47/80, loss = 0.0328\n",
      "epoch 87 / 100, step 48/80, loss = 0.0328\n",
      "epoch 87 / 100, step 49/80, loss = 0.0315\n",
      "epoch 87 / 100, step 50/80, loss = 0.0344\n",
      "epoch 87 / 100, step 51/80, loss = 0.0310\n",
      "epoch 87 / 100, step 52/80, loss = 0.0268\n",
      "epoch 87 / 100, step 53/80, loss = 0.0305\n",
      "epoch 87 / 100, step 54/80, loss = 0.0299\n",
      "epoch 87 / 100, step 55/80, loss = 0.0322\n",
      "epoch 87 / 100, step 56/80, loss = 0.0341\n",
      "epoch 87 / 100, step 57/80, loss = 0.0318\n",
      "epoch 87 / 100, step 58/80, loss = 0.0297\n",
      "epoch 87 / 100, step 59/80, loss = 0.0319\n",
      "epoch 87 / 100, step 60/80, loss = 0.0273\n",
      "epoch 87 / 100, step 61/80, loss = 0.0340\n",
      "epoch 87 / 100, step 62/80, loss = 0.0303\n",
      "epoch 87 / 100, step 63/80, loss = 0.0315\n",
      "epoch 87 / 100, step 64/80, loss = 0.0313\n",
      "epoch 87 / 100, step 65/80, loss = 0.0322\n",
      "epoch 87 / 100, step 66/80, loss = 0.0351\n",
      "epoch 87 / 100, step 67/80, loss = 0.0357\n",
      "epoch 87 / 100, step 68/80, loss = 0.0323\n",
      "epoch 87 / 100, step 69/80, loss = 0.0296\n",
      "epoch 87 / 100, step 70/80, loss = 0.0300\n",
      "epoch 87 / 100, step 71/80, loss = 0.0306\n",
      "epoch 87 / 100, step 72/80, loss = 0.0292\n",
      "epoch 87 / 100, step 73/80, loss = 0.0268\n",
      "epoch 87 / 100, step 74/80, loss = 0.0356\n",
      "epoch 87 / 100, step 75/80, loss = 0.0329\n",
      "epoch 87 / 100, step 76/80, loss = 0.0309\n",
      "epoch 87 / 100, step 77/80, loss = 0.0361\n",
      "epoch 87 / 100, step 78/80, loss = 0.0365\n",
      "epoch 87 / 100, step 79/80, loss = 0.0293\n",
      "epoch 87 / 100, step 80/80, loss = 0.0384\n",
      "epoch 88 / 100, step 1/80, loss = 0.0331\n",
      "epoch 88 / 100, step 2/80, loss = 0.0307\n",
      "epoch 88 / 100, step 3/80, loss = 0.0282\n",
      "epoch 88 / 100, step 4/80, loss = 0.0301\n",
      "epoch 88 / 100, step 5/80, loss = 0.0297\n",
      "epoch 88 / 100, step 6/80, loss = 0.0339\n",
      "epoch 88 / 100, step 7/80, loss = 0.0324\n",
      "epoch 88 / 100, step 8/80, loss = 0.0363\n",
      "epoch 88 / 100, step 9/80, loss = 0.0351\n",
      "epoch 88 / 100, step 10/80, loss = 0.0318\n",
      "epoch 88 / 100, step 11/80, loss = 0.0332\n",
      "epoch 88 / 100, step 12/80, loss = 0.0313\n",
      "epoch 88 / 100, step 13/80, loss = 0.0317\n",
      "epoch 88 / 100, step 14/80, loss = 0.0304\n",
      "epoch 88 / 100, step 15/80, loss = 0.0290\n",
      "epoch 88 / 100, step 16/80, loss = 0.0309\n",
      "epoch 88 / 100, step 17/80, loss = 0.0281\n",
      "epoch 88 / 100, step 18/80, loss = 0.0319\n",
      "epoch 88 / 100, step 19/80, loss = 0.0339\n",
      "epoch 88 / 100, step 20/80, loss = 0.0319\n",
      "epoch 88 / 100, step 21/80, loss = 0.0258\n",
      "epoch 88 / 100, step 22/80, loss = 0.0299\n",
      "epoch 88 / 100, step 23/80, loss = 0.0301\n",
      "epoch 88 / 100, step 24/80, loss = 0.0314\n",
      "epoch 88 / 100, step 25/80, loss = 0.0303\n",
      "epoch 88 / 100, step 26/80, loss = 0.0294\n",
      "epoch 88 / 100, step 27/80, loss = 0.0301\n",
      "epoch 88 / 100, step 28/80, loss = 0.0281\n",
      "epoch 88 / 100, step 29/80, loss = 0.0295\n",
      "epoch 88 / 100, step 30/80, loss = 0.0324\n",
      "epoch 88 / 100, step 31/80, loss = 0.0334\n",
      "epoch 88 / 100, step 32/80, loss = 0.0315\n",
      "epoch 88 / 100, step 33/80, loss = 0.0294\n",
      "epoch 88 / 100, step 34/80, loss = 0.0370\n",
      "epoch 88 / 100, step 35/80, loss = 0.0334\n",
      "epoch 88 / 100, step 36/80, loss = 0.0321\n",
      "epoch 88 / 100, step 37/80, loss = 0.0339\n",
      "epoch 88 / 100, step 38/80, loss = 0.0313\n",
      "epoch 88 / 100, step 39/80, loss = 0.0369\n",
      "epoch 88 / 100, step 40/80, loss = 0.0321\n",
      "epoch 88 / 100, step 41/80, loss = 0.0293\n",
      "epoch 88 / 100, step 42/80, loss = 0.0322\n",
      "epoch 88 / 100, step 43/80, loss = 0.0302\n",
      "epoch 88 / 100, step 44/80, loss = 0.0327\n",
      "epoch 88 / 100, step 45/80, loss = 0.0312\n",
      "epoch 88 / 100, step 46/80, loss = 0.0274\n",
      "epoch 88 / 100, step 47/80, loss = 0.0304\n",
      "epoch 88 / 100, step 48/80, loss = 0.0342\n",
      "epoch 88 / 100, step 49/80, loss = 0.0295\n",
      "epoch 88 / 100, step 50/80, loss = 0.0341\n",
      "epoch 88 / 100, step 51/80, loss = 0.0312\n",
      "epoch 88 / 100, step 52/80, loss = 0.0335\n",
      "epoch 88 / 100, step 53/80, loss = 0.0339\n",
      "epoch 88 / 100, step 54/80, loss = 0.0334\n",
      "epoch 88 / 100, step 55/80, loss = 0.0321\n",
      "epoch 88 / 100, step 56/80, loss = 0.0326\n",
      "epoch 88 / 100, step 57/80, loss = 0.0291\n",
      "epoch 88 / 100, step 58/80, loss = 0.0331\n",
      "epoch 88 / 100, step 59/80, loss = 0.0343\n",
      "epoch 88 / 100, step 60/80, loss = 0.0314\n",
      "epoch 88 / 100, step 61/80, loss = 0.0330\n",
      "epoch 88 / 100, step 62/80, loss = 0.0296\n",
      "epoch 88 / 100, step 63/80, loss = 0.0348\n",
      "epoch 88 / 100, step 64/80, loss = 0.0358\n",
      "epoch 88 / 100, step 65/80, loss = 0.0318\n",
      "epoch 88 / 100, step 66/80, loss = 0.0272\n",
      "epoch 88 / 100, step 67/80, loss = 0.0273\n",
      "epoch 88 / 100, step 68/80, loss = 0.0316\n",
      "epoch 88 / 100, step 69/80, loss = 0.0289\n",
      "epoch 88 / 100, step 70/80, loss = 0.0281\n",
      "epoch 88 / 100, step 71/80, loss = 0.0309\n",
      "epoch 88 / 100, step 72/80, loss = 0.0346\n",
      "epoch 88 / 100, step 73/80, loss = 0.0309\n",
      "epoch 88 / 100, step 74/80, loss = 0.0337\n",
      "epoch 88 / 100, step 75/80, loss = 0.0296\n",
      "epoch 88 / 100, step 76/80, loss = 0.0285\n",
      "epoch 88 / 100, step 77/80, loss = 0.0317\n",
      "epoch 88 / 100, step 78/80, loss = 0.0285\n",
      "epoch 88 / 100, step 79/80, loss = 0.0312\n",
      "epoch 88 / 100, step 80/80, loss = 0.0340\n",
      "epoch 89 / 100, step 1/80, loss = 0.0267\n",
      "epoch 89 / 100, step 2/80, loss = 0.0316\n",
      "epoch 89 / 100, step 3/80, loss = 0.0296\n",
      "epoch 89 / 100, step 4/80, loss = 0.0290\n",
      "epoch 89 / 100, step 5/80, loss = 0.0309\n",
      "epoch 89 / 100, step 6/80, loss = 0.0296\n",
      "epoch 89 / 100, step 7/80, loss = 0.0365\n",
      "epoch 89 / 100, step 8/80, loss = 0.0299\n",
      "epoch 89 / 100, step 9/80, loss = 0.0339\n",
      "epoch 89 / 100, step 10/80, loss = 0.0340\n",
      "epoch 89 / 100, step 11/80, loss = 0.0317\n",
      "epoch 89 / 100, step 12/80, loss = 0.0295\n",
      "epoch 89 / 100, step 13/80, loss = 0.0296\n",
      "epoch 89 / 100, step 14/80, loss = 0.0292\n",
      "epoch 89 / 100, step 15/80, loss = 0.0326\n",
      "epoch 89 / 100, step 16/80, loss = 0.0286\n",
      "epoch 89 / 100, step 17/80, loss = 0.0292\n",
      "epoch 89 / 100, step 18/80, loss = 0.0343\n",
      "epoch 89 / 100, step 19/80, loss = 0.0354\n",
      "epoch 89 / 100, step 20/80, loss = 0.0325\n",
      "epoch 89 / 100, step 21/80, loss = 0.0290\n",
      "epoch 89 / 100, step 22/80, loss = 0.0331\n",
      "epoch 89 / 100, step 23/80, loss = 0.0305\n",
      "epoch 89 / 100, step 24/80, loss = 0.0300\n",
      "epoch 89 / 100, step 25/80, loss = 0.0296\n",
      "epoch 89 / 100, step 26/80, loss = 0.0291\n",
      "epoch 89 / 100, step 27/80, loss = 0.0295\n",
      "epoch 89 / 100, step 28/80, loss = 0.0310\n",
      "epoch 89 / 100, step 29/80, loss = 0.0315\n",
      "epoch 89 / 100, step 30/80, loss = 0.0264\n",
      "epoch 89 / 100, step 31/80, loss = 0.0321\n",
      "epoch 89 / 100, step 32/80, loss = 0.0286\n",
      "epoch 89 / 100, step 33/80, loss = 0.0290\n",
      "epoch 89 / 100, step 34/80, loss = 0.0260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89 / 100, step 35/80, loss = 0.0300\n",
      "epoch 89 / 100, step 36/80, loss = 0.0298\n",
      "epoch 89 / 100, step 37/80, loss = 0.0300\n",
      "epoch 89 / 100, step 38/80, loss = 0.0336\n",
      "epoch 89 / 100, step 39/80, loss = 0.0301\n",
      "epoch 89 / 100, step 40/80, loss = 0.0286\n",
      "epoch 89 / 100, step 41/80, loss = 0.0319\n",
      "epoch 89 / 100, step 42/80, loss = 0.0314\n",
      "epoch 89 / 100, step 43/80, loss = 0.0322\n",
      "epoch 89 / 100, step 44/80, loss = 0.0369\n",
      "epoch 89 / 100, step 45/80, loss = 0.0358\n",
      "epoch 89 / 100, step 46/80, loss = 0.0283\n",
      "epoch 89 / 100, step 47/80, loss = 0.0335\n",
      "epoch 89 / 100, step 48/80, loss = 0.0312\n",
      "epoch 89 / 100, step 49/80, loss = 0.0327\n",
      "epoch 89 / 100, step 50/80, loss = 0.0305\n",
      "epoch 89 / 100, step 51/80, loss = 0.0320\n",
      "epoch 89 / 100, step 52/80, loss = 0.0315\n",
      "epoch 89 / 100, step 53/80, loss = 0.0313\n",
      "epoch 89 / 100, step 54/80, loss = 0.0297\n",
      "epoch 89 / 100, step 55/80, loss = 0.0306\n",
      "epoch 89 / 100, step 56/80, loss = 0.0281\n",
      "epoch 89 / 100, step 57/80, loss = 0.0414\n",
      "epoch 89 / 100, step 58/80, loss = 0.0303\n",
      "epoch 89 / 100, step 59/80, loss = 0.0341\n",
      "epoch 89 / 100, step 60/80, loss = 0.0303\n",
      "epoch 89 / 100, step 61/80, loss = 0.0295\n",
      "epoch 89 / 100, step 62/80, loss = 0.0295\n",
      "epoch 89 / 100, step 63/80, loss = 0.0344\n",
      "epoch 89 / 100, step 64/80, loss = 0.0322\n",
      "epoch 89 / 100, step 65/80, loss = 0.0306\n",
      "epoch 89 / 100, step 66/80, loss = 0.0309\n",
      "epoch 89 / 100, step 67/80, loss = 0.0286\n",
      "epoch 89 / 100, step 68/80, loss = 0.0326\n",
      "epoch 89 / 100, step 69/80, loss = 0.0367\n",
      "epoch 89 / 100, step 70/80, loss = 0.0359\n",
      "epoch 89 / 100, step 71/80, loss = 0.0308\n",
      "epoch 89 / 100, step 72/80, loss = 0.0323\n",
      "epoch 89 / 100, step 73/80, loss = 0.0339\n",
      "epoch 89 / 100, step 74/80, loss = 0.0310\n",
      "epoch 89 / 100, step 75/80, loss = 0.0339\n",
      "epoch 89 / 100, step 76/80, loss = 0.0303\n",
      "epoch 89 / 100, step 77/80, loss = 0.0322\n",
      "epoch 89 / 100, step 78/80, loss = 0.0341\n",
      "epoch 89 / 100, step 79/80, loss = 0.0307\n",
      "epoch 89 / 100, step 80/80, loss = 0.0322\n",
      "epoch 90 / 100, step 1/80, loss = 0.0264\n",
      "epoch 90 / 100, step 2/80, loss = 0.0304\n",
      "epoch 90 / 100, step 3/80, loss = 0.0297\n",
      "epoch 90 / 100, step 4/80, loss = 0.0325\n",
      "epoch 90 / 100, step 5/80, loss = 0.0295\n",
      "epoch 90 / 100, step 6/80, loss = 0.0332\n",
      "epoch 90 / 100, step 7/80, loss = 0.0311\n",
      "epoch 90 / 100, step 8/80, loss = 0.0286\n",
      "epoch 90 / 100, step 9/80, loss = 0.0314\n",
      "epoch 90 / 100, step 10/80, loss = 0.0290\n",
      "epoch 90 / 100, step 11/80, loss = 0.0301\n",
      "epoch 90 / 100, step 12/80, loss = 0.0291\n",
      "epoch 90 / 100, step 13/80, loss = 0.0280\n",
      "epoch 90 / 100, step 14/80, loss = 0.0283\n",
      "epoch 90 / 100, step 15/80, loss = 0.0327\n",
      "epoch 90 / 100, step 16/80, loss = 0.0288\n",
      "epoch 90 / 100, step 17/80, loss = 0.0325\n",
      "epoch 90 / 100, step 18/80, loss = 0.0294\n",
      "epoch 90 / 100, step 19/80, loss = 0.0298\n",
      "epoch 90 / 100, step 20/80, loss = 0.0300\n",
      "epoch 90 / 100, step 21/80, loss = 0.0307\n",
      "epoch 90 / 100, step 22/80, loss = 0.0282\n",
      "epoch 90 / 100, step 23/80, loss = 0.0313\n",
      "epoch 90 / 100, step 24/80, loss = 0.0293\n",
      "epoch 90 / 100, step 25/80, loss = 0.0287\n",
      "epoch 90 / 100, step 26/80, loss = 0.0325\n",
      "epoch 90 / 100, step 27/80, loss = 0.0325\n",
      "epoch 90 / 100, step 28/80, loss = 0.0311\n",
      "epoch 90 / 100, step 29/80, loss = 0.0376\n",
      "epoch 90 / 100, step 30/80, loss = 0.0333\n",
      "epoch 90 / 100, step 31/80, loss = 0.0332\n",
      "epoch 90 / 100, step 32/80, loss = 0.0297\n",
      "epoch 90 / 100, step 33/80, loss = 0.0333\n",
      "epoch 90 / 100, step 34/80, loss = 0.0321\n",
      "epoch 90 / 100, step 35/80, loss = 0.0309\n",
      "epoch 90 / 100, step 36/80, loss = 0.0295\n",
      "epoch 90 / 100, step 37/80, loss = 0.0296\n",
      "epoch 90 / 100, step 38/80, loss = 0.0314\n",
      "epoch 90 / 100, step 39/80, loss = 0.0303\n",
      "epoch 90 / 100, step 40/80, loss = 0.0299\n",
      "epoch 90 / 100, step 41/80, loss = 0.0283\n",
      "epoch 90 / 100, step 42/80, loss = 0.0299\n",
      "epoch 90 / 100, step 43/80, loss = 0.0301\n",
      "epoch 90 / 100, step 44/80, loss = 0.0273\n",
      "epoch 90 / 100, step 45/80, loss = 0.0289\n",
      "epoch 90 / 100, step 46/80, loss = 0.0303\n",
      "epoch 90 / 100, step 47/80, loss = 0.0309\n",
      "epoch 90 / 100, step 48/80, loss = 0.0278\n",
      "epoch 90 / 100, step 49/80, loss = 0.0346\n",
      "epoch 90 / 100, step 50/80, loss = 0.0340\n",
      "epoch 90 / 100, step 51/80, loss = 0.0336\n",
      "epoch 90 / 100, step 52/80, loss = 0.0312\n",
      "epoch 90 / 100, step 53/80, loss = 0.0304\n",
      "epoch 90 / 100, step 54/80, loss = 0.0311\n",
      "epoch 90 / 100, step 55/80, loss = 0.0311\n",
      "epoch 90 / 100, step 56/80, loss = 0.0299\n",
      "epoch 90 / 100, step 57/80, loss = 0.0276\n",
      "epoch 90 / 100, step 58/80, loss = 0.0282\n",
      "epoch 90 / 100, step 59/80, loss = 0.0316\n",
      "epoch 90 / 100, step 60/80, loss = 0.0340\n",
      "epoch 90 / 100, step 61/80, loss = 0.0341\n",
      "epoch 90 / 100, step 62/80, loss = 0.0319\n",
      "epoch 90 / 100, step 63/80, loss = 0.0305\n",
      "epoch 90 / 100, step 64/80, loss = 0.0320\n",
      "epoch 90 / 100, step 65/80, loss = 0.0286\n",
      "epoch 90 / 100, step 66/80, loss = 0.0298\n",
      "epoch 90 / 100, step 67/80, loss = 0.0342\n",
      "epoch 90 / 100, step 68/80, loss = 0.0403\n",
      "epoch 90 / 100, step 69/80, loss = 0.0326\n",
      "epoch 90 / 100, step 70/80, loss = 0.0285\n",
      "epoch 90 / 100, step 71/80, loss = 0.0308\n",
      "epoch 90 / 100, step 72/80, loss = 0.0301\n",
      "epoch 90 / 100, step 73/80, loss = 0.0318\n",
      "epoch 90 / 100, step 74/80, loss = 0.0307\n",
      "epoch 90 / 100, step 75/80, loss = 0.0363\n",
      "epoch 90 / 100, step 76/80, loss = 0.0297\n",
      "epoch 90 / 100, step 77/80, loss = 0.0278\n",
      "epoch 90 / 100, step 78/80, loss = 0.0304\n",
      "epoch 90 / 100, step 79/80, loss = 0.0302\n",
      "epoch 90 / 100, step 80/80, loss = 0.0309\n",
      "epoch 91 / 100, step 1/80, loss = 0.0296\n",
      "epoch 91 / 100, step 2/80, loss = 0.0293\n",
      "epoch 91 / 100, step 3/80, loss = 0.0338\n",
      "epoch 91 / 100, step 4/80, loss = 0.0279\n",
      "epoch 91 / 100, step 5/80, loss = 0.0286\n",
      "epoch 91 / 100, step 6/80, loss = 0.0296\n",
      "epoch 91 / 100, step 7/80, loss = 0.0285\n",
      "epoch 91 / 100, step 8/80, loss = 0.0288\n",
      "epoch 91 / 100, step 9/80, loss = 0.0305\n",
      "epoch 91 / 100, step 10/80, loss = 0.0289\n",
      "epoch 91 / 100, step 11/80, loss = 0.0306\n",
      "epoch 91 / 100, step 12/80, loss = 0.0304\n",
      "epoch 91 / 100, step 13/80, loss = 0.0304\n",
      "epoch 91 / 100, step 14/80, loss = 0.0297\n",
      "epoch 91 / 100, step 15/80, loss = 0.0301\n",
      "epoch 91 / 100, step 16/80, loss = 0.0291\n",
      "epoch 91 / 100, step 17/80, loss = 0.0279\n",
      "epoch 91 / 100, step 18/80, loss = 0.0280\n",
      "epoch 91 / 100, step 19/80, loss = 0.0261\n",
      "epoch 91 / 100, step 20/80, loss = 0.0280\n",
      "epoch 91 / 100, step 21/80, loss = 0.0277\n",
      "epoch 91 / 100, step 22/80, loss = 0.0315\n",
      "epoch 91 / 100, step 23/80, loss = 0.0322\n",
      "epoch 91 / 100, step 24/80, loss = 0.0301\n",
      "epoch 91 / 100, step 25/80, loss = 0.0315\n",
      "epoch 91 / 100, step 26/80, loss = 0.0272\n",
      "epoch 91 / 100, step 27/80, loss = 0.0290\n",
      "epoch 91 / 100, step 28/80, loss = 0.0314\n",
      "epoch 91 / 100, step 29/80, loss = 0.0272\n",
      "epoch 91 / 100, step 30/80, loss = 0.0293\n",
      "epoch 91 / 100, step 31/80, loss = 0.0282\n",
      "epoch 91 / 100, step 32/80, loss = 0.0312\n",
      "epoch 91 / 100, step 33/80, loss = 0.0306\n",
      "epoch 91 / 100, step 34/80, loss = 0.0297\n",
      "epoch 91 / 100, step 35/80, loss = 0.0299\n",
      "epoch 91 / 100, step 36/80, loss = 0.0290\n",
      "epoch 91 / 100, step 37/80, loss = 0.0326\n",
      "epoch 91 / 100, step 38/80, loss = 0.0350\n",
      "epoch 91 / 100, step 39/80, loss = 0.0299\n",
      "epoch 91 / 100, step 40/80, loss = 0.0308\n",
      "epoch 91 / 100, step 41/80, loss = 0.0339\n",
      "epoch 91 / 100, step 42/80, loss = 0.0390\n",
      "epoch 91 / 100, step 43/80, loss = 0.0294\n",
      "epoch 91 / 100, step 44/80, loss = 0.0287\n",
      "epoch 91 / 100, step 45/80, loss = 0.0294\n",
      "epoch 91 / 100, step 46/80, loss = 0.0283\n",
      "epoch 91 / 100, step 47/80, loss = 0.0307\n",
      "epoch 91 / 100, step 48/80, loss = 0.0297\n",
      "epoch 91 / 100, step 49/80, loss = 0.0307\n",
      "epoch 91 / 100, step 50/80, loss = 0.0295\n",
      "epoch 91 / 100, step 51/80, loss = 0.0284\n",
      "epoch 91 / 100, step 52/80, loss = 0.0305\n",
      "epoch 91 / 100, step 53/80, loss = 0.0321\n",
      "epoch 91 / 100, step 54/80, loss = 0.0326\n",
      "epoch 91 / 100, step 55/80, loss = 0.0277\n",
      "epoch 91 / 100, step 56/80, loss = 0.0337\n",
      "epoch 91 / 100, step 57/80, loss = 0.0295\n",
      "epoch 91 / 100, step 58/80, loss = 0.0302\n",
      "epoch 91 / 100, step 59/80, loss = 0.0337\n",
      "epoch 91 / 100, step 60/80, loss = 0.0313\n",
      "epoch 91 / 100, step 61/80, loss = 0.0313\n",
      "epoch 91 / 100, step 62/80, loss = 0.0319\n",
      "epoch 91 / 100, step 63/80, loss = 0.0339\n",
      "epoch 91 / 100, step 64/80, loss = 0.0290\n",
      "epoch 91 / 100, step 65/80, loss = 0.0303\n",
      "epoch 91 / 100, step 66/80, loss = 0.0345\n",
      "epoch 91 / 100, step 67/80, loss = 0.0313\n",
      "epoch 91 / 100, step 68/80, loss = 0.0314\n",
      "epoch 91 / 100, step 69/80, loss = 0.0325\n",
      "epoch 91 / 100, step 70/80, loss = 0.0312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91 / 100, step 71/80, loss = 0.0325\n",
      "epoch 91 / 100, step 72/80, loss = 0.0335\n",
      "epoch 91 / 100, step 73/80, loss = 0.0290\n",
      "epoch 91 / 100, step 74/80, loss = 0.0325\n",
      "epoch 91 / 100, step 75/80, loss = 0.0296\n",
      "epoch 91 / 100, step 76/80, loss = 0.0317\n",
      "epoch 91 / 100, step 77/80, loss = 0.0297\n",
      "epoch 91 / 100, step 78/80, loss = 0.0356\n",
      "epoch 91 / 100, step 79/80, loss = 0.0306\n",
      "epoch 91 / 100, step 80/80, loss = 0.0327\n",
      "epoch 92 / 100, step 1/80, loss = 0.0296\n",
      "epoch 92 / 100, step 2/80, loss = 0.0309\n",
      "epoch 92 / 100, step 3/80, loss = 0.0295\n",
      "epoch 92 / 100, step 4/80, loss = 0.0293\n",
      "epoch 92 / 100, step 5/80, loss = 0.0290\n",
      "epoch 92 / 100, step 6/80, loss = 0.0332\n",
      "epoch 92 / 100, step 7/80, loss = 0.0291\n",
      "epoch 92 / 100, step 8/80, loss = 0.0290\n",
      "epoch 92 / 100, step 9/80, loss = 0.0318\n",
      "epoch 92 / 100, step 10/80, loss = 0.0291\n",
      "epoch 92 / 100, step 11/80, loss = 0.0309\n",
      "epoch 92 / 100, step 12/80, loss = 0.0275\n",
      "epoch 92 / 100, step 13/80, loss = 0.0303\n",
      "epoch 92 / 100, step 14/80, loss = 0.0294\n",
      "epoch 92 / 100, step 15/80, loss = 0.0303\n",
      "epoch 92 / 100, step 16/80, loss = 0.0278\n",
      "epoch 92 / 100, step 17/80, loss = 0.0292\n",
      "epoch 92 / 100, step 18/80, loss = 0.0347\n",
      "epoch 92 / 100, step 19/80, loss = 0.0271\n",
      "epoch 92 / 100, step 20/80, loss = 0.0279\n",
      "epoch 92 / 100, step 21/80, loss = 0.0278\n",
      "epoch 92 / 100, step 22/80, loss = 0.0296\n",
      "epoch 92 / 100, step 23/80, loss = 0.0266\n",
      "epoch 92 / 100, step 24/80, loss = 0.0273\n",
      "epoch 92 / 100, step 25/80, loss = 0.0299\n",
      "epoch 92 / 100, step 26/80, loss = 0.0291\n",
      "epoch 92 / 100, step 27/80, loss = 0.0356\n",
      "epoch 92 / 100, step 28/80, loss = 0.0285\n",
      "epoch 92 / 100, step 29/80, loss = 0.0284\n",
      "epoch 92 / 100, step 30/80, loss = 0.0265\n",
      "epoch 92 / 100, step 31/80, loss = 0.0295\n",
      "epoch 92 / 100, step 32/80, loss = 0.0295\n",
      "epoch 92 / 100, step 33/80, loss = 0.0274\n",
      "epoch 92 / 100, step 34/80, loss = 0.0273\n",
      "epoch 92 / 100, step 35/80, loss = 0.0281\n",
      "epoch 92 / 100, step 36/80, loss = 0.0335\n",
      "epoch 92 / 100, step 37/80, loss = 0.0269\n",
      "epoch 92 / 100, step 38/80, loss = 0.0294\n",
      "epoch 92 / 100, step 39/80, loss = 0.0288\n",
      "epoch 92 / 100, step 40/80, loss = 0.0298\n",
      "epoch 92 / 100, step 41/80, loss = 0.0274\n",
      "epoch 92 / 100, step 42/80, loss = 0.0298\n",
      "epoch 92 / 100, step 43/80, loss = 0.0268\n",
      "epoch 92 / 100, step 44/80, loss = 0.0318\n",
      "epoch 92 / 100, step 45/80, loss = 0.0305\n",
      "epoch 92 / 100, step 46/80, loss = 0.0313\n",
      "epoch 92 / 100, step 47/80, loss = 0.0299\n",
      "epoch 92 / 100, step 48/80, loss = 0.0311\n",
      "epoch 92 / 100, step 49/80, loss = 0.0280\n",
      "epoch 92 / 100, step 50/80, loss = 0.0274\n",
      "epoch 92 / 100, step 51/80, loss = 0.0264\n",
      "epoch 92 / 100, step 52/80, loss = 0.0309\n",
      "epoch 92 / 100, step 53/80, loss = 0.0325\n",
      "epoch 92 / 100, step 54/80, loss = 0.0311\n",
      "epoch 92 / 100, step 55/80, loss = 0.0294\n",
      "epoch 92 / 100, step 56/80, loss = 0.0312\n",
      "epoch 92 / 100, step 57/80, loss = 0.0332\n",
      "epoch 92 / 100, step 58/80, loss = 0.0290\n",
      "epoch 92 / 100, step 59/80, loss = 0.0316\n",
      "epoch 92 / 100, step 60/80, loss = 0.0273\n",
      "epoch 92 / 100, step 61/80, loss = 0.0303\n",
      "epoch 92 / 100, step 62/80, loss = 0.0298\n",
      "epoch 92 / 100, step 63/80, loss = 0.0277\n",
      "epoch 92 / 100, step 64/80, loss = 0.0343\n",
      "epoch 92 / 100, step 65/80, loss = 0.0320\n",
      "epoch 92 / 100, step 66/80, loss = 0.0324\n",
      "epoch 92 / 100, step 67/80, loss = 0.0332\n",
      "epoch 92 / 100, step 68/80, loss = 0.0308\n",
      "epoch 92 / 100, step 69/80, loss = 0.0308\n",
      "epoch 92 / 100, step 70/80, loss = 0.0317\n",
      "epoch 92 / 100, step 71/80, loss = 0.0290\n",
      "epoch 92 / 100, step 72/80, loss = 0.0322\n",
      "epoch 92 / 100, step 73/80, loss = 0.0281\n",
      "epoch 92 / 100, step 74/80, loss = 0.0299\n",
      "epoch 92 / 100, step 75/80, loss = 0.0288\n",
      "epoch 92 / 100, step 76/80, loss = 0.0274\n",
      "epoch 92 / 100, step 77/80, loss = 0.0325\n",
      "epoch 92 / 100, step 78/80, loss = 0.0319\n",
      "epoch 92 / 100, step 79/80, loss = 0.0332\n",
      "epoch 92 / 100, step 80/80, loss = 0.0277\n",
      "epoch 93 / 100, step 1/80, loss = 0.0338\n",
      "epoch 93 / 100, step 2/80, loss = 0.0287\n",
      "epoch 93 / 100, step 3/80, loss = 0.0296\n",
      "epoch 93 / 100, step 4/80, loss = 0.0322\n",
      "epoch 93 / 100, step 5/80, loss = 0.0288\n",
      "epoch 93 / 100, step 6/80, loss = 0.0299\n",
      "epoch 93 / 100, step 7/80, loss = 0.0279\n",
      "epoch 93 / 100, step 8/80, loss = 0.0327\n",
      "epoch 93 / 100, step 9/80, loss = 0.0273\n",
      "epoch 93 / 100, step 10/80, loss = 0.0276\n",
      "epoch 93 / 100, step 11/80, loss = 0.0297\n",
      "epoch 93 / 100, step 12/80, loss = 0.0300\n",
      "epoch 93 / 100, step 13/80, loss = 0.0276\n",
      "epoch 93 / 100, step 14/80, loss = 0.0284\n",
      "epoch 93 / 100, step 15/80, loss = 0.0297\n",
      "epoch 93 / 100, step 16/80, loss = 0.0300\n",
      "epoch 93 / 100, step 17/80, loss = 0.0275\n",
      "epoch 93 / 100, step 18/80, loss = 0.0310\n",
      "epoch 93 / 100, step 19/80, loss = 0.0252\n",
      "epoch 93 / 100, step 20/80, loss = 0.0387\n",
      "epoch 93 / 100, step 21/80, loss = 0.0318\n",
      "epoch 93 / 100, step 22/80, loss = 0.0283\n",
      "epoch 93 / 100, step 23/80, loss = 0.0325\n",
      "epoch 93 / 100, step 24/80, loss = 0.0333\n",
      "epoch 93 / 100, step 25/80, loss = 0.0338\n",
      "epoch 93 / 100, step 26/80, loss = 0.0306\n",
      "epoch 93 / 100, step 27/80, loss = 0.0325\n",
      "epoch 93 / 100, step 28/80, loss = 0.0363\n",
      "epoch 93 / 100, step 29/80, loss = 0.0297\n",
      "epoch 93 / 100, step 30/80, loss = 0.0298\n",
      "epoch 93 / 100, step 31/80, loss = 0.0315\n",
      "epoch 93 / 100, step 32/80, loss = 0.0310\n",
      "epoch 93 / 100, step 33/80, loss = 0.0317\n",
      "epoch 93 / 100, step 34/80, loss = 0.0293\n",
      "epoch 93 / 100, step 35/80, loss = 0.0285\n",
      "epoch 93 / 100, step 36/80, loss = 0.0302\n",
      "epoch 93 / 100, step 37/80, loss = 0.0295\n",
      "epoch 93 / 100, step 38/80, loss = 0.0344\n",
      "epoch 93 / 100, step 39/80, loss = 0.0321\n",
      "epoch 93 / 100, step 40/80, loss = 0.0315\n",
      "epoch 93 / 100, step 41/80, loss = 0.0300\n",
      "epoch 93 / 100, step 42/80, loss = 0.0282\n",
      "epoch 93 / 100, step 43/80, loss = 0.0276\n",
      "epoch 93 / 100, step 44/80, loss = 0.0290\n",
      "epoch 93 / 100, step 45/80, loss = 0.0309\n",
      "epoch 93 / 100, step 46/80, loss = 0.0316\n",
      "epoch 93 / 100, step 47/80, loss = 0.0326\n",
      "epoch 93 / 100, step 48/80, loss = 0.0295\n",
      "epoch 93 / 100, step 49/80, loss = 0.0287\n",
      "epoch 93 / 100, step 50/80, loss = 0.0313\n",
      "epoch 93 / 100, step 51/80, loss = 0.0294\n",
      "epoch 93 / 100, step 52/80, loss = 0.0326\n",
      "epoch 93 / 100, step 53/80, loss = 0.0324\n",
      "epoch 93 / 100, step 54/80, loss = 0.0278\n",
      "epoch 93 / 100, step 55/80, loss = 0.0286\n",
      "epoch 93 / 100, step 56/80, loss = 0.0339\n",
      "epoch 93 / 100, step 57/80, loss = 0.0307\n",
      "epoch 93 / 100, step 58/80, loss = 0.0285\n",
      "epoch 93 / 100, step 59/80, loss = 0.0325\n",
      "epoch 93 / 100, step 60/80, loss = 0.0328\n",
      "epoch 93 / 100, step 61/80, loss = 0.0301\n",
      "epoch 93 / 100, step 62/80, loss = 0.0278\n",
      "epoch 93 / 100, step 63/80, loss = 0.0308\n",
      "epoch 93 / 100, step 64/80, loss = 0.0283\n",
      "epoch 93 / 100, step 65/80, loss = 0.0309\n",
      "epoch 93 / 100, step 66/80, loss = 0.0386\n",
      "epoch 93 / 100, step 67/80, loss = 0.0291\n",
      "epoch 93 / 100, step 68/80, loss = 0.0287\n",
      "epoch 93 / 100, step 69/80, loss = 0.0266\n",
      "epoch 93 / 100, step 70/80, loss = 0.0324\n",
      "epoch 93 / 100, step 71/80, loss = 0.0280\n",
      "epoch 93 / 100, step 72/80, loss = 0.0310\n",
      "epoch 93 / 100, step 73/80, loss = 0.0337\n",
      "epoch 93 / 100, step 74/80, loss = 0.0285\n",
      "epoch 93 / 100, step 75/80, loss = 0.0332\n",
      "epoch 93 / 100, step 76/80, loss = 0.0302\n",
      "epoch 93 / 100, step 77/80, loss = 0.0390\n",
      "epoch 93 / 100, step 78/80, loss = 0.0311\n",
      "epoch 93 / 100, step 79/80, loss = 0.0262\n",
      "epoch 93 / 100, step 80/80, loss = 0.0328\n",
      "epoch 94 / 100, step 1/80, loss = 0.0280\n",
      "epoch 94 / 100, step 2/80, loss = 0.0288\n",
      "epoch 94 / 100, step 3/80, loss = 0.0306\n",
      "epoch 94 / 100, step 4/80, loss = 0.0338\n",
      "epoch 94 / 100, step 5/80, loss = 0.0409\n",
      "epoch 94 / 100, step 6/80, loss = 0.0266\n",
      "epoch 94 / 100, step 7/80, loss = 0.0302\n",
      "epoch 94 / 100, step 8/80, loss = 0.0327\n",
      "epoch 94 / 100, step 9/80, loss = 0.0319\n",
      "epoch 94 / 100, step 10/80, loss = 0.0286\n",
      "epoch 94 / 100, step 11/80, loss = 0.0329\n",
      "epoch 94 / 100, step 12/80, loss = 0.0283\n",
      "epoch 94 / 100, step 13/80, loss = 0.0288\n",
      "epoch 94 / 100, step 14/80, loss = 0.0277\n",
      "epoch 94 / 100, step 15/80, loss = 0.0328\n",
      "epoch 94 / 100, step 16/80, loss = 0.0332\n",
      "epoch 94 / 100, step 17/80, loss = 0.0293\n",
      "epoch 94 / 100, step 18/80, loss = 0.0327\n",
      "epoch 94 / 100, step 19/80, loss = 0.0273\n",
      "epoch 94 / 100, step 20/80, loss = 0.0296\n",
      "epoch 94 / 100, step 21/80, loss = 0.0294\n",
      "epoch 94 / 100, step 22/80, loss = 0.0293\n",
      "epoch 94 / 100, step 23/80, loss = 0.0318\n",
      "epoch 94 / 100, step 24/80, loss = 0.0312\n",
      "epoch 94 / 100, step 25/80, loss = 0.0353\n",
      "epoch 94 / 100, step 26/80, loss = 0.0302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94 / 100, step 27/80, loss = 0.0356\n",
      "epoch 94 / 100, step 28/80, loss = 0.0306\n",
      "epoch 94 / 100, step 29/80, loss = 0.0309\n",
      "epoch 94 / 100, step 30/80, loss = 0.0320\n",
      "epoch 94 / 100, step 31/80, loss = 0.0367\n",
      "epoch 94 / 100, step 32/80, loss = 0.0282\n",
      "epoch 94 / 100, step 33/80, loss = 0.0283\n",
      "epoch 94 / 100, step 34/80, loss = 0.0280\n",
      "epoch 94 / 100, step 35/80, loss = 0.0324\n",
      "epoch 94 / 100, step 36/80, loss = 0.0291\n",
      "epoch 94 / 100, step 37/80, loss = 0.0298\n",
      "epoch 94 / 100, step 38/80, loss = 0.0328\n",
      "epoch 94 / 100, step 39/80, loss = 0.0322\n",
      "epoch 94 / 100, step 40/80, loss = 0.0296\n",
      "epoch 94 / 100, step 41/80, loss = 0.0299\n",
      "epoch 94 / 100, step 42/80, loss = 0.0280\n",
      "epoch 94 / 100, step 43/80, loss = 0.0282\n",
      "epoch 94 / 100, step 44/80, loss = 0.0280\n",
      "epoch 94 / 100, step 45/80, loss = 0.0265\n",
      "epoch 94 / 100, step 46/80, loss = 0.0308\n",
      "epoch 94 / 100, step 47/80, loss = 0.0288\n",
      "epoch 94 / 100, step 48/80, loss = 0.0322\n",
      "epoch 94 / 100, step 49/80, loss = 0.0325\n",
      "epoch 94 / 100, step 50/80, loss = 0.0288\n",
      "epoch 94 / 100, step 51/80, loss = 0.0308\n",
      "epoch 94 / 100, step 52/80, loss = 0.0321\n",
      "epoch 94 / 100, step 53/80, loss = 0.0296\n",
      "epoch 94 / 100, step 54/80, loss = 0.0322\n",
      "epoch 94 / 100, step 55/80, loss = 0.0314\n",
      "epoch 94 / 100, step 56/80, loss = 0.0284\n",
      "epoch 94 / 100, step 57/80, loss = 0.0276\n",
      "epoch 94 / 100, step 58/80, loss = 0.0311\n",
      "epoch 94 / 100, step 59/80, loss = 0.0299\n",
      "epoch 94 / 100, step 60/80, loss = 0.0331\n",
      "epoch 94 / 100, step 61/80, loss = 0.0291\n",
      "epoch 94 / 100, step 62/80, loss = 0.0276\n",
      "epoch 94 / 100, step 63/80, loss = 0.0275\n",
      "epoch 94 / 100, step 64/80, loss = 0.0289\n",
      "epoch 94 / 100, step 65/80, loss = 0.0299\n",
      "epoch 94 / 100, step 66/80, loss = 0.0294\n",
      "epoch 94 / 100, step 67/80, loss = 0.0294\n",
      "epoch 94 / 100, step 68/80, loss = 0.0310\n",
      "epoch 94 / 100, step 69/80, loss = 0.0305\n",
      "epoch 94 / 100, step 70/80, loss = 0.0326\n",
      "epoch 94 / 100, step 71/80, loss = 0.0301\n",
      "epoch 94 / 100, step 72/80, loss = 0.0310\n",
      "epoch 94 / 100, step 73/80, loss = 0.0350\n",
      "epoch 94 / 100, step 74/80, loss = 0.0368\n",
      "epoch 94 / 100, step 75/80, loss = 0.0374\n",
      "epoch 94 / 100, step 76/80, loss = 0.0296\n",
      "epoch 94 / 100, step 77/80, loss = 0.0300\n",
      "epoch 94 / 100, step 78/80, loss = 0.0355\n",
      "epoch 94 / 100, step 79/80, loss = 0.0372\n",
      "epoch 94 / 100, step 80/80, loss = 0.0301\n",
      "epoch 95 / 100, step 1/80, loss = 0.0274\n",
      "epoch 95 / 100, step 2/80, loss = 0.0292\n",
      "epoch 95 / 100, step 3/80, loss = 0.0286\n",
      "epoch 95 / 100, step 4/80, loss = 0.0313\n",
      "epoch 95 / 100, step 5/80, loss = 0.0280\n",
      "epoch 95 / 100, step 6/80, loss = 0.0290\n",
      "epoch 95 / 100, step 7/80, loss = 0.0275\n",
      "epoch 95 / 100, step 8/80, loss = 0.0283\n",
      "epoch 95 / 100, step 9/80, loss = 0.0280\n",
      "epoch 95 / 100, step 10/80, loss = 0.0300\n",
      "epoch 95 / 100, step 11/80, loss = 0.0314\n",
      "epoch 95 / 100, step 12/80, loss = 0.0307\n",
      "epoch 95 / 100, step 13/80, loss = 0.0261\n",
      "epoch 95 / 100, step 14/80, loss = 0.0307\n",
      "epoch 95 / 100, step 15/80, loss = 0.0264\n",
      "epoch 95 / 100, step 16/80, loss = 0.0279\n",
      "epoch 95 / 100, step 17/80, loss = 0.0291\n",
      "epoch 95 / 100, step 18/80, loss = 0.0290\n",
      "epoch 95 / 100, step 19/80, loss = 0.0271\n",
      "epoch 95 / 100, step 20/80, loss = 0.0290\n",
      "epoch 95 / 100, step 21/80, loss = 0.0313\n",
      "epoch 95 / 100, step 22/80, loss = 0.0287\n",
      "epoch 95 / 100, step 23/80, loss = 0.0271\n",
      "epoch 95 / 100, step 24/80, loss = 0.0257\n",
      "epoch 95 / 100, step 25/80, loss = 0.0343\n",
      "epoch 95 / 100, step 26/80, loss = 0.0295\n",
      "epoch 95 / 100, step 27/80, loss = 0.0286\n",
      "epoch 95 / 100, step 28/80, loss = 0.0304\n",
      "epoch 95 / 100, step 29/80, loss = 0.0324\n",
      "epoch 95 / 100, step 30/80, loss = 0.0288\n",
      "epoch 95 / 100, step 31/80, loss = 0.0302\n",
      "epoch 95 / 100, step 32/80, loss = 0.0290\n",
      "epoch 95 / 100, step 33/80, loss = 0.0354\n",
      "epoch 95 / 100, step 34/80, loss = 0.0273\n",
      "epoch 95 / 100, step 35/80, loss = 0.0280\n",
      "epoch 95 / 100, step 36/80, loss = 0.0290\n",
      "epoch 95 / 100, step 37/80, loss = 0.0305\n",
      "epoch 95 / 100, step 38/80, loss = 0.0309\n",
      "epoch 95 / 100, step 39/80, loss = 0.0285\n",
      "epoch 95 / 100, step 40/80, loss = 0.0273\n",
      "epoch 95 / 100, step 41/80, loss = 0.0287\n",
      "epoch 95 / 100, step 42/80, loss = 0.0262\n",
      "epoch 95 / 100, step 43/80, loss = 0.0280\n",
      "epoch 95 / 100, step 44/80, loss = 0.0289\n",
      "epoch 95 / 100, step 45/80, loss = 0.0265\n",
      "epoch 95 / 100, step 46/80, loss = 0.0287\n",
      "epoch 95 / 100, step 47/80, loss = 0.0291\n",
      "epoch 95 / 100, step 48/80, loss = 0.0375\n",
      "epoch 95 / 100, step 49/80, loss = 0.0299\n",
      "epoch 95 / 100, step 50/80, loss = 0.0255\n",
      "epoch 95 / 100, step 51/80, loss = 0.0276\n",
      "epoch 95 / 100, step 52/80, loss = 0.0253\n",
      "epoch 95 / 100, step 53/80, loss = 0.0267\n",
      "epoch 95 / 100, step 54/80, loss = 0.0274\n",
      "epoch 95 / 100, step 55/80, loss = 0.0322\n",
      "epoch 95 / 100, step 56/80, loss = 0.0280\n",
      "epoch 95 / 100, step 57/80, loss = 0.0315\n",
      "epoch 95 / 100, step 58/80, loss = 0.0317\n",
      "epoch 95 / 100, step 59/80, loss = 0.0342\n",
      "epoch 95 / 100, step 60/80, loss = 0.0306\n",
      "epoch 95 / 100, step 61/80, loss = 0.0310\n",
      "epoch 95 / 100, step 62/80, loss = 0.0317\n",
      "epoch 95 / 100, step 63/80, loss = 0.0280\n",
      "epoch 95 / 100, step 64/80, loss = 0.0325\n",
      "epoch 95 / 100, step 65/80, loss = 0.0299\n",
      "epoch 95 / 100, step 66/80, loss = 0.0282\n",
      "epoch 95 / 100, step 67/80, loss = 0.0304\n",
      "epoch 95 / 100, step 68/80, loss = 0.0339\n",
      "epoch 95 / 100, step 69/80, loss = 0.0295\n",
      "epoch 95 / 100, step 70/80, loss = 0.0281\n",
      "epoch 95 / 100, step 71/80, loss = 0.0339\n",
      "epoch 95 / 100, step 72/80, loss = 0.0301\n",
      "epoch 95 / 100, step 73/80, loss = 0.0267\n",
      "epoch 95 / 100, step 74/80, loss = 0.0280\n",
      "epoch 95 / 100, step 75/80, loss = 0.0322\n",
      "epoch 95 / 100, step 76/80, loss = 0.0327\n",
      "epoch 95 / 100, step 77/80, loss = 0.0319\n",
      "epoch 95 / 100, step 78/80, loss = 0.0334\n",
      "epoch 95 / 100, step 79/80, loss = 0.0290\n",
      "epoch 95 / 100, step 80/80, loss = 0.0280\n",
      "epoch 96 / 100, step 1/80, loss = 0.0279\n",
      "epoch 96 / 100, step 2/80, loss = 0.0290\n",
      "epoch 96 / 100, step 3/80, loss = 0.0289\n",
      "epoch 96 / 100, step 4/80, loss = 0.0269\n",
      "epoch 96 / 100, step 5/80, loss = 0.0304\n",
      "epoch 96 / 100, step 6/80, loss = 0.0350\n",
      "epoch 96 / 100, step 7/80, loss = 0.0285\n",
      "epoch 96 / 100, step 8/80, loss = 0.0281\n",
      "epoch 96 / 100, step 9/80, loss = 0.0294\n",
      "epoch 96 / 100, step 10/80, loss = 0.0287\n",
      "epoch 96 / 100, step 11/80, loss = 0.0282\n",
      "epoch 96 / 100, step 12/80, loss = 0.0323\n",
      "epoch 96 / 100, step 13/80, loss = 0.0257\n",
      "epoch 96 / 100, step 14/80, loss = 0.0287\n",
      "epoch 96 / 100, step 15/80, loss = 0.0259\n",
      "epoch 96 / 100, step 16/80, loss = 0.0272\n",
      "epoch 96 / 100, step 17/80, loss = 0.0282\n",
      "epoch 96 / 100, step 18/80, loss = 0.0296\n",
      "epoch 96 / 100, step 19/80, loss = 0.0287\n",
      "epoch 96 / 100, step 20/80, loss = 0.0271\n",
      "epoch 96 / 100, step 21/80, loss = 0.0264\n",
      "epoch 96 / 100, step 22/80, loss = 0.0276\n",
      "epoch 96 / 100, step 23/80, loss = 0.0309\n",
      "epoch 96 / 100, step 24/80, loss = 0.0293\n",
      "epoch 96 / 100, step 25/80, loss = 0.0293\n",
      "epoch 96 / 100, step 26/80, loss = 0.0271\n",
      "epoch 96 / 100, step 27/80, loss = 0.0267\n",
      "epoch 96 / 100, step 28/80, loss = 0.0297\n",
      "epoch 96 / 100, step 29/80, loss = 0.0320\n",
      "epoch 96 / 100, step 30/80, loss = 0.0304\n",
      "epoch 96 / 100, step 31/80, loss = 0.0317\n",
      "epoch 96 / 100, step 32/80, loss = 0.0303\n",
      "epoch 96 / 100, step 33/80, loss = 0.0278\n",
      "epoch 96 / 100, step 34/80, loss = 0.0286\n",
      "epoch 96 / 100, step 35/80, loss = 0.0292\n",
      "epoch 96 / 100, step 36/80, loss = 0.0339\n",
      "epoch 96 / 100, step 37/80, loss = 0.0277\n",
      "epoch 96 / 100, step 38/80, loss = 0.0284\n",
      "epoch 96 / 100, step 39/80, loss = 0.0323\n",
      "epoch 96 / 100, step 40/80, loss = 0.0291\n",
      "epoch 96 / 100, step 41/80, loss = 0.0276\n",
      "epoch 96 / 100, step 42/80, loss = 0.0284\n",
      "epoch 96 / 100, step 43/80, loss = 0.0274\n",
      "epoch 96 / 100, step 44/80, loss = 0.0286\n",
      "epoch 96 / 100, step 45/80, loss = 0.0309\n",
      "epoch 96 / 100, step 46/80, loss = 0.0293\n",
      "epoch 96 / 100, step 47/80, loss = 0.0296\n",
      "epoch 96 / 100, step 48/80, loss = 0.0326\n",
      "epoch 96 / 100, step 49/80, loss = 0.0288\n",
      "epoch 96 / 100, step 50/80, loss = 0.0289\n",
      "epoch 96 / 100, step 51/80, loss = 0.0287\n",
      "epoch 96 / 100, step 52/80, loss = 0.0278\n",
      "epoch 96 / 100, step 53/80, loss = 0.0288\n",
      "epoch 96 / 100, step 54/80, loss = 0.0260\n",
      "epoch 96 / 100, step 55/80, loss = 0.0313\n",
      "epoch 96 / 100, step 56/80, loss = 0.0320\n",
      "epoch 96 / 100, step 57/80, loss = 0.0250\n",
      "epoch 96 / 100, step 58/80, loss = 0.0391\n",
      "epoch 96 / 100, step 59/80, loss = 0.0317\n",
      "epoch 96 / 100, step 60/80, loss = 0.0267\n",
      "epoch 96 / 100, step 61/80, loss = 0.0283\n",
      "epoch 96 / 100, step 62/80, loss = 0.0310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 / 100, step 63/80, loss = 0.0298\n",
      "epoch 96 / 100, step 64/80, loss = 0.0316\n",
      "epoch 96 / 100, step 65/80, loss = 0.0276\n",
      "epoch 96 / 100, step 66/80, loss = 0.0270\n",
      "epoch 96 / 100, step 67/80, loss = 0.0286\n",
      "epoch 96 / 100, step 68/80, loss = 0.0268\n",
      "epoch 96 / 100, step 69/80, loss = 0.0314\n",
      "epoch 96 / 100, step 70/80, loss = 0.0305\n",
      "epoch 96 / 100, step 71/80, loss = 0.0272\n",
      "epoch 96 / 100, step 72/80, loss = 0.0276\n",
      "epoch 96 / 100, step 73/80, loss = 0.0320\n",
      "epoch 96 / 100, step 74/80, loss = 0.0262\n",
      "epoch 96 / 100, step 75/80, loss = 0.0285\n",
      "epoch 96 / 100, step 76/80, loss = 0.0275\n",
      "epoch 96 / 100, step 77/80, loss = 0.0300\n",
      "epoch 96 / 100, step 78/80, loss = 0.0354\n",
      "epoch 96 / 100, step 79/80, loss = 0.0344\n",
      "epoch 96 / 100, step 80/80, loss = 0.0262\n",
      "epoch 97 / 100, step 1/80, loss = 0.0274\n",
      "epoch 97 / 100, step 2/80, loss = 0.0283\n",
      "epoch 97 / 100, step 3/80, loss = 0.0264\n",
      "epoch 97 / 100, step 4/80, loss = 0.0295\n",
      "epoch 97 / 100, step 5/80, loss = 0.0301\n",
      "epoch 97 / 100, step 6/80, loss = 0.0330\n",
      "epoch 97 / 100, step 7/80, loss = 0.0276\n",
      "epoch 97 / 100, step 8/80, loss = 0.0253\n",
      "epoch 97 / 100, step 9/80, loss = 0.0303\n",
      "epoch 97 / 100, step 10/80, loss = 0.0271\n",
      "epoch 97 / 100, step 11/80, loss = 0.0273\n",
      "epoch 97 / 100, step 12/80, loss = 0.0262\n",
      "epoch 97 / 100, step 13/80, loss = 0.0254\n",
      "epoch 97 / 100, step 14/80, loss = 0.0258\n",
      "epoch 97 / 100, step 15/80, loss = 0.0295\n",
      "epoch 97 / 100, step 16/80, loss = 0.0267\n",
      "epoch 97 / 100, step 17/80, loss = 0.0246\n",
      "epoch 97 / 100, step 18/80, loss = 0.0300\n",
      "epoch 97 / 100, step 19/80, loss = 0.0280\n",
      "epoch 97 / 100, step 20/80, loss = 0.0275\n",
      "epoch 97 / 100, step 21/80, loss = 0.0313\n",
      "epoch 97 / 100, step 22/80, loss = 0.0259\n",
      "epoch 97 / 100, step 23/80, loss = 0.0292\n",
      "epoch 97 / 100, step 24/80, loss = 0.0273\n",
      "epoch 97 / 100, step 25/80, loss = 0.0261\n",
      "epoch 97 / 100, step 26/80, loss = 0.0309\n",
      "epoch 97 / 100, step 27/80, loss = 0.0303\n",
      "epoch 97 / 100, step 28/80, loss = 0.0283\n",
      "epoch 97 / 100, step 29/80, loss = 0.0268\n",
      "epoch 97 / 100, step 30/80, loss = 0.0306\n",
      "epoch 97 / 100, step 31/80, loss = 0.0279\n",
      "epoch 97 / 100, step 32/80, loss = 0.0283\n",
      "epoch 97 / 100, step 33/80, loss = 0.0280\n",
      "epoch 97 / 100, step 34/80, loss = 0.0292\n",
      "epoch 97 / 100, step 35/80, loss = 0.0250\n",
      "epoch 97 / 100, step 36/80, loss = 0.0254\n",
      "epoch 97 / 100, step 37/80, loss = 0.0269\n",
      "epoch 97 / 100, step 38/80, loss = 0.0273\n",
      "epoch 97 / 100, step 39/80, loss = 0.0285\n",
      "epoch 97 / 100, step 40/80, loss = 0.0273\n",
      "epoch 97 / 100, step 41/80, loss = 0.0257\n",
      "epoch 97 / 100, step 42/80, loss = 0.0278\n",
      "epoch 97 / 100, step 43/80, loss = 0.0289\n",
      "epoch 97 / 100, step 44/80, loss = 0.0279\n",
      "epoch 97 / 100, step 45/80, loss = 0.0275\n",
      "epoch 97 / 100, step 46/80, loss = 0.0278\n",
      "epoch 97 / 100, step 47/80, loss = 0.0275\n",
      "epoch 97 / 100, step 48/80, loss = 0.0291\n",
      "epoch 97 / 100, step 49/80, loss = 0.0305\n",
      "epoch 97 / 100, step 50/80, loss = 0.0261\n",
      "epoch 97 / 100, step 51/80, loss = 0.0279\n",
      "epoch 97 / 100, step 52/80, loss = 0.0317\n",
      "epoch 97 / 100, step 53/80, loss = 0.0260\n",
      "epoch 97 / 100, step 54/80, loss = 0.0307\n",
      "epoch 97 / 100, step 55/80, loss = 0.0289\n",
      "epoch 97 / 100, step 56/80, loss = 0.0285\n",
      "epoch 97 / 100, step 57/80, loss = 0.0294\n",
      "epoch 97 / 100, step 58/80, loss = 0.0273\n",
      "epoch 97 / 100, step 59/80, loss = 0.0288\n",
      "epoch 97 / 100, step 60/80, loss = 0.0289\n",
      "epoch 97 / 100, step 61/80, loss = 0.0271\n",
      "epoch 97 / 100, step 62/80, loss = 0.0300\n",
      "epoch 97 / 100, step 63/80, loss = 0.0275\n",
      "epoch 97 / 100, step 64/80, loss = 0.0290\n",
      "epoch 97 / 100, step 65/80, loss = 0.0266\n",
      "epoch 97 / 100, step 66/80, loss = 0.0271\n",
      "epoch 97 / 100, step 67/80, loss = 0.0287\n",
      "epoch 97 / 100, step 68/80, loss = 0.0309\n",
      "epoch 97 / 100, step 69/80, loss = 0.0270\n",
      "epoch 97 / 100, step 70/80, loss = 0.0296\n",
      "epoch 97 / 100, step 71/80, loss = 0.0297\n",
      "epoch 97 / 100, step 72/80, loss = 0.0280\n",
      "epoch 97 / 100, step 73/80, loss = 0.0258\n",
      "epoch 97 / 100, step 74/80, loss = 0.0292\n",
      "epoch 97 / 100, step 75/80, loss = 0.0265\n",
      "epoch 97 / 100, step 76/80, loss = 0.0307\n",
      "epoch 97 / 100, step 77/80, loss = 0.0306\n",
      "epoch 97 / 100, step 78/80, loss = 0.0270\n",
      "epoch 97 / 100, step 79/80, loss = 0.0273\n",
      "epoch 97 / 100, step 80/80, loss = 0.0309\n",
      "epoch 98 / 100, step 1/80, loss = 0.0262\n",
      "epoch 98 / 100, step 2/80, loss = 0.0300\n",
      "epoch 98 / 100, step 3/80, loss = 0.0274\n",
      "epoch 98 / 100, step 4/80, loss = 0.0274\n",
      "epoch 98 / 100, step 5/80, loss = 0.0236\n",
      "epoch 98 / 100, step 6/80, loss = 0.0295\n",
      "epoch 98 / 100, step 7/80, loss = 0.0287\n",
      "epoch 98 / 100, step 8/80, loss = 0.0290\n",
      "epoch 98 / 100, step 9/80, loss = 0.0280\n",
      "epoch 98 / 100, step 10/80, loss = 0.0328\n",
      "epoch 98 / 100, step 11/80, loss = 0.0277\n",
      "epoch 98 / 100, step 12/80, loss = 0.0280\n",
      "epoch 98 / 100, step 13/80, loss = 0.0266\n",
      "epoch 98 / 100, step 14/80, loss = 0.0279\n",
      "epoch 98 / 100, step 15/80, loss = 0.0346\n",
      "epoch 98 / 100, step 16/80, loss = 0.0259\n",
      "epoch 98 / 100, step 17/80, loss = 0.0259\n",
      "epoch 98 / 100, step 18/80, loss = 0.0266\n",
      "epoch 98 / 100, step 19/80, loss = 0.0276\n",
      "epoch 98 / 100, step 20/80, loss = 0.0270\n",
      "epoch 98 / 100, step 21/80, loss = 0.0298\n",
      "epoch 98 / 100, step 22/80, loss = 0.0269\n",
      "epoch 98 / 100, step 23/80, loss = 0.0276\n",
      "epoch 98 / 100, step 24/80, loss = 0.0278\n",
      "epoch 98 / 100, step 25/80, loss = 0.0264\n",
      "epoch 98 / 100, step 26/80, loss = 0.0279\n",
      "epoch 98 / 100, step 27/80, loss = 0.0310\n",
      "epoch 98 / 100, step 28/80, loss = 0.0275\n",
      "epoch 98 / 100, step 29/80, loss = 0.0262\n",
      "epoch 98 / 100, step 30/80, loss = 0.0295\n",
      "epoch 98 / 100, step 31/80, loss = 0.0281\n",
      "epoch 98 / 100, step 32/80, loss = 0.0272\n",
      "epoch 98 / 100, step 33/80, loss = 0.0283\n",
      "epoch 98 / 100, step 34/80, loss = 0.0279\n",
      "epoch 98 / 100, step 35/80, loss = 0.0307\n",
      "epoch 98 / 100, step 36/80, loss = 0.0250\n",
      "epoch 98 / 100, step 37/80, loss = 0.0265\n",
      "epoch 98 / 100, step 38/80, loss = 0.0278\n",
      "epoch 98 / 100, step 39/80, loss = 0.0348\n",
      "epoch 98 / 100, step 40/80, loss = 0.0347\n",
      "epoch 98 / 100, step 41/80, loss = 0.0320\n",
      "epoch 98 / 100, step 42/80, loss = 0.0297\n",
      "epoch 98 / 100, step 43/80, loss = 0.0301\n",
      "epoch 98 / 100, step 44/80, loss = 0.0294\n",
      "epoch 98 / 100, step 45/80, loss = 0.0321\n",
      "epoch 98 / 100, step 46/80, loss = 0.0336\n",
      "epoch 98 / 100, step 47/80, loss = 0.0285\n",
      "epoch 98 / 100, step 48/80, loss = 0.0264\n",
      "epoch 98 / 100, step 49/80, loss = 0.0263\n",
      "epoch 98 / 100, step 50/80, loss = 0.0352\n",
      "epoch 98 / 100, step 51/80, loss = 0.0280\n",
      "epoch 98 / 100, step 52/80, loss = 0.0316\n",
      "epoch 98 / 100, step 53/80, loss = 0.0274\n",
      "epoch 98 / 100, step 54/80, loss = 0.0272\n",
      "epoch 98 / 100, step 55/80, loss = 0.0273\n",
      "epoch 98 / 100, step 56/80, loss = 0.0293\n",
      "epoch 98 / 100, step 57/80, loss = 0.0312\n",
      "epoch 98 / 100, step 58/80, loss = 0.0287\n",
      "epoch 98 / 100, step 59/80, loss = 0.0261\n",
      "epoch 98 / 100, step 60/80, loss = 0.0312\n",
      "epoch 98 / 100, step 61/80, loss = 0.0359\n",
      "epoch 98 / 100, step 62/80, loss = 0.0358\n",
      "epoch 98 / 100, step 63/80, loss = 0.0285\n",
      "epoch 98 / 100, step 64/80, loss = 0.0268\n",
      "epoch 98 / 100, step 65/80, loss = 0.0348\n",
      "epoch 98 / 100, step 66/80, loss = 0.0306\n",
      "epoch 98 / 100, step 67/80, loss = 0.0296\n",
      "epoch 98 / 100, step 68/80, loss = 0.0263\n",
      "epoch 98 / 100, step 69/80, loss = 0.0282\n",
      "epoch 98 / 100, step 70/80, loss = 0.0304\n",
      "epoch 98 / 100, step 71/80, loss = 0.0310\n",
      "epoch 98 / 100, step 72/80, loss = 0.0280\n",
      "epoch 98 / 100, step 73/80, loss = 0.0283\n",
      "epoch 98 / 100, step 74/80, loss = 0.0286\n",
      "epoch 98 / 100, step 75/80, loss = 0.0310\n",
      "epoch 98 / 100, step 76/80, loss = 0.0303\n",
      "epoch 98 / 100, step 77/80, loss = 0.0294\n",
      "epoch 98 / 100, step 78/80, loss = 0.0305\n",
      "epoch 98 / 100, step 79/80, loss = 0.0285\n",
      "epoch 98 / 100, step 80/80, loss = 0.0306\n",
      "epoch 99 / 100, step 1/80, loss = 0.0260\n",
      "epoch 99 / 100, step 2/80, loss = 0.0259\n",
      "epoch 99 / 100, step 3/80, loss = 0.0282\n",
      "epoch 99 / 100, step 4/80, loss = 0.0249\n",
      "epoch 99 / 100, step 5/80, loss = 0.0272\n",
      "epoch 99 / 100, step 6/80, loss = 0.0258\n",
      "epoch 99 / 100, step 7/80, loss = 0.0396\n",
      "epoch 99 / 100, step 8/80, loss = 0.0268\n",
      "epoch 99 / 100, step 9/80, loss = 0.0385\n",
      "epoch 99 / 100, step 10/80, loss = 0.0275\n",
      "epoch 99 / 100, step 11/80, loss = 0.0295\n",
      "epoch 99 / 100, step 12/80, loss = 0.0318\n",
      "epoch 99 / 100, step 13/80, loss = 0.0281\n",
      "epoch 99 / 100, step 14/80, loss = 0.0303\n",
      "epoch 99 / 100, step 15/80, loss = 0.0288\n",
      "epoch 99 / 100, step 16/80, loss = 0.0289\n",
      "epoch 99 / 100, step 17/80, loss = 0.0289\n",
      "epoch 99 / 100, step 18/80, loss = 0.0276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99 / 100, step 19/80, loss = 0.0289\n",
      "epoch 99 / 100, step 20/80, loss = 0.0268\n",
      "epoch 99 / 100, step 21/80, loss = 0.0278\n",
      "epoch 99 / 100, step 22/80, loss = 0.0302\n",
      "epoch 99 / 100, step 23/80, loss = 0.0287\n",
      "epoch 99 / 100, step 24/80, loss = 0.0289\n",
      "epoch 99 / 100, step 25/80, loss = 0.0249\n",
      "epoch 99 / 100, step 26/80, loss = 0.0259\n",
      "epoch 99 / 100, step 27/80, loss = 0.0331\n",
      "epoch 99 / 100, step 28/80, loss = 0.0295\n",
      "epoch 99 / 100, step 29/80, loss = 0.0300\n",
      "epoch 99 / 100, step 30/80, loss = 0.0339\n",
      "epoch 99 / 100, step 31/80, loss = 0.0309\n",
      "epoch 99 / 100, step 32/80, loss = 0.0274\n",
      "epoch 99 / 100, step 33/80, loss = 0.0302\n",
      "epoch 99 / 100, step 34/80, loss = 0.0277\n",
      "epoch 99 / 100, step 35/80, loss = 0.0296\n",
      "epoch 99 / 100, step 36/80, loss = 0.0266\n",
      "epoch 99 / 100, step 37/80, loss = 0.0257\n",
      "epoch 99 / 100, step 38/80, loss = 0.0298\n",
      "epoch 99 / 100, step 39/80, loss = 0.0253\n",
      "epoch 99 / 100, step 40/80, loss = 0.0310\n",
      "epoch 99 / 100, step 41/80, loss = 0.0293\n",
      "epoch 99 / 100, step 42/80, loss = 0.0282\n",
      "epoch 99 / 100, step 43/80, loss = 0.0262\n",
      "epoch 99 / 100, step 44/80, loss = 0.0315\n",
      "epoch 99 / 100, step 45/80, loss = 0.0298\n",
      "epoch 99 / 100, step 46/80, loss = 0.0284\n",
      "epoch 99 / 100, step 47/80, loss = 0.0243\n",
      "epoch 99 / 100, step 48/80, loss = 0.0278\n",
      "epoch 99 / 100, step 49/80, loss = 0.0293\n",
      "epoch 99 / 100, step 50/80, loss = 0.0268\n",
      "epoch 99 / 100, step 51/80, loss = 0.0317\n",
      "epoch 99 / 100, step 52/80, loss = 0.0277\n",
      "epoch 99 / 100, step 53/80, loss = 0.0305\n",
      "epoch 99 / 100, step 54/80, loss = 0.0318\n",
      "epoch 99 / 100, step 55/80, loss = 0.0294\n",
      "epoch 99 / 100, step 56/80, loss = 0.0280\n",
      "epoch 99 / 100, step 57/80, loss = 0.0295\n",
      "epoch 99 / 100, step 58/80, loss = 0.0277\n",
      "epoch 99 / 100, step 59/80, loss = 0.0287\n",
      "epoch 99 / 100, step 60/80, loss = 0.0266\n",
      "epoch 99 / 100, step 61/80, loss = 0.0316\n",
      "epoch 99 / 100, step 62/80, loss = 0.0301\n",
      "epoch 99 / 100, step 63/80, loss = 0.0315\n",
      "epoch 99 / 100, step 64/80, loss = 0.0283\n",
      "epoch 99 / 100, step 65/80, loss = 0.0305\n",
      "epoch 99 / 100, step 66/80, loss = 0.0265\n",
      "epoch 99 / 100, step 67/80, loss = 0.0346\n",
      "epoch 99 / 100, step 68/80, loss = 0.0275\n",
      "epoch 99 / 100, step 69/80, loss = 0.0362\n",
      "epoch 99 / 100, step 70/80, loss = 0.0276\n",
      "epoch 99 / 100, step 71/80, loss = 0.0298\n",
      "epoch 99 / 100, step 72/80, loss = 0.0317\n",
      "epoch 99 / 100, step 73/80, loss = 0.0269\n",
      "epoch 99 / 100, step 74/80, loss = 0.0285\n",
      "epoch 99 / 100, step 75/80, loss = 0.0304\n",
      "epoch 99 / 100, step 76/80, loss = 0.0283\n",
      "epoch 99 / 100, step 77/80, loss = 0.0306\n",
      "epoch 99 / 100, step 78/80, loss = 0.0274\n",
      "epoch 99 / 100, step 79/80, loss = 0.0293\n",
      "epoch 99 / 100, step 80/80, loss = 0.0353\n",
      "epoch 100 / 100, step 1/80, loss = 0.0269\n",
      "epoch 100 / 100, step 2/80, loss = 0.0270\n",
      "epoch 100 / 100, step 3/80, loss = 0.0297\n",
      "epoch 100 / 100, step 4/80, loss = 0.0304\n",
      "epoch 100 / 100, step 5/80, loss = 0.0261\n",
      "epoch 100 / 100, step 6/80, loss = 0.0297\n",
      "epoch 100 / 100, step 7/80, loss = 0.0311\n",
      "epoch 100 / 100, step 8/80, loss = 0.0260\n",
      "epoch 100 / 100, step 9/80, loss = 0.0293\n",
      "epoch 100 / 100, step 10/80, loss = 0.0292\n",
      "epoch 100 / 100, step 11/80, loss = 0.0287\n",
      "epoch 100 / 100, step 12/80, loss = 0.0295\n",
      "epoch 100 / 100, step 13/80, loss = 0.0283\n",
      "epoch 100 / 100, step 14/80, loss = 0.0277\n",
      "epoch 100 / 100, step 15/80, loss = 0.0263\n",
      "epoch 100 / 100, step 16/80, loss = 0.0281\n",
      "epoch 100 / 100, step 17/80, loss = 0.0288\n",
      "epoch 100 / 100, step 18/80, loss = 0.0288\n",
      "epoch 100 / 100, step 19/80, loss = 0.0293\n",
      "epoch 100 / 100, step 20/80, loss = 0.0270\n",
      "epoch 100 / 100, step 21/80, loss = 0.0270\n",
      "epoch 100 / 100, step 22/80, loss = 0.0281\n",
      "epoch 100 / 100, step 23/80, loss = 0.0279\n",
      "epoch 100 / 100, step 24/80, loss = 0.0257\n",
      "epoch 100 / 100, step 25/80, loss = 0.0303\n",
      "epoch 100 / 100, step 26/80, loss = 0.0286\n",
      "epoch 100 / 100, step 27/80, loss = 0.0274\n",
      "epoch 100 / 100, step 28/80, loss = 0.0270\n",
      "epoch 100 / 100, step 29/80, loss = 0.0328\n",
      "epoch 100 / 100, step 30/80, loss = 0.0291\n",
      "epoch 100 / 100, step 31/80, loss = 0.0299\n",
      "epoch 100 / 100, step 32/80, loss = 0.0267\n",
      "epoch 100 / 100, step 33/80, loss = 0.0280\n",
      "epoch 100 / 100, step 34/80, loss = 0.0261\n",
      "epoch 100 / 100, step 35/80, loss = 0.0275\n",
      "epoch 100 / 100, step 36/80, loss = 0.0298\n",
      "epoch 100 / 100, step 37/80, loss = 0.0276\n",
      "epoch 100 / 100, step 38/80, loss = 0.0289\n",
      "epoch 100 / 100, step 39/80, loss = 0.0336\n",
      "epoch 100 / 100, step 40/80, loss = 0.0285\n",
      "epoch 100 / 100, step 41/80, loss = 0.0353\n",
      "epoch 100 / 100, step 42/80, loss = 0.0284\n",
      "epoch 100 / 100, step 43/80, loss = 0.0292\n",
      "epoch 100 / 100, step 44/80, loss = 0.0263\n",
      "epoch 100 / 100, step 45/80, loss = 0.0305\n",
      "epoch 100 / 100, step 46/80, loss = 0.0244\n",
      "epoch 100 / 100, step 47/80, loss = 0.0303\n",
      "epoch 100 / 100, step 48/80, loss = 0.0276\n",
      "epoch 100 / 100, step 49/80, loss = 0.0236\n",
      "epoch 100 / 100, step 50/80, loss = 0.0298\n",
      "epoch 100 / 100, step 51/80, loss = 0.0260\n",
      "epoch 100 / 100, step 52/80, loss = 0.0275\n",
      "epoch 100 / 100, step 53/80, loss = 0.0337\n",
      "epoch 100 / 100, step 54/80, loss = 0.0264\n",
      "epoch 100 / 100, step 55/80, loss = 0.0323\n",
      "epoch 100 / 100, step 56/80, loss = 0.0298\n",
      "epoch 100 / 100, step 57/80, loss = 0.0320\n",
      "epoch 100 / 100, step 58/80, loss = 0.0280\n",
      "epoch 100 / 100, step 59/80, loss = 0.0265\n",
      "epoch 100 / 100, step 60/80, loss = 0.0278\n",
      "epoch 100 / 100, step 61/80, loss = 0.0282\n",
      "epoch 100 / 100, step 62/80, loss = 0.0337\n",
      "epoch 100 / 100, step 63/80, loss = 0.0310\n",
      "epoch 100 / 100, step 64/80, loss = 0.0267\n",
      "epoch 100 / 100, step 65/80, loss = 0.0277\n",
      "epoch 100 / 100, step 66/80, loss = 0.0263\n",
      "epoch 100 / 100, step 67/80, loss = 0.0280\n",
      "epoch 100 / 100, step 68/80, loss = 0.0302\n",
      "epoch 100 / 100, step 69/80, loss = 0.0293\n",
      "epoch 100 / 100, step 70/80, loss = 0.0271\n",
      "epoch 100 / 100, step 71/80, loss = 0.0274\n",
      "epoch 100 / 100, step 72/80, loss = 0.0285\n",
      "epoch 100 / 100, step 73/80, loss = 0.0257\n",
      "epoch 100 / 100, step 74/80, loss = 0.0306\n",
      "epoch 100 / 100, step 75/80, loss = 0.0308\n",
      "epoch 100 / 100, step 76/80, loss = 0.0295\n",
      "epoch 100 / 100, step 77/80, loss = 0.0278\n",
      "epoch 100 / 100, step 78/80, loss = 0.0313\n",
      "epoch 100 / 100, step 79/80, loss = 0.0289\n",
      "epoch 100 / 100, step 80/80, loss = 0.0288\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "criterion1 = nn.BCELoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "#         print(images.shape, labels.shape)\n",
    "        outputs = model(images)\n",
    "        loss1 = criterion1(outputs, labels)\n",
    "        loss2 = criterion2(outputs, labels)\n",
    "        loss = loss1 + loss2\n",
    "        # backward\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 1 ==0:\n",
    "            \n",
    "            print(f'epoch {epoch+1} / {n_epochs}, step {i+1}/{len(train_loader)}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9242a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ex = next(iter(test_set))\n",
    "train_ex = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68601cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_loaded = torch.load('checkpoint_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d45fdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(in_channels=2, out_channels = 1, features = checkpoint_loaded['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5edea9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint_loaded['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83302f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model(test_ex[0])\n",
    "tr = model(train_ex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f06ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d026e527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pr = pr.detach().apply_(lambda x: x>0.5).clone()\n",
    "tr = tr.detach().apply_(lambda x: x>0.5).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78eba0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f339af24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'True')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4000x4000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test \n",
    "plt.figure(figsize=(40,40))\n",
    "plt.subplot(4,2,1)\n",
    "plt.imshow(pr[num].view(40,40), cmap='binary')\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "plt.imshow(test_ex[1][num].view(40,40), cmap='binary')\n",
    "plt.title('True')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.imshow(pr[num+1].view(40,40), cmap='binary')\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "plt.imshow(test_ex[1][num+1].view(40,40), cmap='binary')\n",
    "plt.title('True')\n",
    "\n",
    "plt.subplot(4,2,5)\n",
    "plt.imshow(pr[num+2].view(40,40), cmap='binary')\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(4,2,6)\n",
    "plt.imshow(test_ex[1][num+2].view(40,40), cmap='binary')\n",
    "plt.title('True')\n",
    "\n",
    "plt.subplot(4,2,7)\n",
    "plt.imshow(pr[num+3].view(40,40), cmap='binary')\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(4,2,8)\n",
    "plt.imshow(test_ex[1][num+3].view(40,40), cmap='binary')\n",
    "plt.title('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee2e0847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'True')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4000x4000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4000x4000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "plt.figure(figsize=(40,40))\n",
    "plt.subplot(4,2,1)\n",
    "plt.imshow(tr[num].view(40,40), cmap='binary')\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "plt.imshow(train_ex[1][num].view(40,40), cmap='binary')\n",
    "plt.title('True')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.imshow(tr[num+1].view(40,40), cmap='binary')\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "plt.imshow(train_ex[1][num+1].view(40,40), cmap='binary')\n",
    "plt.title('True')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(40,40))\n",
    "plt.subplot(4,2,1)\n",
    "plt.imshow(tr[num+2].view(40,40), cmap='binary')\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "plt.imshow(train_ex[1][num+2].view(40,40), cmap='binary')\n",
    "plt.title('True')\n",
    "\n",
    "plt.subplot(4,2,3)\n",
    "plt.imshow(tr[num+3].view(40,40), cmap='binary')\n",
    "plt.title('Predicted')\n",
    "\n",
    "plt.subplot(4,2,4)\n",
    "plt.imshow(train_ex[1][num+3].view(40,40), cmap='binary')\n",
    "plt.title('True')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "614e554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"model_2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1542ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [16,32,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5ff95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31febd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"epoch\":100,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optim_state\": optimizer.state_dict(),\n",
    "    \"features\": features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8344faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(checkpoint, \"checkpoint_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ex[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "963f4fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('ups.0.weight',\n",
       "              tensor([[[[-0.0393,  0.0040],\n",
       "                        [ 0.0193,  0.0318]],\n",
       "              \n",
       "                       [[-0.0155, -0.0132],\n",
       "                        [-0.0328, -0.0550]],\n",
       "              \n",
       "                       [[ 0.0053, -0.0055],\n",
       "                        [-0.0275,  0.0153]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0402, -0.0462],\n",
       "                        [-0.0195, -0.0232]],\n",
       "              \n",
       "                       [[ 0.0281, -0.0281],\n",
       "                        [ 0.0158, -0.0295]],\n",
       "              \n",
       "                       [[ 0.0133, -0.0452],\n",
       "                        [-0.0280, -0.0294]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0207,  0.0302],\n",
       "                        [ 0.0211,  0.0052]],\n",
       "              \n",
       "                       [[-0.0162,  0.0448],\n",
       "                        [-0.0067, -0.0090]],\n",
       "              \n",
       "                       [[-0.0220, -0.0052],\n",
       "                        [ 0.0220,  0.0299]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0067, -0.0283],\n",
       "                        [-0.0070, -0.0265]],\n",
       "              \n",
       "                       [[ 0.0232,  0.0340],\n",
       "                        [ 0.0287, -0.0312]],\n",
       "              \n",
       "                       [[ 0.0617,  0.0459],\n",
       "                        [ 0.0080, -0.0437]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0234,  0.0248],\n",
       "                        [ 0.0151, -0.0241]],\n",
       "              \n",
       "                       [[ 0.0055, -0.0419],\n",
       "                        [ 0.0062, -0.0138]],\n",
       "              \n",
       "                       [[ 0.0090,  0.0004],\n",
       "                        [-0.0085, -0.0403]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0135,  0.0001],\n",
       "                        [ 0.0233,  0.0461]],\n",
       "              \n",
       "                       [[-0.0203,  0.0381],\n",
       "                        [ 0.0263,  0.0243]],\n",
       "              \n",
       "                       [[ 0.0100, -0.0580],\n",
       "                        [ 0.0098,  0.0275]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0273, -0.0155],\n",
       "                        [-0.0040, -0.0118]],\n",
       "              \n",
       "                       [[ 0.0264,  0.0463],\n",
       "                        [ 0.0230, -0.0389]],\n",
       "              \n",
       "                       [[ 0.0018, -0.0247],\n",
       "                        [-0.0200, -0.0229]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0068, -0.0113],\n",
       "                        [ 0.0181, -0.0013]],\n",
       "              \n",
       "                       [[ 0.0523, -0.0279],\n",
       "                        [-0.0023, -0.0242]],\n",
       "              \n",
       "                       [[-0.0208,  0.0050],\n",
       "                        [-0.0204,  0.0417]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0325, -0.0489],\n",
       "                        [-0.0460, -0.0160]],\n",
       "              \n",
       "                       [[ 0.0404,  0.0202],\n",
       "                        [-0.0119, -0.0449]],\n",
       "              \n",
       "                       [[ 0.0466,  0.0022],\n",
       "                        [-0.0212, -0.0316]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0052,  0.0292],\n",
       "                        [-0.0382,  0.0055]],\n",
       "              \n",
       "                       [[-0.0367,  0.0435],\n",
       "                        [-0.0181, -0.0357]],\n",
       "              \n",
       "                       [[ 0.0034,  0.0232],\n",
       "                        [-0.0102,  0.0307]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0226, -0.0134],\n",
       "                        [ 0.0030,  0.0136]],\n",
       "              \n",
       "                       [[ 0.0247,  0.0002],\n",
       "                        [ 0.0491,  0.0355]],\n",
       "              \n",
       "                       [[ 0.0427,  0.0055],\n",
       "                        [-0.0213,  0.0525]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0419,  0.0174],\n",
       "                        [-0.0142,  0.0243]],\n",
       "              \n",
       "                       [[ 0.0014,  0.0032],\n",
       "                        [-0.0343,  0.0335]],\n",
       "              \n",
       "                       [[-0.0082, -0.0122],\n",
       "                        [ 0.0136, -0.0081]]]])),\n",
       "             ('ups.0.bias',\n",
       "              tensor([ 2.0436e-02, -2.3901e-02,  1.9222e-02,  2.7759e-02,  4.0955e-02,\n",
       "                       8.0221e-03,  2.5672e-02, -2.0095e-02, -7.7745e-03, -2.5527e-03,\n",
       "                      -3.9290e-02, -4.4106e-02,  1.8787e-02, -2.1343e-02, -3.3799e-02,\n",
       "                      -2.5860e-02, -2.4257e-02, -7.5495e-03,  2.2875e-02,  2.0012e-03,\n",
       "                      -3.6892e-02, -3.7785e-02, -2.4746e-02,  2.8459e-02,  3.2102e-02,\n",
       "                      -1.2712e-02, -8.2576e-03,  1.1875e-02, -3.1206e-02,  1.7231e-02,\n",
       "                      -2.6898e-02, -1.9266e-02, -1.1648e-02, -8.7381e-03, -2.8238e-02,\n",
       "                       3.2781e-02, -3.5123e-02, -1.3818e-02,  5.7881e-03,  1.2456e-02,\n",
       "                      -3.9028e-02, -2.4603e-02,  3.4509e-02, -1.5345e-02,  3.1488e-02,\n",
       "                      -3.2798e-03,  1.6528e-02, -2.9058e-02,  9.0040e-04, -1.2934e-02,\n",
       "                      -2.0948e-02, -7.7597e-03, -3.6904e-02,  2.3858e-02,  1.9611e-02,\n",
       "                      -9.4908e-05,  2.9519e-02,  1.9159e-02, -3.6086e-02, -3.5961e-02,\n",
       "                       3.1174e-02, -8.6949e-03, -1.7666e-02, -3.1067e-02,  2.6409e-02,\n",
       "                      -2.0119e-02, -3.6995e-02, -1.2777e-02,  2.6541e-02,  2.1888e-02,\n",
       "                       3.6148e-02,  3.5722e-02,  2.3649e-02,  3.8363e-02, -4.4239e-02,\n",
       "                      -3.7813e-02, -8.2599e-03,  1.5946e-02, -1.2292e-02,  3.9926e-02,\n",
       "                      -3.4888e-03,  6.0037e-03,  3.5065e-02, -3.9832e-03,  2.8054e-02,\n",
       "                       2.1364e-02, -5.6458e-02, -1.9030e-03, -1.7505e-02, -4.8056e-02,\n",
       "                       2.4326e-02, -1.9505e-02,  3.8368e-02,  4.6201e-02, -2.1528e-02,\n",
       "                      -3.1901e-02,  1.4216e-02,  4.1169e-02, -3.3073e-02, -1.1823e-02,\n",
       "                      -3.4294e-02,  1.1099e-02,  2.3980e-02, -1.1468e-02,  1.8642e-02,\n",
       "                       8.2657e-03,  1.9453e-02,  3.6899e-02,  9.2657e-03, -1.0227e-02,\n",
       "                      -4.2606e-02, -1.3177e-02, -8.3617e-03,  3.2692e-02,  2.4610e-02,\n",
       "                       2.8772e-02, -1.3258e-02,  1.8296e-02, -2.8834e-02, -2.1166e-02,\n",
       "                       2.9442e-03,  3.3129e-02,  1.9447e-02, -2.3001e-02, -3.4571e-02,\n",
       "                      -7.6619e-03, -4.9106e-03,  1.0745e-02])),\n",
       "             ('ups.1.conv.0.weight',\n",
       "              tensor([[[[-0.0165,  0.0006,  0.0005],\n",
       "                        [-0.0161,  0.0181,  0.0226],\n",
       "                        [-0.0175, -0.0170,  0.0082]],\n",
       "              \n",
       "                       [[-0.0053, -0.0289,  0.0123],\n",
       "                        [-0.0125,  0.0158, -0.0111],\n",
       "                        [ 0.0165,  0.0105, -0.0239]],\n",
       "              \n",
       "                       [[ 0.0180, -0.0320,  0.0014],\n",
       "                        [-0.0224, -0.0209,  0.0107],\n",
       "                        [-0.0029,  0.0056, -0.0135]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0208,  0.0188,  0.0165],\n",
       "                        [-0.0182, -0.0192,  0.0128],\n",
       "                        [-0.0103, -0.0123,  0.0159]],\n",
       "              \n",
       "                       [[-0.0327, -0.0288, -0.0056],\n",
       "                        [ 0.0077, -0.0236, -0.0215],\n",
       "                        [ 0.0204,  0.0122,  0.0071]],\n",
       "              \n",
       "                       [[-0.0071,  0.0061,  0.0131],\n",
       "                        [ 0.0063, -0.0002, -0.0044],\n",
       "                        [-0.0180, -0.0224, -0.0218]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0117,  0.0156, -0.0072],\n",
       "                        [-0.0092,  0.0163, -0.0108],\n",
       "                        [-0.0043,  0.0212,  0.0276]],\n",
       "              \n",
       "                       [[ 0.0055,  0.0004,  0.0193],\n",
       "                        [ 0.0273,  0.0006, -0.0094],\n",
       "                        [-0.0075, -0.0191, -0.0024]],\n",
       "              \n",
       "                       [[-0.0064, -0.0241,  0.0153],\n",
       "                        [-0.0157, -0.0170,  0.0163],\n",
       "                        [ 0.0173, -0.0150, -0.0052]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0016, -0.0190, -0.0029],\n",
       "                        [ 0.0079, -0.0070,  0.0187],\n",
       "                        [-0.0178,  0.0034, -0.0066]],\n",
       "              \n",
       "                       [[-0.0106, -0.0092, -0.0126],\n",
       "                        [-0.0034,  0.0220,  0.0284],\n",
       "                        [ 0.0320,  0.0109, -0.0077]],\n",
       "              \n",
       "                       [[-0.0236, -0.0065,  0.0113],\n",
       "                        [ 0.0165, -0.0033,  0.0009],\n",
       "                        [-0.0203, -0.0293,  0.0166]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0035,  0.0082,  0.0043],\n",
       "                        [-0.0145,  0.0096, -0.0239],\n",
       "                        [ 0.0094,  0.0051,  0.0012]],\n",
       "              \n",
       "                       [[-0.0146,  0.0114, -0.0105],\n",
       "                        [ 0.0228,  0.0093, -0.0086],\n",
       "                        [ 0.0103, -0.0004,  0.0008]],\n",
       "              \n",
       "                       [[ 0.0169,  0.0180, -0.0168],\n",
       "                        [-0.0102,  0.0076,  0.0053],\n",
       "                        [-0.0154,  0.0041,  0.0081]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0101, -0.0136, -0.0013],\n",
       "                        [-0.0063, -0.0013, -0.0260],\n",
       "                        [ 0.0124,  0.0124,  0.0213]],\n",
       "              \n",
       "                       [[ 0.0012,  0.0060,  0.0103],\n",
       "                        [-0.0238,  0.0186, -0.0075],\n",
       "                        [ 0.0121,  0.0057,  0.0091]],\n",
       "              \n",
       "                       [[ 0.0197, -0.0028,  0.0029],\n",
       "                        [ 0.0340,  0.0289, -0.0153],\n",
       "                        [ 0.0251,  0.0155, -0.0063]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0218, -0.0191,  0.0037],\n",
       "                        [-0.0253,  0.0110, -0.0133],\n",
       "                        [ 0.0230, -0.0070, -0.0137]],\n",
       "              \n",
       "                       [[ 0.0203, -0.0091,  0.0182],\n",
       "                        [ 0.0020,  0.0101, -0.0138],\n",
       "                        [ 0.0268, -0.0123, -0.0190]],\n",
       "              \n",
       "                       [[-0.0073, -0.0159,  0.0265],\n",
       "                        [ 0.0129, -0.0157,  0.0137],\n",
       "                        [-0.0091,  0.0009,  0.0086]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0056, -0.0160, -0.0182],\n",
       "                        [-0.0116, -0.0046,  0.0046],\n",
       "                        [-0.0220, -0.0149, -0.0149]],\n",
       "              \n",
       "                       [[-0.0093,  0.0065,  0.0204],\n",
       "                        [ 0.0095,  0.0051, -0.0109],\n",
       "                        [ 0.0319,  0.0165,  0.0029]],\n",
       "              \n",
       "                       [[ 0.0128,  0.0303,  0.0149],\n",
       "                        [ 0.0147, -0.0215, -0.0215],\n",
       "                        [-0.0116, -0.0318, -0.0113]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0103, -0.0008, -0.0220],\n",
       "                        [ 0.0092,  0.0103, -0.0310],\n",
       "                        [ 0.0158,  0.0063,  0.0116]],\n",
       "              \n",
       "                       [[-0.0188, -0.0056, -0.0078],\n",
       "                        [-0.0244, -0.0096, -0.0137],\n",
       "                        [-0.0230, -0.0177, -0.0034]],\n",
       "              \n",
       "                       [[-0.0088, -0.0081, -0.0202],\n",
       "                        [ 0.0102,  0.0277, -0.0249],\n",
       "                        [ 0.0101, -0.0079, -0.0228]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0182, -0.0316, -0.0016],\n",
       "                        [ 0.0168, -0.0157, -0.0063],\n",
       "                        [-0.0122,  0.0062,  0.0190]],\n",
       "              \n",
       "                       [[-0.0027,  0.0149, -0.0145],\n",
       "                        [ 0.0237, -0.0143,  0.0237],\n",
       "                        [-0.0263, -0.0287, -0.0202]],\n",
       "              \n",
       "                       [[-0.0028,  0.0123, -0.0155],\n",
       "                        [-0.0295, -0.0150, -0.0048],\n",
       "                        [-0.0058,  0.0253,  0.0249]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0053,  0.0104,  0.0043],\n",
       "                        [-0.0191, -0.0173,  0.0247],\n",
       "                        [ 0.0160,  0.0069,  0.0007]],\n",
       "              \n",
       "                       [[ 0.0106,  0.0169, -0.0172],\n",
       "                        [ 0.0059,  0.0096, -0.0136],\n",
       "                        [ 0.0220, -0.0167, -0.0119]],\n",
       "              \n",
       "                       [[-0.0115, -0.0130,  0.0243],\n",
       "                        [-0.0099,  0.0024, -0.0008],\n",
       "                        [ 0.0131, -0.0009, -0.0126]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0071, -0.0151,  0.0106],\n",
       "                        [ 0.0061, -0.0108, -0.0069],\n",
       "                        [-0.0043,  0.0162, -0.0321]],\n",
       "              \n",
       "                       [[ 0.0004, -0.0038,  0.0062],\n",
       "                        [-0.0103, -0.0153, -0.0236],\n",
       "                        [-0.0020,  0.0059,  0.0315]],\n",
       "              \n",
       "                       [[-0.0131,  0.0168, -0.0062],\n",
       "                        [ 0.0006, -0.0112,  0.0290],\n",
       "                        [ 0.0170,  0.0147, -0.0300]]]])),\n",
       "             ('ups.1.conv.1.weight',\n",
       "              tensor([1.0003, 1.0004, 1.0031, 1.0040, 0.9859, 1.0029, 0.9956, 0.9946, 0.9841,\n",
       "                      1.0027, 0.9932, 1.0000, 1.0044, 0.9955, 1.0002, 0.9971, 0.9844, 1.0082,\n",
       "                      0.9962, 0.9895, 1.0012, 1.0074, 0.9968, 0.9968, 1.0099, 1.0099, 0.9858,\n",
       "                      0.9952, 0.9979, 0.9969, 0.9964, 1.0009, 0.9977, 0.9960, 1.0012, 1.0028,\n",
       "                      1.0023, 1.0034, 0.9984, 0.9897, 0.9908, 0.9925, 1.0028, 0.9908, 0.9985,\n",
       "                      0.9933, 0.9985, 1.0153, 1.0090, 0.9971, 0.9911, 0.9980, 1.0074, 1.0019,\n",
       "                      0.9980, 1.0091, 1.0058, 0.9874, 0.9876, 1.0153, 1.0053, 1.0068, 1.0025,\n",
       "                      1.0009, 1.0081, 0.9973, 1.0021, 1.0051, 0.9992, 1.0102, 0.9923, 1.0082,\n",
       "                      0.9971, 1.0006, 1.0111, 0.9984, 0.9900, 0.9978, 0.9880, 0.9954, 1.0167,\n",
       "                      0.9956, 1.0091, 0.9976, 1.0084, 0.9964, 0.9987, 0.9937, 0.9958, 1.0049,\n",
       "                      1.0072, 1.0038, 1.0098, 0.9976, 0.9950, 0.9926, 1.0119, 1.0018, 0.9922,\n",
       "                      0.9912, 1.0053, 0.9941, 1.0163, 1.0155, 1.0005, 0.9871, 0.9984, 1.0053,\n",
       "                      1.0197, 1.0127, 1.0156, 1.0082, 1.0032, 1.0054, 0.9927, 1.0075, 1.0056,\n",
       "                      1.0008, 0.9977, 1.0033, 0.9922, 1.0088, 1.0000, 0.9949, 1.0045, 0.9976,\n",
       "                      0.9997, 1.0076])),\n",
       "             ('ups.1.conv.1.bias',\n",
       "              tensor([-0.0367, -0.0380, -0.0239, -0.0169, -0.0504, -0.0267, -0.0312, -0.0233,\n",
       "                      -0.0505, -0.0352, -0.0385, -0.0296, -0.0238, -0.0470, -0.0292, -0.0339,\n",
       "                      -0.0471, -0.0287, -0.0282, -0.0360, -0.0354, -0.0359, -0.0263, -0.0444,\n",
       "                      -0.0204, -0.0273, -0.0290, -0.0442, -0.0370, -0.0344, -0.0503, -0.0139,\n",
       "                      -0.0338, -0.0316, -0.0216, -0.0136, -0.0269, -0.0227, -0.0324, -0.0401,\n",
       "                      -0.0449, -0.0416, -0.0320, -0.0239, -0.0256, -0.0400, -0.0267, -0.0395,\n",
       "                      -0.0289, -0.0337, -0.0388, -0.0401, -0.0344, -0.0350, -0.0355, -0.0221,\n",
       "                      -0.0330, -0.0440, -0.0434, -0.0351, -0.0288, -0.0466, -0.0214, -0.0433,\n",
       "                      -0.0288, -0.0240, -0.0515, -0.0184, -0.0368, -0.0269, -0.0446, -0.0444,\n",
       "                      -0.0218, -0.0453, -0.0238, -0.0292, -0.0359, -0.0377, -0.0253, -0.0276,\n",
       "                      -0.0337, -0.0280, -0.0477, -0.0205, -0.0342, -0.0407, -0.0298, -0.0401,\n",
       "                      -0.0328, -0.0154, -0.0305, -0.0327, -0.0297, -0.0388, -0.0317, -0.0246,\n",
       "                      -0.0358, -0.0270, -0.0464, -0.0463, -0.0342, -0.0391, -0.0175, -0.0305,\n",
       "                      -0.0312, -0.0353, -0.0356, -0.0346, -0.0284, -0.0423, -0.0399, -0.0304,\n",
       "                      -0.0392, -0.0369, -0.0229, -0.0227, -0.0187, -0.0401, -0.0303, -0.0405,\n",
       "                      -0.0286, -0.0375, -0.0378, -0.0345, -0.0368, -0.0348, -0.0339, -0.0257])),\n",
       "             ('ups.1.conv.1.running_mean',\n",
       "              tensor([-1.9662e-01, -1.7371e-01, -3.4956e-02, -1.5835e-01, -1.5522e-01,\n",
       "                      -2.6471e-01, -1.2165e-01, -2.1465e-01,  1.6029e-01,  1.5093e-01,\n",
       "                      -4.0774e-02,  4.2765e-02, -3.3786e-01,  1.1337e-01, -3.2258e-02,\n",
       "                       1.7884e-01,  1.2915e-01, -1.8644e-01,  3.1790e-02, -1.2818e-02,\n",
       "                      -9.5687e-02, -1.6324e-01, -1.4950e-01, -9.1304e-02, -7.7508e-02,\n",
       "                      -1.4885e-01,  2.8051e-01, -1.0638e-01, -1.1512e-01, -2.1486e-01,\n",
       "                       3.8263e-04, -1.5403e-01, -2.9010e-01, -1.2643e-01, -2.8907e-01,\n",
       "                       6.0089e-03, -1.6351e-01, -5.8948e-02,  9.0118e-02, -2.9838e-01,\n",
       "                      -5.9173e-03, -1.1027e-01, -1.4146e-01,  1.1721e-01,  2.0786e-02,\n",
       "                       2.8965e-03, -3.7999e-02, -1.9281e-01, -1.6987e-01, -1.1986e-01,\n",
       "                      -2.3500e-01, -3.4431e-01, -2.7099e-01, -2.2301e-01, -1.1954e-01,\n",
       "                      -2.9409e-01, -3.0061e-02,  1.3429e-02, -1.8782e-01, -2.7709e-01,\n",
       "                      -2.1075e-01, -2.8338e-01, -7.3994e-02, -6.4000e-02, -2.5686e-01,\n",
       "                      -7.4485e-02, -2.2633e-01, -2.7744e-01,  2.8884e-02, -3.3424e-01,\n",
       "                      -6.0785e-02, -6.6299e-02,  4.5244e-02, -4.7945e-02, -3.0183e-01,\n",
       "                      -1.0252e-01, -1.0627e-01, -1.0699e-01, -2.4437e-01, -4.6901e-02,\n",
       "                      -1.1141e-01,  1.5163e-01, -2.5840e-01,  2.6156e-02, -1.1475e-01,\n",
       "                      -4.0267e-02, -1.7083e-01, -3.6334e-01, -1.0276e-01, -2.4307e-01,\n",
       "                      -2.2587e-01, -3.5160e-02, -3.0475e-01, -1.5882e-01, -1.0592e-01,\n",
       "                       4.2698e-01, -3.9023e-02, -1.1737e-01, -2.6328e-02,  8.4340e-04,\n",
       "                      -4.7772e-02, -1.1008e-01, -2.9056e-01, -2.6422e-01, -3.0110e-01,\n",
       "                       1.2806e-01, -3.0798e-01, -4.1262e-02, -3.1680e-01, -2.0180e-01,\n",
       "                      -3.9483e-01, -3.7022e-01, -1.6400e-01, -1.3867e-01,  5.0236e-02,\n",
       "                      -1.0870e-01, -5.3408e-02, -1.6963e-01, -1.2006e-01, -2.5607e-02,\n",
       "                      -1.2127e-01, -6.8493e-02, -1.7858e-01, -1.3166e-01, -1.1434e-01,\n",
       "                      -2.0839e-01, -9.9601e-02, -3.9706e-01])),\n",
       "             ('ups.1.conv.1.running_var',\n",
       "              tensor([0.0814, 0.0817, 0.0858, 0.0800, 0.0828, 0.0802, 0.0837, 0.0808, 0.0870,\n",
       "                      0.0744, 0.0832, 0.0822, 0.0837, 0.0915, 0.0897, 0.0826, 0.0783, 0.0808,\n",
       "                      0.0845, 0.0862, 0.0770, 0.0807, 0.0921, 0.0981, 0.0798, 0.0767, 0.0848,\n",
       "                      0.0741, 0.0745, 0.0783, 0.0829, 0.1086, 0.0875, 0.0888, 0.0971, 0.0821,\n",
       "                      0.0878, 0.0858, 0.0915, 0.0885, 0.0886, 0.0789, 0.0820, 0.0824, 0.0861,\n",
       "                      0.0778, 0.0789, 0.0812, 0.0784, 0.0703, 0.0897, 0.0903, 0.0779, 0.0869,\n",
       "                      0.0917, 0.0876, 0.0814, 0.0818, 0.0965, 0.0861, 0.0816, 0.0913, 0.0722,\n",
       "                      0.0846, 0.0872, 0.0903, 0.0936, 0.0943, 0.0832, 0.0829, 0.0738, 0.0697,\n",
       "                      0.0856, 0.0789, 0.0861, 0.0855, 0.0797, 0.0788, 0.0800, 0.0809, 0.0811,\n",
       "                      0.0901, 0.0902, 0.0808, 0.0802, 0.0884, 0.0846, 0.0893, 0.0904, 0.0855,\n",
       "                      0.0890, 0.0805, 0.0807, 0.0865, 0.0830, 0.0951, 0.0836, 0.0964, 0.0876,\n",
       "                      0.0818, 0.0842, 0.0898, 0.0966, 0.0881, 0.1071, 0.0868, 0.0867, 0.0756,\n",
       "                      0.0850, 0.0825, 0.0879, 0.0997, 0.0821, 0.0824, 0.0840, 0.0993, 0.0731,\n",
       "                      0.0935, 0.0822, 0.0811, 0.0962, 0.0816, 0.0939, 0.0869, 0.0734, 0.0841,\n",
       "                      0.0819, 0.0901])),\n",
       "             ('ups.1.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('ups.1.conv.3.weight',\n",
       "              tensor([[[[ 6.4041e-03, -2.4299e-02, -1.4827e-03],\n",
       "                        [-1.8064e-02,  1.9391e-02, -1.7841e-02],\n",
       "                        [ 3.0800e-03, -1.2047e-02,  1.6678e-02]],\n",
       "              \n",
       "                       [[-1.7318e-02,  2.7406e-02,  1.8135e-02],\n",
       "                        [-2.9005e-02,  1.5167e-02, -2.6096e-02],\n",
       "                        [-1.1046e-02,  2.3570e-03, -1.6927e-02]],\n",
       "              \n",
       "                       [[-6.6100e-03, -4.3780e-02,  5.5707e-03],\n",
       "                        [ 1.9909e-02, -9.3621e-03, -2.9060e-02],\n",
       "                        [ 6.4254e-03, -2.7096e-02, -1.3547e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.5744e-03, -9.8571e-04, -1.5458e-02],\n",
       "                        [-1.8204e-02,  1.3285e-02, -1.4288e-02],\n",
       "                        [-3.0055e-03,  2.6135e-02, -1.9021e-02]],\n",
       "              \n",
       "                       [[ 3.5517e-03,  1.7116e-02, -1.1625e-03],\n",
       "                        [-1.1005e-02, -1.4385e-02,  6.3802e-03],\n",
       "                        [ 1.3454e-02,  8.6040e-03, -1.3509e-02]],\n",
       "              \n",
       "                       [[-2.2676e-02, -1.0742e-03, -2.1767e-02],\n",
       "                        [-1.6673e-02,  8.9415e-03, -7.5951e-03],\n",
       "                        [-3.5275e-03, -1.5556e-02,  3.3645e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0708e-02,  3.0164e-02, -5.7602e-03],\n",
       "                        [ 1.9335e-02, -1.6675e-02,  7.0009e-03],\n",
       "                        [ 5.3144e-03, -3.4398e-02,  2.2209e-02]],\n",
       "              \n",
       "                       [[-3.3031e-02, -2.2047e-02, -2.4275e-02],\n",
       "                        [ 1.4736e-02,  1.7501e-02, -2.3284e-02],\n",
       "                        [-2.3831e-03, -5.4616e-03,  8.2509e-04]],\n",
       "              \n",
       "                       [[-1.0989e-02, -3.4483e-02, -4.4837e-03],\n",
       "                        [-2.5954e-02, -2.3861e-03, -7.7152e-03],\n",
       "                        [ 2.0639e-02,  3.9716e-03,  1.4726e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 8.4760e-04, -2.7786e-02, -6.4425e-03],\n",
       "                        [-6.7362e-03,  1.8971e-02,  6.5456e-03],\n",
       "                        [-1.4202e-02, -2.5901e-02,  1.9709e-02]],\n",
       "              \n",
       "                       [[-1.0701e-02,  4.9092e-03,  1.2477e-02],\n",
       "                        [ 2.4857e-02, -1.4355e-02, -2.0687e-02],\n",
       "                        [-2.8039e-03, -2.0869e-02, -7.4110e-03]],\n",
       "              \n",
       "                       [[ 1.5955e-02, -7.1135e-03, -8.2079e-04],\n",
       "                        [ 2.4455e-02, -9.9687e-03, -2.4577e-03],\n",
       "                        [ 2.7527e-02,  1.1251e-02,  1.0120e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7705e-02,  4.9239e-02,  2.4769e-02],\n",
       "                        [ 2.0924e-02,  1.3868e-02,  4.9635e-03],\n",
       "                        [-6.2571e-03,  1.4363e-02,  2.5853e-02]],\n",
       "              \n",
       "                       [[ 3.8875e-03, -2.2392e-02, -2.1829e-03],\n",
       "                        [-4.4106e-03,  2.0223e-02,  2.5894e-02],\n",
       "                        [ 2.9899e-02,  3.3440e-02, -1.0928e-02]],\n",
       "              \n",
       "                       [[ 4.0199e-03, -1.9694e-02,  2.0752e-02],\n",
       "                        [ 1.7542e-02, -4.2788e-03, -1.2460e-02],\n",
       "                        [-1.7724e-02, -1.0447e-02,  5.0789e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4753e-03,  1.7288e-02,  6.0261e-03],\n",
       "                        [-1.5411e-03,  1.9156e-02, -1.6876e-02],\n",
       "                        [ 5.8751e-04,  1.4929e-02,  9.9980e-03]],\n",
       "              \n",
       "                       [[-2.9728e-02,  1.0330e-02, -3.3271e-02],\n",
       "                        [-3.4160e-03,  7.4209e-03, -2.5035e-02],\n",
       "                        [-3.2497e-02, -1.8608e-02,  1.8767e-02]],\n",
       "              \n",
       "                       [[-3.8735e-02, -2.5007e-02, -2.1906e-02],\n",
       "                        [-2.1269e-02,  7.5414e-04,  7.0448e-03],\n",
       "                        [ 3.1668e-02,  1.2655e-02, -2.9094e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.1613e-02, -4.8528e-02, -4.9123e-02],\n",
       "                        [-4.1464e-02, -1.2593e-02, -2.3448e-03],\n",
       "                        [-6.2365e-03,  2.3302e-02, -3.6645e-02]],\n",
       "              \n",
       "                       [[-3.2720e-03, -1.8691e-02, -4.1460e-03],\n",
       "                        [-1.8610e-02, -2.3214e-02, -1.4773e-02],\n",
       "                        [-1.5678e-02, -1.0336e-02, -3.6370e-05]],\n",
       "              \n",
       "                       [[ 2.0472e-02, -3.8921e-02, -5.3198e-03],\n",
       "                        [ 2.1912e-02, -2.3078e-02, -2.8448e-02],\n",
       "                        [-1.1482e-02,  2.1337e-02, -1.6315e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.5242e-03,  9.3474e-03,  1.6635e-02],\n",
       "                        [ 2.7743e-02,  2.1112e-02, -7.8390e-03],\n",
       "                        [ 1.6067e-02, -1.5850e-02, -3.0836e-02]],\n",
       "              \n",
       "                       [[-1.2563e-02,  2.2164e-03, -3.6985e-02],\n",
       "                        [-8.7137e-03,  4.6540e-02, -2.1231e-02],\n",
       "                        [-6.2517e-03, -1.3499e-02,  2.6305e-02]],\n",
       "              \n",
       "                       [[ 3.3133e-02,  1.2595e-03,  1.3111e-02],\n",
       "                        [ 1.5569e-03, -2.1904e-02,  9.1546e-03],\n",
       "                        [-8.5587e-03,  2.0011e-02,  2.2389e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5902e-02,  3.7055e-02, -2.0747e-02],\n",
       "                        [ 1.4034e-02,  1.4973e-02, -4.5505e-03],\n",
       "                        [ 1.8440e-02, -4.3061e-02, -1.0388e-02]],\n",
       "              \n",
       "                       [[-1.3690e-02, -2.0492e-02, -8.4207e-03],\n",
       "                        [ 1.6197e-02,  1.1951e-02, -9.5246e-03],\n",
       "                        [-1.8286e-02, -1.3748e-02, -2.1381e-02]],\n",
       "              \n",
       "                       [[ 3.7208e-02, -6.6088e-03,  1.5213e-02],\n",
       "                        [ 2.2659e-02, -3.2034e-02, -1.5483e-02],\n",
       "                        [-4.9839e-03, -1.4608e-02,  3.1070e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.8408e-02, -2.1437e-02,  2.8845e-03],\n",
       "                        [-1.5037e-02, -3.4832e-03, -3.5985e-03],\n",
       "                        [ 1.9630e-02,  1.6655e-02, -1.4395e-02]],\n",
       "              \n",
       "                       [[-4.7337e-03, -1.8028e-02, -5.7085e-03],\n",
       "                        [-2.6133e-02,  2.3350e-02, -2.5217e-02],\n",
       "                        [ 1.1409e-02,  2.6841e-02,  2.6099e-02]],\n",
       "              \n",
       "                       [[-4.2161e-03,  2.4295e-02, -5.2538e-03],\n",
       "                        [-5.0880e-03,  9.4208e-03, -2.0757e-02],\n",
       "                        [ 8.8505e-03,  4.2410e-04, -9.7443e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.3720e-02, -2.5520e-02,  2.2896e-02],\n",
       "                        [ 1.7316e-02,  4.6661e-03, -3.2799e-02],\n",
       "                        [-1.4498e-02,  1.4050e-02, -1.8188e-02]],\n",
       "              \n",
       "                       [[ 3.5506e-02, -8.1854e-04,  6.3634e-03],\n",
       "                        [ 1.3718e-02,  4.1147e-02, -1.2254e-02],\n",
       "                        [ 1.3774e-02, -2.4618e-02, -1.6896e-02]],\n",
       "              \n",
       "                       [[ 5.1254e-03, -7.1028e-03, -3.7310e-03],\n",
       "                        [ 2.4531e-02,  1.9118e-02, -1.8901e-02],\n",
       "                        [ 2.0460e-02, -1.0995e-02,  3.0144e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.3577e-02, -3.9028e-03,  1.6689e-02],\n",
       "                        [ 1.4677e-02, -3.6103e-02,  2.9534e-02],\n",
       "                        [ 1.1261e-02,  1.4604e-02,  1.4033e-02]],\n",
       "              \n",
       "                       [[-1.2732e-03, -2.5908e-02, -1.8234e-02],\n",
       "                        [ 1.5788e-02, -1.2558e-02, -2.7492e-02],\n",
       "                        [-1.1627e-02,  2.0829e-02,  5.2227e-03]],\n",
       "              \n",
       "                       [[ 1.4446e-02,  1.3079e-02, -1.2359e-02],\n",
       "                        [-3.3132e-04, -9.7698e-03, -3.1480e-03],\n",
       "                        [-5.6803e-03, -9.1826e-03, -9.6564e-03]]]])),\n",
       "             ('ups.1.conv.4.weight',\n",
       "              tensor([1.0541, 1.0339, 1.0502, 1.0328, 1.0379, 1.0240, 1.0172, 1.0344, 1.0364,\n",
       "                      1.0580, 1.0182, 1.0373, 1.0433, 1.0472, 1.0327, 1.0597, 1.0441, 1.0428,\n",
       "                      1.0376, 1.0264, 1.0353, 1.0346, 1.0420, 1.0141, 1.0240, 1.0473, 1.0300,\n",
       "                      1.0541, 1.0265, 1.0738, 1.0568, 1.0639, 1.0437, 1.0417, 1.0435, 1.0326,\n",
       "                      1.0202, 1.0442, 1.0412, 1.0457, 1.0429, 1.0329, 1.0441, 1.0300, 1.0319,\n",
       "                      1.0425, 1.0509, 1.0348, 1.0483, 1.0394, 1.0366, 1.0499, 1.0269, 1.0380,\n",
       "                      1.0481, 1.0446, 1.0358, 1.0604, 1.0214, 1.0366, 1.0346, 1.0274, 1.0391,\n",
       "                      1.0253, 1.0383, 1.0340, 1.0512, 1.0431, 1.0457, 1.0483, 1.0572, 1.0363,\n",
       "                      1.0425, 1.0609, 1.0374, 1.0245, 1.0326, 1.0234, 1.0522, 1.0567, 1.0340,\n",
       "                      1.0447, 1.0211, 1.0471, 1.0392, 1.0662, 1.0674, 1.0327, 1.0553, 1.0351,\n",
       "                      1.0395, 1.0625, 1.0233, 1.0425, 1.0449, 1.0313, 1.0436, 1.0565, 1.0272,\n",
       "                      1.0334, 1.0372, 1.0448, 1.0441, 1.0356, 1.0443, 1.0307, 1.0395, 1.0425,\n",
       "                      1.0385, 1.0263, 1.0504, 1.0109, 1.0286, 1.0376, 1.0347, 1.0328, 1.0449,\n",
       "                      1.0355, 1.0197, 1.0408, 1.0292, 1.0398, 1.0321, 1.0188, 1.0390, 1.0413,\n",
       "                      1.0522, 1.0442])),\n",
       "             ('ups.1.conv.4.bias',\n",
       "              tensor([-0.0055, -0.0150, -0.0089, -0.0136, -0.0196, -0.0046, -0.0231, -0.0076,\n",
       "                      -0.0061, -0.0051, -0.0223, -0.0057, -0.0113, -0.0106, -0.0035,  0.0112,\n",
       "                      -0.0034, -0.0066, -0.0253, -0.0224, -0.0278, -0.0138, -0.0017, -0.0119,\n",
       "                      -0.0194, -0.0041, -0.0183, -0.0129, -0.0085, -0.0078, -0.0103, -0.0135,\n",
       "                      -0.0040, -0.0121, -0.0216,  0.0006, -0.0115, -0.0119,  0.0022, -0.0161,\n",
       "                      -0.0036, -0.0204, -0.0043, -0.0347, -0.0103, -0.0176, -0.0106, -0.0128,\n",
       "                      -0.0281, -0.0232, -0.0174, -0.0047, -0.0151, -0.0350, -0.0240, -0.0006,\n",
       "                      -0.0175, -0.0018, -0.0308, -0.0156, -0.0146, -0.0156, -0.0092, -0.0107,\n",
       "                      -0.0111, -0.0144,  0.0107, -0.0002,  0.0059, -0.0059, -0.0144, -0.0067,\n",
       "                      -0.0214, -0.0096, -0.0201, -0.0172, -0.0208, -0.0218, -0.0085, -0.0069,\n",
       "                      -0.0173,  0.0003, -0.0163, -0.0130, -0.0005,  0.0014,  0.0077, -0.0144,\n",
       "                      -0.0104, -0.0016, -0.0266, -0.0075, -0.0070,  0.0004, -0.0193, -0.0250,\n",
       "                      -0.0198, -0.0157,  0.0002, -0.0243,  0.0008, -0.0202, -0.0130,  0.0013,\n",
       "                      -0.0011, -0.0266, -0.0289, -0.0100, -0.0229, -0.0065, -0.0095, -0.0336,\n",
       "                      -0.0174, -0.0143, -0.0073, -0.0121, -0.0096, -0.0077, -0.0191, -0.0136,\n",
       "                      -0.0162, -0.0197, -0.0337, -0.0255, -0.0108, -0.0169, -0.0130, -0.0080])),\n",
       "             ('ups.1.conv.4.running_mean',\n",
       "              tensor([-0.3181, -0.1207, -0.0962,  0.0276,  0.1986, -0.3633,  0.2566, -0.0065,\n",
       "                      -0.2734, -0.2813,  0.1314, -0.2472, -0.0446,  0.1262,  0.1249, -0.2207,\n",
       "                      -0.1341, -0.0177, -0.1906,  0.1075,  0.1608, -0.0020, -0.2392,  0.0143,\n",
       "                      -0.0415, -0.2524,  0.0655, -0.2148, -0.0849, -0.1745,  0.1126, -0.1911,\n",
       "                      -0.0274,  0.1986,  0.1475, -0.3623,  0.0047, -0.0389,  0.0160, -0.0088,\n",
       "                      -0.3886, -0.0369, -0.0308,  0.0085,  0.1302,  0.0371, -0.1205,  0.0877,\n",
       "                      -0.1056,  0.1427,  0.1568,  0.1082,  0.2502,  0.0991, -0.0004, -0.1944,\n",
       "                      -0.1113,  0.1351, -0.0312, -0.3229,  0.1053,  0.0074, -0.0153, -0.2687,\n",
       "                       0.0159,  0.2539,  0.1786, -0.1573,  0.1278, -0.2257,  0.2126, -0.3836,\n",
       "                      -0.0084,  0.0948, -0.2397,  0.0047,  0.2934,  0.1259, -0.1632,  0.2101,\n",
       "                      -0.0033,  0.0167, -0.2355,  0.1059, -0.3785, -0.0567, -0.1386, -0.0998,\n",
       "                      -0.1384,  0.1331,  0.0697, -0.0650,  0.2461,  0.0088, -0.0023,  0.0867,\n",
       "                      -0.3189,  0.0249, -0.3554, -0.1743,  0.0124, -0.0459,  0.1544,  0.2438,\n",
       "                      -0.0086, -0.0731,  0.3349,  0.1985,  0.1013,  0.0558,  0.1016, -0.0298,\n",
       "                       0.0029, -0.1133, -0.3011,  0.0187, -0.0805, -0.1978, -0.1761, -0.1468,\n",
       "                       0.2707,  0.0094,  0.1160, -0.2061, -0.0497, -0.0315, -0.1589, -0.0564])),\n",
       "             ('ups.1.conv.4.running_var',\n",
       "              tensor([0.1298, 0.1233, 0.1292, 0.1112, 0.1098, 0.1235, 0.1085, 0.1029, 0.1309,\n",
       "                      0.1197, 0.1284, 0.1175, 0.1102, 0.1062, 0.1174, 0.0996, 0.1039, 0.1018,\n",
       "                      0.1120, 0.1086, 0.0981, 0.1014, 0.1126, 0.1247, 0.1142, 0.1132, 0.1235,\n",
       "                      0.1051, 0.1287, 0.0985, 0.1126, 0.1009, 0.1164, 0.1078, 0.1013, 0.1137,\n",
       "                      0.1160, 0.1148, 0.1240, 0.1056, 0.1174, 0.1192, 0.1072, 0.1043, 0.1041,\n",
       "                      0.1161, 0.1115, 0.1020, 0.1031, 0.1123, 0.0919, 0.0962, 0.1199, 0.0956,\n",
       "                      0.1021, 0.1086, 0.1220, 0.1055, 0.1121, 0.1442, 0.1077, 0.1400, 0.1120,\n",
       "                      0.1158, 0.0937, 0.0964, 0.1003, 0.1318, 0.1075, 0.1020, 0.1011, 0.1056,\n",
       "                      0.1012, 0.1059, 0.1110, 0.1183, 0.1033, 0.1214, 0.1101, 0.1091, 0.1324,\n",
       "                      0.1035, 0.1158, 0.1090, 0.1113, 0.1026, 0.1091, 0.1129, 0.0969, 0.1019,\n",
       "                      0.1111, 0.1116, 0.1033, 0.1104, 0.1171, 0.1108, 0.1359, 0.1019, 0.1280,\n",
       "                      0.1092, 0.1084, 0.1009, 0.1074, 0.1131, 0.1090, 0.0931, 0.1002, 0.0976,\n",
       "                      0.1117, 0.1179, 0.0979, 0.1164, 0.1115, 0.1281, 0.1276, 0.1073, 0.1243,\n",
       "                      0.1139, 0.1192, 0.1038, 0.1112, 0.1086, 0.0980, 0.1103, 0.1033, 0.0979,\n",
       "                      0.0945, 0.1158])),\n",
       "             ('ups.1.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('ups.2.weight',\n",
       "              tensor([[[[-0.0040, -0.0282],\n",
       "                        [-0.0375, -0.0146]],\n",
       "              \n",
       "                       [[-0.0365,  0.0473],\n",
       "                        [-0.0217,  0.0017]],\n",
       "              \n",
       "                       [[-0.0558, -0.0488],\n",
       "                        [-0.0695,  0.0181]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0413,  0.0424],\n",
       "                        [-0.0578, -0.0254]],\n",
       "              \n",
       "                       [[-0.0584,  0.0374],\n",
       "                        [-0.0190,  0.0283]],\n",
       "              \n",
       "                       [[ 0.0505,  0.0035],\n",
       "                        [-0.0297,  0.0587]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0381,  0.0254],\n",
       "                        [ 0.0545,  0.0391]],\n",
       "              \n",
       "                       [[-0.0177, -0.0325],\n",
       "                        [ 0.0187, -0.0364]],\n",
       "              \n",
       "                       [[-0.0140, -0.0905],\n",
       "                        [ 0.0493,  0.0007]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0299,  0.0604],\n",
       "                        [ 0.0573, -0.0151]],\n",
       "              \n",
       "                       [[ 0.0325, -0.0534],\n",
       "                        [-0.0156, -0.0614]],\n",
       "              \n",
       "                       [[-0.0483,  0.0538],\n",
       "                        [ 0.0764,  0.0611]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0073,  0.0445],\n",
       "                        [ 0.0078,  0.0134]],\n",
       "              \n",
       "                       [[-0.0192, -0.0429],\n",
       "                        [ 0.0817,  0.0062]],\n",
       "              \n",
       "                       [[ 0.0480, -0.0390],\n",
       "                        [-0.0004, -0.0379]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0172, -0.0699],\n",
       "                        [-0.0520,  0.0483]],\n",
       "              \n",
       "                       [[-0.0447,  0.0227],\n",
       "                        [-0.0469,  0.0119]],\n",
       "              \n",
       "                       [[-0.0574, -0.0035],\n",
       "                        [-0.0010, -0.0073]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0208, -0.0198],\n",
       "                        [ 0.0372,  0.0342]],\n",
       "              \n",
       "                       [[ 0.0102,  0.0095],\n",
       "                        [-0.0449, -0.0131]],\n",
       "              \n",
       "                       [[-0.0604,  0.0275],\n",
       "                        [ 0.0443,  0.0623]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0121,  0.0084],\n",
       "                        [ 0.0556,  0.0644]],\n",
       "              \n",
       "                       [[-0.0602,  0.0111],\n",
       "                        [ 0.0310,  0.0042]],\n",
       "              \n",
       "                       [[-0.0551, -0.0613],\n",
       "                        [-0.0340, -0.0251]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0395,  0.0412],\n",
       "                        [-0.0223,  0.0439]],\n",
       "              \n",
       "                       [[-0.0300, -0.0265],\n",
       "                        [ 0.0032, -0.0472]],\n",
       "              \n",
       "                       [[-0.0407,  0.0119],\n",
       "                        [ 0.0185, -0.0587]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0234, -0.0037],\n",
       "                        [-0.0209, -0.0469]],\n",
       "              \n",
       "                       [[ 0.0087, -0.0350],\n",
       "                        [ 0.0270, -0.0374]],\n",
       "              \n",
       "                       [[ 0.0804, -0.0729],\n",
       "                        [ 0.0614,  0.0197]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0588, -0.0143],\n",
       "                        [ 0.0506,  0.0014]],\n",
       "              \n",
       "                       [[-0.0335,  0.0394],\n",
       "                        [ 0.0349, -0.0484]],\n",
       "              \n",
       "                       [[ 0.0341,  0.0416],\n",
       "                        [ 0.0298,  0.0272]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0496,  0.0174],\n",
       "                        [ 0.0139,  0.0066]],\n",
       "              \n",
       "                       [[ 0.0055,  0.0278],\n",
       "                        [-0.0331, -0.0028]],\n",
       "              \n",
       "                       [[ 0.0254, -0.0152],\n",
       "                        [-0.0347,  0.0695]]]])),\n",
       "             ('ups.2.bias',\n",
       "              tensor([-0.0426, -0.0166,  0.0082,  0.0312,  0.0240, -0.0428,  0.0106,  0.0020,\n",
       "                       0.0175, -0.0275, -0.0528,  0.0287, -0.0069, -0.0431,  0.0568, -0.0125,\n",
       "                      -0.0108, -0.0169,  0.0277,  0.0425,  0.0205, -0.0293,  0.0375,  0.0304,\n",
       "                       0.0434, -0.0407,  0.0670, -0.0114, -0.0635, -0.0375, -0.0062, -0.0221,\n",
       "                       0.0057,  0.0347, -0.0347, -0.0246,  0.0450, -0.0401,  0.0056, -0.0187,\n",
       "                      -0.0037,  0.0237, -0.0191, -0.0453, -0.0600, -0.0433,  0.0229, -0.0401,\n",
       "                       0.0058,  0.0032,  0.0415,  0.0180,  0.0194, -0.0459, -0.0095, -0.0061,\n",
       "                      -0.0193, -0.0054,  0.0111,  0.0328, -0.0298,  0.0115, -0.0130, -0.0125])),\n",
       "             ('ups.3.conv.0.weight',\n",
       "              tensor([[[[-3.0094e-03,  2.4223e-02, -1.0803e-02],\n",
       "                        [-1.9590e-02, -6.6313e-03, -3.0585e-03],\n",
       "                        [ 1.5383e-02, -1.0102e-02, -1.3407e-02]],\n",
       "              \n",
       "                       [[-2.8903e-02, -1.6541e-02,  9.4407e-03],\n",
       "                        [ 7.3464e-03, -1.2793e-02,  3.9910e-02],\n",
       "                        [ 1.7132e-04,  1.0436e-02, -1.7567e-03]],\n",
       "              \n",
       "                       [[ 7.9190e-03, -1.8848e-02,  1.3253e-02],\n",
       "                        [-7.9666e-04, -1.2890e-02, -1.6814e-02],\n",
       "                        [-1.8566e-04, -2.5689e-02,  1.9240e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.3945e-02, -5.7150e-03, -2.2195e-02],\n",
       "                        [-3.8959e-02,  3.0001e-02,  4.8838e-03],\n",
       "                        [ 2.3921e-03, -1.9239e-02, -1.2717e-02]],\n",
       "              \n",
       "                       [[ 7.5702e-03,  1.5165e-02,  1.3344e-02],\n",
       "                        [ 2.0458e-02,  8.8309e-03,  1.0292e-02],\n",
       "                        [ 1.7041e-02, -1.1483e-02,  1.8524e-02]],\n",
       "              \n",
       "                       [[ 3.8724e-02,  2.1054e-02, -5.7980e-03],\n",
       "                        [-8.1632e-03,  3.4515e-02, -1.0012e-02],\n",
       "                        [-3.2136e-02,  1.2594e-02,  2.3989e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.2196e-03, -9.7561e-03,  1.3635e-02],\n",
       "                        [ 1.3949e-02, -8.1975e-03,  4.4695e-03],\n",
       "                        [ 1.0138e-02, -1.8529e-02, -1.3107e-02]],\n",
       "              \n",
       "                       [[-3.0470e-03,  2.5015e-02,  1.0686e-02],\n",
       "                        [ 4.6737e-03,  4.5945e-03,  1.6117e-02],\n",
       "                        [-1.1456e-02,  2.3010e-02,  2.5132e-02]],\n",
       "              \n",
       "                       [[-2.7929e-02, -1.1555e-02,  4.8356e-03],\n",
       "                        [ 1.9183e-02,  2.6340e-03, -7.7480e-03],\n",
       "                        [-1.0100e-02,  2.0464e-03,  2.6263e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.5934e-02,  6.7361e-03, -5.5241e-03],\n",
       "                        [ 6.6067e-03, -4.4967e-02,  1.2783e-02],\n",
       "                        [-2.8114e-02, -5.7414e-03, -1.7294e-02]],\n",
       "              \n",
       "                       [[-1.9777e-02,  8.4229e-03,  1.0935e-02],\n",
       "                        [ 2.2900e-02,  2.3724e-02, -5.1776e-03],\n",
       "                        [ 1.8451e-02, -5.5230e-03,  9.5355e-03]],\n",
       "              \n",
       "                       [[ 1.8863e-02,  3.4578e-02,  3.4917e-03],\n",
       "                        [-4.5020e-02, -1.4493e-02, -1.2495e-02],\n",
       "                        [ 4.5326e-02,  1.3727e-02, -2.3575e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7310e-02, -1.0242e-02,  2.2027e-02],\n",
       "                        [-1.9692e-02, -1.8832e-02, -2.7566e-02],\n",
       "                        [ 1.8873e-02,  1.6078e-02,  2.1738e-02]],\n",
       "              \n",
       "                       [[ 1.3976e-02,  4.0735e-03, -1.8723e-03],\n",
       "                        [ 2.5772e-02, -2.2704e-02,  1.2244e-02],\n",
       "                        [ 1.6928e-02, -2.5618e-03,  6.3594e-03]],\n",
       "              \n",
       "                       [[ 1.5910e-03,  1.8074e-02, -1.9672e-02],\n",
       "                        [ 2.4349e-02, -2.8331e-02, -1.7384e-02],\n",
       "                        [ 2.4560e-02,  1.2392e-02,  1.4746e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9300e-02,  2.2773e-02,  5.9963e-03],\n",
       "                        [-1.5812e-02,  1.8040e-02, -2.7866e-02],\n",
       "                        [-2.3183e-02, -2.2962e-02, -1.6884e-02]],\n",
       "              \n",
       "                       [[-6.5068e-03, -1.9224e-02,  1.0684e-02],\n",
       "                        [ 2.8259e-02, -8.1839e-03, -1.3119e-02],\n",
       "                        [ 4.8144e-03, -2.8944e-02,  5.8150e-02]],\n",
       "              \n",
       "                       [[-8.2867e-03,  3.8209e-02, -1.6821e-02],\n",
       "                        [-3.2268e-02, -2.4929e-02, -3.8379e-02],\n",
       "                        [ 1.9340e-03, -4.8268e-02,  2.3385e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4001e-02,  4.5995e-03,  5.9753e-03],\n",
       "                        [ 1.0677e-02, -6.1165e-03,  5.8973e-03],\n",
       "                        [-1.4939e-02, -2.8937e-02, -2.4642e-03]],\n",
       "              \n",
       "                       [[-1.8194e-02,  1.0491e-02,  1.9768e-02],\n",
       "                        [-2.1724e-02,  1.7778e-02, -2.1017e-03],\n",
       "                        [ 1.2439e-02,  1.4530e-02, -1.7837e-03]],\n",
       "              \n",
       "                       [[-9.4375e-05, -4.4791e-03, -3.1508e-05],\n",
       "                        [ 2.9579e-02, -2.4687e-02, -1.2686e-04],\n",
       "                        [ 1.7416e-02,  3.3047e-02, -1.2000e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.9616e-02, -1.8556e-02, -3.1259e-02],\n",
       "                        [ 1.5784e-02, -3.4581e-02,  2.7063e-02],\n",
       "                        [-6.3370e-03, -2.0320e-02,  7.0171e-03]],\n",
       "              \n",
       "                       [[-2.5530e-02,  8.9545e-03,  1.8091e-02],\n",
       "                        [-1.0291e-02,  1.4301e-02, -1.2460e-02],\n",
       "                        [-4.7103e-02, -1.5866e-02, -1.2974e-02]],\n",
       "              \n",
       "                       [[-1.1009e-02, -2.9392e-02, -9.4839e-03],\n",
       "                        [ 8.5856e-03,  3.2707e-02, -1.5959e-02],\n",
       "                        [-1.6470e-02,  1.4974e-02, -9.8733e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8638e-02, -1.5283e-02, -3.6792e-02],\n",
       "                        [-2.0633e-04,  1.3191e-02,  1.5843e-02],\n",
       "                        [-2.9457e-02, -2.1468e-03, -1.0535e-02]],\n",
       "              \n",
       "                       [[ 2.0415e-02,  7.7307e-03,  1.7037e-02],\n",
       "                        [ 1.6347e-02,  3.6625e-03,  2.2531e-02],\n",
       "                        [ 3.1992e-03, -2.7653e-03,  4.3612e-03]],\n",
       "              \n",
       "                       [[-1.1632e-02, -1.7613e-02,  3.4703e-03],\n",
       "                        [ 1.0364e-02, -1.1449e-02, -4.2771e-03],\n",
       "                        [ 2.2146e-03, -3.6083e-03, -1.0104e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.9255e-03,  4.0463e-02,  3.7868e-02],\n",
       "                        [ 4.7147e-02, -9.6827e-03,  7.2568e-03],\n",
       "                        [ 2.6320e-02, -6.0171e-05,  3.1725e-02]],\n",
       "              \n",
       "                       [[ 1.1236e-02, -5.0280e-03,  3.6654e-02],\n",
       "                        [-2.0347e-02,  4.5139e-02, -5.0866e-02],\n",
       "                        [ 2.5529e-03, -8.6323e-03, -1.4311e-02]],\n",
       "              \n",
       "                       [[-4.8750e-03, -2.3239e-02, -8.6698e-03],\n",
       "                        [ 1.3200e-02,  2.2121e-02, -2.3260e-03],\n",
       "                        [-2.1374e-02,  3.2112e-02,  4.2120e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7377e-03, -8.7680e-03, -1.0058e-02],\n",
       "                        [-1.6859e-02,  1.9783e-02, -2.0224e-02],\n",
       "                        [ 1.7817e-02,  1.7721e-02, -1.2421e-02]],\n",
       "              \n",
       "                       [[ 4.4315e-03, -1.4071e-02, -2.0655e-02],\n",
       "                        [-1.4519e-02, -2.5825e-02, -2.8889e-02],\n",
       "                        [ 2.5377e-02, -1.7943e-02, -1.0072e-02]],\n",
       "              \n",
       "                       [[ 3.6659e-03, -3.8424e-04, -2.4909e-02],\n",
       "                        [-2.9628e-02,  2.1947e-02,  2.7646e-02],\n",
       "                        [-2.9408e-02,  1.4675e-02, -2.1887e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0980e-02, -3.2760e-03, -1.0136e-02],\n",
       "                        [-2.5119e-02,  1.2958e-02,  8.4818e-03],\n",
       "                        [ 4.3959e-02, -1.1204e-02,  4.3351e-03]],\n",
       "              \n",
       "                       [[-6.2780e-03, -4.5655e-02, -3.0894e-02],\n",
       "                        [-3.3105e-02, -1.6350e-02,  1.8501e-02],\n",
       "                        [ 4.5879e-02, -5.5044e-03,  1.4781e-02]],\n",
       "              \n",
       "                       [[-2.0274e-02,  3.1807e-02, -2.5734e-02],\n",
       "                        [ 1.7345e-03,  4.3984e-03,  3.1546e-04],\n",
       "                        [ 1.8815e-02,  4.1741e-02,  9.0078e-03]]]])),\n",
       "             ('ups.3.conv.1.weight',\n",
       "              tensor([0.9959, 0.9916, 0.9962, 1.0019, 1.0002, 0.9970, 1.0065, 0.9963, 1.0167,\n",
       "                      1.0121, 0.9892, 0.9944, 1.0001, 0.9907, 0.9931, 0.9947, 1.0100, 1.0008,\n",
       "                      1.0042, 1.0113, 1.0056, 0.9900, 1.0143, 1.0022, 1.0057, 0.9969, 0.9889,\n",
       "                      0.9892, 1.0012, 1.0109, 0.9989, 1.0178, 0.9973, 1.0072, 1.0021, 1.0043,\n",
       "                      0.9980, 0.9847, 0.9922, 1.0023, 0.9883, 1.0005, 1.0145, 0.9913, 1.0105,\n",
       "                      0.9990, 0.9977, 0.9824, 1.0192, 1.0118, 0.9844, 1.0102, 1.0059, 0.9935,\n",
       "                      1.0006, 0.9916, 1.0022, 1.0234, 0.9927, 0.9868, 0.9878, 1.0022, 1.0037,\n",
       "                      1.0097])),\n",
       "             ('ups.3.conv.1.bias',\n",
       "              tensor([-0.0135, -0.0070, -0.0150, -0.0163, -0.0166, -0.0099, -0.0428, -0.0151,\n",
       "                      -0.0286, -0.0145, -0.0378, -0.0234, -0.0216, -0.0268, -0.0245, -0.0262,\n",
       "                      -0.0377,  0.0015, -0.0347, -0.0183, -0.0289, -0.0189, -0.0325, -0.0183,\n",
       "                      -0.0176, -0.0160, -0.0228, -0.0125, -0.0302, -0.0261, -0.0325, -0.0072,\n",
       "                      -0.0046, -0.0416, -0.0163, -0.0407, -0.0234, -0.0384, -0.0469, -0.0126,\n",
       "                      -0.0274, -0.0195, -0.0261, -0.0137, -0.0147, -0.0169, -0.0295, -0.0433,\n",
       "                      -0.0170, -0.0279, -0.0434, -0.0144, -0.0242, -0.0219, -0.0350, -0.0175,\n",
       "                      -0.0343,  0.0017, -0.0343, -0.0290, -0.0244, -0.0380, -0.0150, -0.0392])),\n",
       "             ('ups.3.conv.1.running_mean',\n",
       "              tensor([-0.1159,  0.2547,  0.2237, -0.1789, -0.0785, -0.2455, -0.1807, -0.4244,\n",
       "                       0.0611, -0.3742,  0.0020,  0.1083,  0.3624,  0.1261, -0.2277, -0.2681,\n",
       "                      -0.3587, -0.3817,  0.0058, -0.0779,  0.0677, -0.0271, -0.0774, -0.1750,\n",
       "                      -0.1466, -0.2002, -0.2015, -0.3528, -0.4066,  0.0527,  0.0481, -0.2986,\n",
       "                      -0.1969,  0.0330, -0.0580,  0.2299, -0.2518,  0.0963, -0.0653, -0.2508,\n",
       "                       0.3337,  0.0594,  0.0230, -0.0378,  0.0917, -0.1722,  0.0606,  0.0697,\n",
       "                      -0.2597,  0.1455,  0.1333, -0.0844, -0.2335,  0.0373, -0.0388, -0.2598,\n",
       "                       0.0469, -0.3042, -0.0844, -0.0396, -0.3247,  0.2498, -0.0893, -0.0374])),\n",
       "             ('ups.3.conv.1.running_var',\n",
       "              tensor([0.0995, 0.1050, 0.1078, 0.1087, 0.0944, 0.1318, 0.0903, 0.1222, 0.0790,\n",
       "                      0.1012, 0.0862, 0.0887, 0.1032, 0.1022, 0.1083, 0.1002, 0.0987, 0.1324,\n",
       "                      0.0957, 0.0947, 0.1108, 0.1217, 0.0917, 0.0885, 0.0896, 0.1056, 0.1071,\n",
       "                      0.1265, 0.1046, 0.0883, 0.0865, 0.0930, 0.1141, 0.0837, 0.0926, 0.0761,\n",
       "                      0.0991, 0.0844, 0.0899, 0.0844, 0.1117, 0.0802, 0.0857, 0.0868, 0.0872,\n",
       "                      0.0774, 0.0910, 0.0975, 0.0777, 0.0986, 0.0789, 0.1119, 0.1057, 0.0776,\n",
       "                      0.0945, 0.0894, 0.0795, 0.1143, 0.0810, 0.1064, 0.1315, 0.1083, 0.0864,\n",
       "                      0.0722])),\n",
       "             ('ups.3.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('ups.3.conv.3.weight',\n",
       "              tensor([[[[ 0.0279, -0.0323,  0.0381],\n",
       "                        [ 0.0231, -0.0066,  0.0278],\n",
       "                        [ 0.0080, -0.0006,  0.0104]],\n",
       "              \n",
       "                       [[ 0.0350, -0.0344, -0.0044],\n",
       "                        [-0.0225,  0.0096,  0.0020],\n",
       "                        [-0.0480, -0.0119, -0.0197]],\n",
       "              \n",
       "                       [[ 0.0225, -0.0262,  0.0347],\n",
       "                        [ 0.0130,  0.0102, -0.0225],\n",
       "                        [ 0.0079,  0.0092, -0.0272]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0141,  0.0164,  0.0144],\n",
       "                        [ 0.0188, -0.0208, -0.0377],\n",
       "                        [-0.0154, -0.0009, -0.0329]],\n",
       "              \n",
       "                       [[-0.0056, -0.0007,  0.0117],\n",
       "                        [-0.0257,  0.0140, -0.0159],\n",
       "                        [-0.0117, -0.0336, -0.0361]],\n",
       "              \n",
       "                       [[ 0.0188,  0.0222,  0.0039],\n",
       "                        [-0.0210, -0.0161, -0.0351],\n",
       "                        [ 0.0188, -0.0127, -0.0140]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0331, -0.0175, -0.0285],\n",
       "                        [-0.0180,  0.0154,  0.0222],\n",
       "                        [ 0.0141,  0.0361, -0.0322]],\n",
       "              \n",
       "                       [[-0.0004, -0.0163, -0.0125],\n",
       "                        [-0.0220,  0.0160, -0.0005],\n",
       "                        [-0.0075, -0.0058,  0.0445]],\n",
       "              \n",
       "                       [[ 0.0010,  0.0086,  0.0199],\n",
       "                        [-0.0334,  0.0192,  0.0065],\n",
       "                        [ 0.0328,  0.0433, -0.0062]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0048,  0.0093,  0.0200],\n",
       "                        [ 0.0142, -0.0142, -0.0169],\n",
       "                        [ 0.0108, -0.0430,  0.0007]],\n",
       "              \n",
       "                       [[-0.0391,  0.0165,  0.0026],\n",
       "                        [ 0.0129, -0.0453, -0.0108],\n",
       "                        [-0.0356, -0.0352, -0.0134]],\n",
       "              \n",
       "                       [[ 0.0262, -0.0373,  0.0269],\n",
       "                        [ 0.0370, -0.0228, -0.0228],\n",
       "                        [-0.0322,  0.0249,  0.0213]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0303,  0.0261, -0.0135],\n",
       "                        [-0.0015,  0.0249, -0.0224],\n",
       "                        [-0.0176, -0.0396, -0.0207]],\n",
       "              \n",
       "                       [[ 0.0147, -0.0103,  0.0165],\n",
       "                        [ 0.0396,  0.0283, -0.0014],\n",
       "                        [ 0.0096, -0.0111, -0.0356]],\n",
       "              \n",
       "                       [[ 0.0200,  0.0298, -0.0330],\n",
       "                        [ 0.0304,  0.0168,  0.0056],\n",
       "                        [-0.0072, -0.0109,  0.0330]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0352, -0.0005, -0.0094],\n",
       "                        [ 0.0437, -0.0098, -0.0077],\n",
       "                        [-0.0129,  0.0369, -0.0013]],\n",
       "              \n",
       "                       [[-0.0312, -0.0365, -0.0234],\n",
       "                        [-0.0347,  0.0309, -0.0138],\n",
       "                        [-0.0075,  0.0086,  0.0002]],\n",
       "              \n",
       "                       [[-0.0145, -0.0465,  0.0101],\n",
       "                        [ 0.0062,  0.0331, -0.0118],\n",
       "                        [ 0.0132, -0.0260,  0.0413]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0438, -0.0032, -0.0563],\n",
       "                        [-0.0159,  0.0167,  0.0258],\n",
       "                        [ 0.0328,  0.0117, -0.0209]],\n",
       "              \n",
       "                       [[ 0.0310,  0.0345, -0.0127],\n",
       "                        [ 0.0064,  0.0105, -0.0154],\n",
       "                        [ 0.0223,  0.0524, -0.0134]],\n",
       "              \n",
       "                       [[ 0.0222,  0.0143, -0.0322],\n",
       "                        [-0.0174, -0.0068, -0.0169],\n",
       "                        [ 0.0267,  0.0215, -0.0109]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0417,  0.0237,  0.0133],\n",
       "                        [-0.0238,  0.0351, -0.0291],\n",
       "                        [-0.0171, -0.0454, -0.0037]],\n",
       "              \n",
       "                       [[ 0.0292, -0.0205, -0.0144],\n",
       "                        [-0.0325,  0.0077, -0.0085],\n",
       "                        [-0.0016,  0.0065, -0.0180]],\n",
       "              \n",
       "                       [[-0.0229,  0.0027, -0.0367],\n",
       "                        [ 0.0265, -0.0161, -0.0023],\n",
       "                        [ 0.0065, -0.0322,  0.0144]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0141, -0.0145,  0.0208],\n",
       "                        [-0.0037, -0.0339,  0.0261],\n",
       "                        [-0.0289, -0.0111,  0.0283]],\n",
       "              \n",
       "                       [[-0.0129, -0.0203, -0.0135],\n",
       "                        [ 0.0231, -0.0311,  0.0048],\n",
       "                        [ 0.0376, -0.0243, -0.0175]],\n",
       "              \n",
       "                       [[ 0.0178,  0.0237,  0.0171],\n",
       "                        [-0.0184, -0.0511, -0.0468],\n",
       "                        [-0.0217, -0.0190,  0.0167]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0106,  0.0022,  0.0372],\n",
       "                        [ 0.0512, -0.0214, -0.0336],\n",
       "                        [ 0.0362, -0.0256, -0.0402]],\n",
       "              \n",
       "                       [[-0.0065, -0.0033, -0.0292],\n",
       "                        [ 0.0073,  0.0219, -0.0020],\n",
       "                        [ 0.0122,  0.0342, -0.0115]],\n",
       "              \n",
       "                       [[ 0.0318,  0.0116, -0.0364],\n",
       "                        [ 0.0222,  0.0067,  0.0014],\n",
       "                        [-0.0351,  0.0017, -0.0290]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0272, -0.0156, -0.0190],\n",
       "                        [ 0.0006,  0.0097,  0.0183],\n",
       "                        [ 0.0399,  0.0193,  0.0253]],\n",
       "              \n",
       "                       [[ 0.0125, -0.0390,  0.0153],\n",
       "                        [-0.0513,  0.0373, -0.0173],\n",
       "                        [ 0.0227, -0.0265,  0.0046]],\n",
       "              \n",
       "                       [[ 0.0389, -0.0009,  0.0312],\n",
       "                        [ 0.0081,  0.0405, -0.0209],\n",
       "                        [ 0.0285, -0.0065, -0.0375]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0014, -0.0162,  0.0344],\n",
       "                        [ 0.0290,  0.0288, -0.0238],\n",
       "                        [ 0.0115, -0.0125,  0.0135]],\n",
       "              \n",
       "                       [[ 0.0274, -0.0476,  0.0110],\n",
       "                        [ 0.0027,  0.0093,  0.0325],\n",
       "                        [ 0.0085, -0.0170, -0.0301]],\n",
       "              \n",
       "                       [[ 0.0084, -0.0083,  0.0152],\n",
       "                        [ 0.0195,  0.0269,  0.0268],\n",
       "                        [-0.0112,  0.0371,  0.0211]]]])),\n",
       "             ('ups.3.conv.4.weight',\n",
       "              tensor([1.0307, 1.0112, 1.0147, 1.0106, 1.0142, 1.0226, 1.0090, 1.0162, 1.0054,\n",
       "                      1.0331, 1.0223, 1.0128, 1.0062, 1.0117, 1.0103, 1.0534, 0.9982, 1.0024,\n",
       "                      1.0165, 1.0030, 1.0190, 1.0220, 1.0085, 1.0267, 1.0367, 1.0111, 1.0131,\n",
       "                      1.0159, 1.0104, 0.9989, 0.9965, 1.0178, 1.0166, 1.0171, 1.0070, 1.0193,\n",
       "                      1.0110, 1.0638, 1.0006, 1.0295, 1.0402, 1.0392, 1.0155, 1.0256, 1.0099,\n",
       "                      1.0213, 0.9983, 1.0242, 1.0162, 1.0235, 1.0145, 1.0135, 1.0127, 1.0180,\n",
       "                      1.0124, 1.0226, 1.0091, 1.0262, 1.0252, 0.9970, 0.9997, 1.0251, 1.0070,\n",
       "                      1.0141])),\n",
       "             ('ups.3.conv.4.bias',\n",
       "              tensor([-0.0260, -0.0104, -0.0235, -0.0252, -0.0131, -0.0297, -0.0179, -0.0071,\n",
       "                      -0.0157, -0.0355, -0.0127, -0.0269, -0.0210, -0.0330,  0.0017, -0.0877,\n",
       "                      -0.0146, -0.0265, -0.0191, -0.0091, -0.0213, -0.0251, -0.0199, -0.0281,\n",
       "                      -0.0050, -0.0449, -0.0315, -0.0267, -0.0189, -0.0182, -0.0232, -0.0385,\n",
       "                      -0.0631, -0.0258, -0.0357,  0.0038, -0.0367, -0.0368, -0.0314, -0.0573,\n",
       "                       0.0153, -0.0067, -0.0026, -0.0058, -0.0363, -0.0051, -0.0362, -0.0369,\n",
       "                      -0.0270, -0.0003, -0.0404,  0.0070, -0.0098, -0.0404, -0.0270,  0.0022,\n",
       "                      -0.0012, -0.0281, -0.0247, -0.0320, -0.0339,  0.0048, -0.0249, -0.0437])),\n",
       "             ('ups.3.conv.4.running_mean',\n",
       "              tensor([-0.1766, -0.3323, -0.1409,  0.0618, -0.0853,  0.0639,  0.0689, -0.1703,\n",
       "                       0.0359, -0.3273, -0.0292,  0.2448,  0.0532, -0.1939, -0.4139, -0.0925,\n",
       "                      -0.1433, -0.2980, -0.2588,  0.2684,  0.0326,  0.0284,  0.3701, -0.2991,\n",
       "                      -0.4476,  0.1527,  0.3196, -0.3953, -0.0089,  0.2981, -0.1463,  0.2834,\n",
       "                       0.0754, -0.0412,  0.4416, -0.2395,  0.1757,  0.1947,  0.0895, -0.0546,\n",
       "                      -0.1525, -0.3695, -0.5397, -0.0145,  0.2797, -0.3567, -0.0298, -0.0651,\n",
       "                      -0.0644, -0.1665,  0.0102, -0.2669,  0.0615,  0.2041,  0.2591, -0.2241,\n",
       "                      -0.2614,  0.0275, -0.0829,  0.3463, -0.2045,  0.0389,  0.2277,  0.1751])),\n",
       "             ('ups.3.conv.4.running_var',\n",
       "              tensor([0.1225, 0.1607, 0.1354, 0.1093, 0.1159, 0.1398, 0.1324, 0.1199, 0.1132,\n",
       "                      0.1230, 0.1785, 0.1214, 0.1140, 0.1287, 0.1356, 0.1075, 0.1442, 0.1178,\n",
       "                      0.1329, 0.1413, 0.1414, 0.0963, 0.1324, 0.1224, 0.1518, 0.0999, 0.1328,\n",
       "                      0.1340, 0.1264, 0.1128, 0.1381, 0.1276, 0.1159, 0.1084, 0.2326, 0.1746,\n",
       "                      0.1259, 0.1137, 0.1204, 0.1239, 0.1775, 0.1451, 0.1866, 0.1222, 0.1909,\n",
       "                      0.1277, 0.1328, 0.1257, 0.1304, 0.1367, 0.1309, 0.1440, 0.1288, 0.1383,\n",
       "                      0.1275, 0.1390, 0.1408, 0.1158, 0.1142, 0.1329, 0.1091, 0.1328, 0.1342,\n",
       "                      0.1064])),\n",
       "             ('ups.3.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('ups.4.weight',\n",
       "              tensor([[[[-6.8476e-02,  5.6301e-02],\n",
       "                        [ 1.4438e-02,  7.4539e-02]],\n",
       "              \n",
       "                       [[ 3.3268e-02, -4.6593e-02],\n",
       "                        [-4.7667e-03, -4.9549e-02]],\n",
       "              \n",
       "                       [[ 1.0482e-02,  3.5913e-03],\n",
       "                        [ 1.4645e-03,  1.8347e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.1977e-02, -3.3946e-02],\n",
       "                        [ 4.3555e-02,  6.6289e-02]],\n",
       "              \n",
       "                       [[-7.9581e-02, -1.3160e-02],\n",
       "                        [-6.6289e-02,  1.3419e-02]],\n",
       "              \n",
       "                       [[ 8.7344e-02, -2.6217e-02],\n",
       "                        [-3.4851e-02,  3.5819e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.6737e-02, -4.6467e-02],\n",
       "                        [-4.0064e-02,  4.9773e-02]],\n",
       "              \n",
       "                       [[ 8.8353e-02,  3.7515e-02],\n",
       "                        [-3.7483e-02, -5.3544e-02]],\n",
       "              \n",
       "                       [[ 8.3325e-03,  6.1390e-02],\n",
       "                        [ 9.1474e-02, -3.6423e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.6874e-02,  6.6779e-02],\n",
       "                        [ 3.9833e-02, -3.9156e-02]],\n",
       "              \n",
       "                       [[ 1.2006e-01,  5.4816e-02],\n",
       "                        [ 1.0221e-01,  2.0698e-02]],\n",
       "              \n",
       "                       [[-7.2836e-03, -1.8016e-02],\n",
       "                        [-4.1211e-02, -4.8754e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8027e-02,  7.8880e-02],\n",
       "                        [-3.2498e-03, -8.5067e-02]],\n",
       "              \n",
       "                       [[-3.1276e-02, -7.8261e-02],\n",
       "                        [-8.8313e-02, -2.4920e-02]],\n",
       "              \n",
       "                       [[ 5.4525e-02,  3.5371e-02],\n",
       "                        [ 7.7610e-02,  3.9705e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.5009e-02, -3.6624e-02],\n",
       "                        [-4.1249e-02, -2.6123e-02]],\n",
       "              \n",
       "                       [[-1.6856e-03,  7.5217e-02],\n",
       "                        [ 2.0818e-02,  2.6787e-02]],\n",
       "              \n",
       "                       [[ 3.6422e-03, -3.1586e-02],\n",
       "                        [-7.7828e-02,  4.1616e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.0938e-02,  9.5422e-03],\n",
       "                        [-1.8694e-02, -1.0958e-01]],\n",
       "              \n",
       "                       [[ 9.3127e-03, -1.6589e-02],\n",
       "                        [-6.6812e-02,  5.1962e-02]],\n",
       "              \n",
       "                       [[ 2.6134e-02,  7.4367e-02],\n",
       "                        [-5.6250e-02, -6.3537e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.0821e-02, -8.4553e-02],\n",
       "                        [ 2.6977e-02,  8.9112e-02]],\n",
       "              \n",
       "                       [[-2.7379e-02,  4.1617e-02],\n",
       "                        [ 6.7676e-02,  5.4787e-02]],\n",
       "              \n",
       "                       [[ 2.0774e-02,  4.3928e-02],\n",
       "                        [ 2.2666e-03, -1.5131e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9202e-02,  5.2041e-02],\n",
       "                        [ 3.1424e-02, -4.4531e-02]],\n",
       "              \n",
       "                       [[-2.8017e-02,  6.9206e-02],\n",
       "                        [ 2.8348e-02,  5.5032e-02]],\n",
       "              \n",
       "                       [[ 3.3113e-02,  2.0635e-02],\n",
       "                        [ 2.1610e-02, -1.8066e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 8.5482e-02,  4.8950e-02],\n",
       "                        [ 8.9648e-02,  3.5654e-02]],\n",
       "              \n",
       "                       [[-2.2026e-02, -1.3932e-02],\n",
       "                        [-2.9605e-02,  4.7894e-02]],\n",
       "              \n",
       "                       [[-5.7626e-02, -9.1381e-02],\n",
       "                        [-1.9645e-02, -4.8859e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3385e-02,  2.8111e-02],\n",
       "                        [ 8.5451e-02,  1.0988e-01]],\n",
       "              \n",
       "                       [[-6.2604e-02,  8.0143e-02],\n",
       "                        [-7.9749e-02, -4.2993e-02]],\n",
       "              \n",
       "                       [[-5.1607e-02,  7.9149e-02],\n",
       "                        [-4.8345e-06,  7.7166e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.0208e-01,  3.9122e-02],\n",
       "                        [ 5.1035e-02,  8.4526e-02]],\n",
       "              \n",
       "                       [[-6.5563e-02, -4.4138e-02],\n",
       "                        [-1.3794e-02,  4.6012e-02]],\n",
       "              \n",
       "                       [[ 3.0137e-02,  5.2440e-02],\n",
       "                        [ 8.1449e-04,  1.8529e-02]]]])),\n",
       "             ('ups.4.bias',\n",
       "              tensor([-0.0281, -0.0036,  0.0035,  0.0392, -0.0052,  0.0085,  0.0057, -0.0136,\n",
       "                       0.0664, -0.0337,  0.0605,  0.0028, -0.0463, -0.0181, -0.0203,  0.0196,\n",
       "                       0.0404, -0.0426, -0.0287,  0.0126,  0.0318, -0.0671, -0.0705,  0.0688,\n",
       "                       0.0840, -0.0517,  0.0869,  0.0081, -0.0131, -0.0938, -0.0300, -0.0407])),\n",
       "             ('ups.5.conv.0.weight',\n",
       "              tensor([[[[-0.0075, -0.0019,  0.0124],\n",
       "                        [-0.0299,  0.0341, -0.0121],\n",
       "                        [ 0.0177, -0.0162,  0.0231]],\n",
       "              \n",
       "                       [[-0.0141,  0.0123,  0.0086],\n",
       "                        [-0.0475, -0.0240,  0.0236],\n",
       "                        [ 0.0176,  0.0177, -0.0028]],\n",
       "              \n",
       "                       [[-0.0146,  0.0222, -0.0381],\n",
       "                        [ 0.0178, -0.0188, -0.0387],\n",
       "                        [ 0.0056, -0.0277, -0.0266]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0018, -0.0086,  0.0276],\n",
       "                        [ 0.0294, -0.0298,  0.0088],\n",
       "                        [ 0.0100, -0.0083, -0.0233]],\n",
       "              \n",
       "                       [[ 0.0063, -0.0096, -0.0133],\n",
       "                        [ 0.0094,  0.0006, -0.0081],\n",
       "                        [-0.0268, -0.0313, -0.0037]],\n",
       "              \n",
       "                       [[ 0.0078,  0.0313,  0.0437],\n",
       "                        [ 0.0038,  0.0358,  0.0276],\n",
       "                        [-0.0095,  0.0463, -0.0059]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0135, -0.0308, -0.0106],\n",
       "                        [ 0.0076,  0.0365,  0.0259],\n",
       "                        [ 0.0015, -0.0137,  0.0230]],\n",
       "              \n",
       "                       [[-0.0245,  0.0093,  0.0100],\n",
       "                        [ 0.0175, -0.0007,  0.0364],\n",
       "                        [-0.0267, -0.0056, -0.0025]],\n",
       "              \n",
       "                       [[-0.0240, -0.0284,  0.0061],\n",
       "                        [ 0.0127, -0.0272, -0.0210],\n",
       "                        [-0.0224,  0.0094, -0.0132]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0256,  0.0341,  0.0499],\n",
       "                        [ 0.0179, -0.0173, -0.0038],\n",
       "                        [ 0.0255, -0.0155, -0.0139]],\n",
       "              \n",
       "                       [[-0.0327, -0.0340,  0.0170],\n",
       "                        [-0.0364, -0.0333,  0.0267],\n",
       "                        [-0.0429, -0.0394,  0.0611]],\n",
       "              \n",
       "                       [[-0.0206,  0.0020, -0.0414],\n",
       "                        [ 0.0200,  0.0052, -0.0198],\n",
       "                        [ 0.0327,  0.0151,  0.0034]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0239,  0.0096, -0.0067],\n",
       "                        [-0.0152, -0.0305,  0.0230],\n",
       "                        [ 0.0103,  0.0004, -0.0059]],\n",
       "              \n",
       "                       [[ 0.0003, -0.0015, -0.0357],\n",
       "                        [-0.0243, -0.0330, -0.0413],\n",
       "                        [-0.0104,  0.0134,  0.0055]],\n",
       "              \n",
       "                       [[ 0.0055,  0.0162,  0.0273],\n",
       "                        [ 0.0219,  0.0200,  0.0190],\n",
       "                        [-0.0079, -0.0119,  0.0031]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0044,  0.0124,  0.0451],\n",
       "                        [ 0.0554,  0.0233,  0.0541],\n",
       "                        [-0.0327, -0.0109,  0.0218]],\n",
       "              \n",
       "                       [[-0.0377, -0.0405, -0.0310],\n",
       "                        [-0.0096, -0.0516, -0.0229],\n",
       "                        [-0.0534, -0.0280, -0.0280]],\n",
       "              \n",
       "                       [[-0.0084, -0.0301, -0.0067],\n",
       "                        [-0.0487, -0.0326,  0.0352],\n",
       "                        [-0.0321,  0.0254,  0.0410]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0090, -0.0239,  0.0195],\n",
       "                        [ 0.0280, -0.0266, -0.0365],\n",
       "                        [ 0.0383, -0.0230,  0.0011]],\n",
       "              \n",
       "                       [[ 0.0080,  0.0104,  0.0420],\n",
       "                        [-0.0061,  0.0221, -0.0339],\n",
       "                        [-0.0326,  0.0027, -0.0103]],\n",
       "              \n",
       "                       [[-0.0057, -0.0428,  0.0193],\n",
       "                        [-0.0425,  0.0166,  0.0144],\n",
       "                        [ 0.0041,  0.0229,  0.0138]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0678,  0.0348,  0.0288],\n",
       "                        [ 0.0019,  0.0478,  0.0089],\n",
       "                        [ 0.0018, -0.0116,  0.0013]],\n",
       "              \n",
       "                       [[ 0.0247,  0.0249, -0.0222],\n",
       "                        [ 0.0350, -0.0343,  0.0143],\n",
       "                        [-0.0489, -0.0066, -0.0247]],\n",
       "              \n",
       "                       [[-0.0095, -0.0407, -0.0100],\n",
       "                        [ 0.0050, -0.0465, -0.0383],\n",
       "                        [ 0.0159, -0.0431,  0.0056]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0116, -0.0589,  0.0377],\n",
       "                        [-0.0179, -0.0219,  0.0107],\n",
       "                        [ 0.0093,  0.0227,  0.0176]],\n",
       "              \n",
       "                       [[-0.0031,  0.0168,  0.0047],\n",
       "                        [ 0.0270, -0.0361, -0.0114],\n",
       "                        [ 0.0425,  0.0082,  0.0029]],\n",
       "              \n",
       "                       [[ 0.0038,  0.0200,  0.0044],\n",
       "                        [ 0.0088, -0.0107, -0.0378],\n",
       "                        [-0.0134, -0.0164,  0.0250]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0369,  0.0374,  0.0309],\n",
       "                        [-0.0034,  0.0268,  0.0391],\n",
       "                        [ 0.0114, -0.0159,  0.0230]],\n",
       "              \n",
       "                       [[-0.0331, -0.0445, -0.0478],\n",
       "                        [-0.0445, -0.0104, -0.0160],\n",
       "                        [-0.0499, -0.0263,  0.0194]],\n",
       "              \n",
       "                       [[ 0.0020,  0.0025,  0.0044],\n",
       "                        [ 0.0274, -0.0099, -0.0177],\n",
       "                        [ 0.0432,  0.0311, -0.0210]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0104, -0.0323,  0.0149],\n",
       "                        [ 0.0077,  0.0066,  0.0155],\n",
       "                        [-0.0101, -0.0064, -0.0208]],\n",
       "              \n",
       "                       [[ 0.0145, -0.0024,  0.0411],\n",
       "                        [ 0.0071,  0.0346, -0.0019],\n",
       "                        [ 0.0229,  0.0280,  0.0295]],\n",
       "              \n",
       "                       [[-0.0237,  0.0283, -0.0406],\n",
       "                        [-0.0499, -0.0160, -0.0207],\n",
       "                        [ 0.0084, -0.0234,  0.0286]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0132, -0.0507,  0.0154],\n",
       "                        [ 0.0214, -0.0224,  0.0128],\n",
       "                        [ 0.0020, -0.0427, -0.0101]],\n",
       "              \n",
       "                       [[-0.0246,  0.0241,  0.0435],\n",
       "                        [ 0.0078,  0.0278,  0.0297],\n",
       "                        [ 0.0392,  0.0401, -0.0021]],\n",
       "              \n",
       "                       [[-0.0465, -0.0074, -0.0050],\n",
       "                        [-0.0064, -0.0097,  0.0279],\n",
       "                        [-0.0111, -0.0127,  0.0257]]]])),\n",
       "             ('ups.5.conv.1.weight',\n",
       "              tensor([0.9900, 1.0264, 0.9920, 0.9904, 0.9942, 0.9735, 0.9925, 1.0074, 0.9632,\n",
       "                      1.0013, 1.0146, 1.0212, 1.0181, 1.0008, 0.9876, 1.0029, 0.9906, 0.9647,\n",
       "                      0.9730, 1.0155, 0.9726, 0.9949, 1.0059, 1.0050, 0.9921, 1.0592, 0.9944,\n",
       "                      1.0122, 0.9881, 0.9919, 1.0081, 0.9932])),\n",
       "             ('ups.5.conv.1.bias',\n",
       "              tensor([-0.0109, -0.0226, -0.0478, -0.0302,  0.0028, -0.0433, -0.0178, -0.0374,\n",
       "                      -0.0284,  0.0123,  0.0283,  0.0290, -0.0023,  0.0194, -0.0147, -0.0030,\n",
       "                      -0.0411, -0.0418, -0.0845, -0.0220, -0.0166, -0.0096, -0.0129, -0.0507,\n",
       "                      -0.0058,  0.0144, -0.0235, -0.0221, -0.0482, -0.0496, -0.0429, -0.0208])),\n",
       "             ('ups.5.conv.1.running_mean',\n",
       "              tensor([-3.1519e-02, -1.0974e-01, -1.5540e-01, -8.9658e-02,  2.2192e-02,\n",
       "                      -6.2669e-02, -1.2082e-01,  2.8423e-01, -5.5212e-02, -3.1401e-01,\n",
       "                      -3.7246e-01, -4.4210e-04,  9.4470e-02,  7.2847e-02, -3.5457e-01,\n",
       "                       2.8281e-02, -1.9272e-01, -1.3443e-01, -1.4900e-01,  3.1514e-01,\n",
       "                      -5.1537e-01, -2.6323e-02, -4.9612e-02, -1.0059e-01, -1.4013e-01,\n",
       "                      -6.4098e-02, -7.6767e-02, -2.8663e-01, -2.2415e-01,  2.3188e-01,\n",
       "                      -4.0025e-03, -1.7525e-01])),\n",
       "             ('ups.5.conv.1.running_var',\n",
       "              tensor([0.2710, 0.1308, 0.1880, 0.2077, 0.2612, 0.1583, 0.2647, 0.2436, 0.1826,\n",
       "                      0.1867, 0.2742, 0.3963, 0.2006, 0.1806, 0.1211, 0.1875, 0.1259, 0.2135,\n",
       "                      0.1466, 0.2976, 0.1298, 0.2342, 0.2430, 0.1272, 0.1412, 0.3178, 0.1298,\n",
       "                      0.1542, 0.1020, 0.2355, 0.1766, 0.2240])),\n",
       "             ('ups.5.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('ups.5.conv.3.weight',\n",
       "              tensor([[[[-0.0443, -0.0053, -0.0036],\n",
       "                        [-0.0128, -0.0091, -0.0712],\n",
       "                        [-0.0009,  0.0146,  0.0176]],\n",
       "              \n",
       "                       [[ 0.0369,  0.0457,  0.0103],\n",
       "                        [-0.0153, -0.0476,  0.0642],\n",
       "                        [-0.0216,  0.0178, -0.0017]],\n",
       "              \n",
       "                       [[ 0.0139, -0.0070,  0.0207],\n",
       "                        [ 0.0182, -0.0215, -0.0356],\n",
       "                        [ 0.0254,  0.0208,  0.0406]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0192,  0.0424,  0.0390],\n",
       "                        [ 0.0289,  0.0080,  0.0333],\n",
       "                        [ 0.0386,  0.0199, -0.0157]],\n",
       "              \n",
       "                       [[-0.0767,  0.0597,  0.0183],\n",
       "                        [ 0.0197,  0.1014,  0.0295],\n",
       "                        [ 0.0098,  0.0231, -0.0343]],\n",
       "              \n",
       "                       [[ 0.0721,  0.0251, -0.0323],\n",
       "                        [-0.0079, -0.0084, -0.0316],\n",
       "                        [ 0.0104, -0.0058,  0.0645]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0231,  0.0274, -0.0518],\n",
       "                        [ 0.0448, -0.0382,  0.0261],\n",
       "                        [ 0.0466, -0.0351, -0.0686]],\n",
       "              \n",
       "                       [[-0.0299, -0.0390,  0.0049],\n",
       "                        [-0.0384, -0.0514,  0.0486],\n",
       "                        [ 0.0164, -0.0293, -0.0170]],\n",
       "              \n",
       "                       [[ 0.0510, -0.0052, -0.0134],\n",
       "                        [-0.0528, -0.0636, -0.0131],\n",
       "                        [-0.0054,  0.0329, -0.0072]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0153,  0.0356,  0.0534],\n",
       "                        [-0.0418, -0.0311,  0.0154],\n",
       "                        [-0.0377, -0.0348,  0.0100]],\n",
       "              \n",
       "                       [[ 0.0213,  0.0326,  0.0380],\n",
       "                        [-0.0005,  0.0517, -0.0180],\n",
       "                        [-0.0268,  0.0026, -0.0345]],\n",
       "              \n",
       "                       [[ 0.0225,  0.0519,  0.0152],\n",
       "                        [-0.0222,  0.0061,  0.0416],\n",
       "                        [ 0.0029,  0.0101,  0.0499]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0203,  0.0495,  0.0131],\n",
       "                        [-0.0366,  0.0210,  0.0344],\n",
       "                        [ 0.0158, -0.0407,  0.0130]],\n",
       "              \n",
       "                       [[ 0.0434,  0.0456,  0.0047],\n",
       "                        [ 0.0071,  0.0024,  0.0239],\n",
       "                        [ 0.0484,  0.0087,  0.0383]],\n",
       "              \n",
       "                       [[-0.0298,  0.0067, -0.0505],\n",
       "                        [ 0.0213,  0.0539,  0.0140],\n",
       "                        [-0.0034,  0.0391, -0.0138]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0051, -0.0429,  0.0113],\n",
       "                        [-0.0717, -0.0472,  0.0314],\n",
       "                        [-0.0335, -0.0639, -0.0122]],\n",
       "              \n",
       "                       [[-0.0741, -0.0378,  0.0277],\n",
       "                        [-0.0005, -0.0049, -0.0502],\n",
       "                        [ 0.0104,  0.0345, -0.0315]],\n",
       "              \n",
       "                       [[ 0.0139,  0.0056,  0.0414],\n",
       "                        [ 0.0160,  0.0535, -0.0252],\n",
       "                        [ 0.0084, -0.0676, -0.0093]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0474,  0.0586,  0.0476],\n",
       "                        [-0.0031,  0.0058, -0.0628],\n",
       "                        [ 0.0388,  0.0222, -0.0383]],\n",
       "              \n",
       "                       [[-0.0241,  0.0053,  0.0080],\n",
       "                        [ 0.0649, -0.0395,  0.0256],\n",
       "                        [ 0.0718, -0.0334, -0.0599]],\n",
       "              \n",
       "                       [[ 0.0236, -0.0045, -0.0210],\n",
       "                        [-0.0545, -0.0280,  0.0042],\n",
       "                        [ 0.0170, -0.0101, -0.0466]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0387,  0.0291,  0.0059],\n",
       "                        [ 0.0076, -0.0081,  0.0493],\n",
       "                        [ 0.0423,  0.0102,  0.0461]],\n",
       "              \n",
       "                       [[-0.0600, -0.0265,  0.0211],\n",
       "                        [ 0.0437, -0.0007,  0.0231],\n",
       "                        [-0.0263, -0.0219, -0.0491]],\n",
       "              \n",
       "                       [[ 0.0280,  0.0459,  0.0178],\n",
       "                        [ 0.0106, -0.0188,  0.0189],\n",
       "                        [ 0.0497, -0.0031,  0.0169]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0136, -0.0379, -0.0473],\n",
       "                        [-0.0144, -0.0256, -0.0365],\n",
       "                        [-0.0429,  0.0173, -0.0309]],\n",
       "              \n",
       "                       [[-0.0332,  0.0397,  0.0072],\n",
       "                        [-0.0294, -0.0224,  0.0277],\n",
       "                        [-0.0302, -0.0373,  0.0141]],\n",
       "              \n",
       "                       [[ 0.0067, -0.0129, -0.0048],\n",
       "                        [ 0.0241,  0.0073,  0.0461],\n",
       "                        [ 0.0125,  0.0441,  0.0415]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0338,  0.0192, -0.0077],\n",
       "                        [ 0.0068, -0.0107, -0.0513],\n",
       "                        [ 0.0352,  0.0333,  0.0238]],\n",
       "              \n",
       "                       [[-0.0288,  0.0203, -0.0037],\n",
       "                        [ 0.0441, -0.0394, -0.0476],\n",
       "                        [ 0.0566,  0.0567,  0.0151]],\n",
       "              \n",
       "                       [[-0.0046, -0.0655,  0.0155],\n",
       "                        [-0.0050,  0.0104, -0.0246],\n",
       "                        [-0.0256,  0.0203,  0.0481]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0095,  0.0148,  0.0487],\n",
       "                        [ 0.0211, -0.0117, -0.0559],\n",
       "                        [-0.0209, -0.0144, -0.0268]],\n",
       "              \n",
       "                       [[ 0.0227, -0.0124,  0.0132],\n",
       "                        [ 0.0411, -0.0061, -0.0175],\n",
       "                        [-0.0496,  0.0196,  0.0318]],\n",
       "              \n",
       "                       [[ 0.0181,  0.0603,  0.0099],\n",
       "                        [-0.0277,  0.0301,  0.0448],\n",
       "                        [ 0.0401, -0.0425,  0.0248]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0113, -0.0117,  0.0049],\n",
       "                        [ 0.0248, -0.0501,  0.0308],\n",
       "                        [-0.0225, -0.0444, -0.0045]],\n",
       "              \n",
       "                       [[ 0.0181,  0.0364, -0.0098],\n",
       "                        [ 0.0389,  0.0622, -0.0171],\n",
       "                        [ 0.0094,  0.0192,  0.0035]],\n",
       "              \n",
       "                       [[ 0.0138,  0.0038, -0.0568],\n",
       "                        [-0.0110,  0.0177, -0.0175],\n",
       "                        [-0.0193,  0.0683, -0.0147]]]])),\n",
       "             ('ups.5.conv.4.weight',\n",
       "              tensor([1.0336, 1.0267, 1.0417, 1.0007, 1.0377, 1.0621, 0.9822, 1.0502, 1.0237,\n",
       "                      1.0151, 1.0305, 1.0336, 1.0283, 1.0563, 1.0464, 1.0335, 1.0100, 1.0390,\n",
       "                      1.0688, 1.0645, 1.0725, 1.0100, 0.9953, 1.0059, 1.0145, 1.0049, 1.0324,\n",
       "                      1.0182, 1.0179, 1.0150, 1.0184, 1.0076])),\n",
       "             ('ups.5.conv.4.bias',\n",
       "              tensor([ 0.0025, -0.0053,  0.0065, -0.0149,  0.0061,  0.0271, -0.0231,  0.0244,\n",
       "                      -0.0033, -0.0179, -0.0393,  0.0024, -0.0349,  0.0159,  0.0301, -0.0067,\n",
       "                      -0.0115, -0.0341,  0.0082,  0.0353,  0.0451, -0.0127, -0.0422, -0.0162,\n",
       "                       0.0085, -0.0433,  0.0254, -0.0473, -0.0224, -0.0066, -0.0303,  0.0025])),\n",
       "             ('ups.5.conv.4.running_mean',\n",
       "              tensor([ 0.3172,  0.0916, -0.1540, -0.5535,  0.0682,  0.0457, -0.3420,  0.0209,\n",
       "                      -0.0330, -0.5687,  0.1131, -0.1812, -0.2688,  0.6721,  0.1109, -0.2577,\n",
       "                       0.2864, -0.6364,  0.8863,  0.0237, -0.1844, -0.6501, -0.3199, -0.4835,\n",
       "                       0.3796, -0.1164,  0.3189,  0.4195, -0.4031, -0.4032, -0.0671,  0.3587])),\n",
       "             ('ups.5.conv.4.running_var',\n",
       "              tensor([0.2330, 0.3242, 0.1226, 0.1817, 0.2068, 0.3146, 0.1075, 0.1996, 0.4130,\n",
       "                      0.1438, 0.1406, 0.1778, 0.1768, 0.4940, 0.3434, 0.1807, 0.3401, 0.1539,\n",
       "                      0.2536, 0.4909, 0.2957, 0.1608, 0.1162, 0.1545, 0.3206, 0.1360, 0.3257,\n",
       "                      0.1243, 0.1172, 0.1121, 0.1273, 0.3995])),\n",
       "             ('ups.5.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('ups.6.weight',\n",
       "              tensor([[[[ 0.0224,  0.0025],\n",
       "                        [-0.0668, -0.0134]],\n",
       "              \n",
       "                       [[-0.0007, -0.0384],\n",
       "                        [-0.0388, -0.0546]],\n",
       "              \n",
       "                       [[-0.0442, -0.0228],\n",
       "                        [ 0.0125,  0.0580]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0297,  0.1587],\n",
       "                        [ 0.0032,  0.0779]],\n",
       "              \n",
       "                       [[-0.1047, -0.1176],\n",
       "                        [ 0.0268, -0.0593]],\n",
       "              \n",
       "                       [[ 0.0475, -0.1272],\n",
       "                        [-0.0853, -0.1106]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0237, -0.0532],\n",
       "                        [-0.0315,  0.0656]],\n",
       "              \n",
       "                       [[ 0.0825,  0.1014],\n",
       "                        [-0.0940,  0.0020]],\n",
       "              \n",
       "                       [[-0.0361,  0.0668],\n",
       "                        [-0.0878, -0.1339]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0440,  0.1018],\n",
       "                        [ 0.0716,  0.0285]],\n",
       "              \n",
       "                       [[-0.1043, -0.0199],\n",
       "                        [ 0.0149,  0.0959]],\n",
       "              \n",
       "                       [[-0.1648, -0.1529],\n",
       "                        [ 0.0359, -0.1206]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0249, -0.1358],\n",
       "                        [-0.1202, -0.0409]],\n",
       "              \n",
       "                       [[ 0.0677,  0.1495],\n",
       "                        [-0.0300,  0.0297]],\n",
       "              \n",
       "                       [[-0.0108, -0.0401],\n",
       "                        [-0.0860, -0.0362]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0138,  0.1681],\n",
       "                        [ 0.0563,  0.0633]],\n",
       "              \n",
       "                       [[-0.0832, -0.0239],\n",
       "                        [-0.0454, -0.0569]],\n",
       "              \n",
       "                       [[-0.0197,  0.0097],\n",
       "                        [ 0.0821, -0.0431]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0293,  0.0420],\n",
       "                        [-0.0059,  0.0322]],\n",
       "              \n",
       "                       [[ 0.0625, -0.0423],\n",
       "                        [ 0.1069,  0.0972]],\n",
       "              \n",
       "                       [[-0.1368, -0.1402],\n",
       "                        [-0.0389, -0.1191]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0707,  0.0676],\n",
       "                        [ 0.0347,  0.0283]],\n",
       "              \n",
       "                       [[ 0.0851,  0.0630],\n",
       "                        [-0.0082,  0.0355]],\n",
       "              \n",
       "                       [[ 0.0413, -0.0949],\n",
       "                        [-0.1155,  0.0085]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0206,  0.0315],\n",
       "                        [-0.0275, -0.0433]],\n",
       "              \n",
       "                       [[-0.0562, -0.1099],\n",
       "                        [-0.0338, -0.0446]],\n",
       "              \n",
       "                       [[-0.0087,  0.1507],\n",
       "                        [ 0.1053,  0.1350]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0115, -0.0285],\n",
       "                        [-0.1097, -0.1189]],\n",
       "              \n",
       "                       [[ 0.1150,  0.0412],\n",
       "                        [ 0.0628,  0.0260]],\n",
       "              \n",
       "                       [[ 0.0893, -0.0686],\n",
       "                        [-0.0998,  0.0557]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0061, -0.0841],\n",
       "                        [ 0.0488, -0.0513]],\n",
       "              \n",
       "                       [[ 0.0363, -0.0201],\n",
       "                        [-0.1043,  0.0236]],\n",
       "              \n",
       "                       [[ 0.0209,  0.0762],\n",
       "                        [-0.0213, -0.0168]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0095,  0.0711],\n",
       "                        [ 0.1519,  0.0880]],\n",
       "              \n",
       "                       [[ 0.0724, -0.0259],\n",
       "                        [-0.0624, -0.0326]],\n",
       "              \n",
       "                       [[ 0.0431, -0.1220],\n",
       "                        [ 0.0298, -0.0536]]]])),\n",
       "             ('ups.6.bias',\n",
       "              tensor([-0.1047, -0.0229, -0.0989,  0.0301, -0.0379,  0.0596, -0.1049,  0.0118,\n",
       "                      -0.0105,  0.1214,  0.0294, -0.1139,  0.0846,  0.0709,  0.0109, -0.0105])),\n",
       "             ('ups.7.conv.0.weight',\n",
       "              tensor([[[[-0.0420, -0.0029, -0.0677],\n",
       "                        [ 0.0341,  0.0097, -0.0185],\n",
       "                        [ 0.0047, -0.0012, -0.0601]],\n",
       "              \n",
       "                       [[ 0.0319,  0.0224,  0.0051],\n",
       "                        [-0.0138, -0.0307, -0.0054],\n",
       "                        [-0.0358, -0.0563,  0.0145]],\n",
       "              \n",
       "                       [[ 0.0008,  0.0270, -0.0316],\n",
       "                        [-0.0183, -0.0616,  0.0471],\n",
       "                        [ 0.0180, -0.0569, -0.0141]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0562, -0.0084,  0.0246],\n",
       "                        [ 0.0520, -0.0700,  0.0025],\n",
       "                        [-0.0335, -0.0090, -0.0200]],\n",
       "              \n",
       "                       [[ 0.0307, -0.0003, -0.0497],\n",
       "                        [-0.0187, -0.0163, -0.0338],\n",
       "                        [ 0.0185,  0.0317, -0.0199]],\n",
       "              \n",
       "                       [[-0.0582,  0.0155,  0.0247],\n",
       "                        [-0.0370, -0.0352, -0.0312],\n",
       "                        [ 0.0205, -0.0861, -0.0247]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0244, -0.0258, -0.0573],\n",
       "                        [ 0.0497, -0.0279, -0.0227],\n",
       "                        [-0.0039, -0.0415, -0.0495]],\n",
       "              \n",
       "                       [[-0.0289,  0.0188,  0.0441],\n",
       "                        [-0.0212, -0.0689, -0.0709],\n",
       "                        [ 0.0342, -0.0410, -0.0388]],\n",
       "              \n",
       "                       [[ 0.0078, -0.0273, -0.0140],\n",
       "                        [-0.0359, -0.0418, -0.0473],\n",
       "                        [ 0.0026, -0.0405,  0.0003]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0482,  0.0268, -0.0290],\n",
       "                        [ 0.0745,  0.0489,  0.0403],\n",
       "                        [ 0.0099,  0.0134, -0.0216]],\n",
       "              \n",
       "                       [[ 0.0386,  0.0106,  0.0271],\n",
       "                        [ 0.0469,  0.0211, -0.0618],\n",
       "                        [ 0.0411, -0.0284, -0.0658]],\n",
       "              \n",
       "                       [[ 0.0142, -0.0383, -0.0131],\n",
       "                        [-0.0486, -0.0100, -0.0575],\n",
       "                        [-0.0249, -0.0450, -0.0572]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0204, -0.0417,  0.0395],\n",
       "                        [-0.0222, -0.0021, -0.0074],\n",
       "                        [-0.0320, -0.0299,  0.0257]],\n",
       "              \n",
       "                       [[-0.0138, -0.0175,  0.0182],\n",
       "                        [-0.0052,  0.0156,  0.0126],\n",
       "                        [-0.0218, -0.0118,  0.0139]],\n",
       "              \n",
       "                       [[-0.0397, -0.0523, -0.0462],\n",
       "                        [ 0.0168,  0.0039, -0.0476],\n",
       "                        [ 0.0273, -0.0641, -0.0031]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0268,  0.0116,  0.0181],\n",
       "                        [ 0.0450,  0.0356, -0.0642],\n",
       "                        [-0.0138,  0.0244,  0.0266]],\n",
       "              \n",
       "                       [[ 0.0537,  0.0581, -0.0424],\n",
       "                        [ 0.0499,  0.0097,  0.0366],\n",
       "                        [-0.0376, -0.0052,  0.0606]],\n",
       "              \n",
       "                       [[-0.0401, -0.0150, -0.0639],\n",
       "                        [-0.0015, -0.0212, -0.0028],\n",
       "                        [-0.0283,  0.0138,  0.0014]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0624, -0.0190, -0.0298],\n",
       "                        [-0.0051,  0.0435,  0.0400],\n",
       "                        [-0.0075,  0.0153, -0.0422]],\n",
       "              \n",
       "                       [[ 0.0175, -0.0110,  0.0162],\n",
       "                        [-0.0554,  0.0234,  0.0610],\n",
       "                        [ 0.0346,  0.0375, -0.0486]],\n",
       "              \n",
       "                       [[ 0.0440,  0.0091, -0.0146],\n",
       "                        [ 0.0233, -0.0379, -0.0234],\n",
       "                        [ 0.0301, -0.0424,  0.0415]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0376,  0.0240,  0.0031],\n",
       "                        [-0.0628, -0.0761, -0.0245],\n",
       "                        [-0.0547, -0.0259,  0.0320]],\n",
       "              \n",
       "                       [[ 0.0492,  0.0431,  0.0539],\n",
       "                        [ 0.0435,  0.0312,  0.0406],\n",
       "                        [ 0.0546,  0.0035,  0.0068]],\n",
       "              \n",
       "                       [[ 0.0613,  0.0291,  0.0088],\n",
       "                        [ 0.0277,  0.0389, -0.0068],\n",
       "                        [-0.0259,  0.0105,  0.0443]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0471, -0.0197, -0.0303],\n",
       "                        [ 0.0436, -0.0339,  0.0534],\n",
       "                        [ 0.0450,  0.0343, -0.0100]],\n",
       "              \n",
       "                       [[-0.0193, -0.0050, -0.0412],\n",
       "                        [-0.0135, -0.0516, -0.0061],\n",
       "                        [ 0.0108, -0.0329,  0.0479]],\n",
       "              \n",
       "                       [[-0.0021,  0.0084,  0.0084],\n",
       "                        [ 0.0229, -0.0162,  0.0251],\n",
       "                        [ 0.0153, -0.0061,  0.0442]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0738,  0.0061,  0.0156],\n",
       "                        [-0.0575, -0.0758, -0.0317],\n",
       "                        [-0.0188, -0.0476, -0.0238]],\n",
       "              \n",
       "                       [[ 0.0519, -0.0668, -0.0143],\n",
       "                        [-0.0250,  0.0177, -0.0283],\n",
       "                        [-0.0424,  0.0296, -0.0411]],\n",
       "              \n",
       "                       [[-0.0300, -0.0202, -0.0277],\n",
       "                        [-0.0460, -0.0593, -0.0197],\n",
       "                        [ 0.0242,  0.0322,  0.0181]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0299,  0.0373, -0.0165],\n",
       "                        [ 0.0060, -0.0059, -0.0469],\n",
       "                        [-0.0438, -0.0422, -0.0361]],\n",
       "              \n",
       "                       [[-0.0089, -0.0148, -0.0149],\n",
       "                        [ 0.0151, -0.0073,  0.0246],\n",
       "                        [ 0.0328,  0.0058, -0.0229]],\n",
       "              \n",
       "                       [[-0.0197,  0.0045,  0.0390],\n",
       "                        [ 0.0139,  0.0102,  0.0142],\n",
       "                        [-0.0384,  0.0053, -0.0328]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0234, -0.0210,  0.0236],\n",
       "                        [ 0.0149, -0.0135,  0.0348],\n",
       "                        [ 0.0337,  0.0374, -0.0168]],\n",
       "              \n",
       "                       [[ 0.0126,  0.0255, -0.0488],\n",
       "                        [ 0.0518,  0.0049,  0.0262],\n",
       "                        [ 0.0440, -0.0415,  0.0294]],\n",
       "              \n",
       "                       [[-0.0449, -0.0644, -0.0528],\n",
       "                        [-0.0702, -0.0670, -0.0382],\n",
       "                        [ 0.0337, -0.0459,  0.0374]]]])),\n",
       "             ('ups.7.conv.1.weight',\n",
       "              tensor([0.9904, 0.9833, 0.9938, 1.0226, 1.1430, 0.9755, 1.0064, 0.9722, 0.9769,\n",
       "                      1.0013, 0.9827, 1.0146, 1.0185, 0.9886, 0.9903, 1.0026])),\n",
       "             ('ups.7.conv.1.bias',\n",
       "              tensor([ 0.0042, -0.0042,  0.0435, -0.0032,  0.0478, -0.0125,  0.0138,  0.0094,\n",
       "                      -0.0073,  0.0113,  0.0087,  0.0502,  0.0121,  0.0206,  0.0181,  0.0113])),\n",
       "             ('ups.7.conv.1.running_mean',\n",
       "              tensor([-0.4146, -0.0697, -0.2373, -0.0551, -0.1426, -0.1348, -0.3891, -0.1218,\n",
       "                      -0.3529, -0.3723, -0.4009, -0.2877, -0.2788, -0.2473, -0.4038, -0.0539])),\n",
       "             ('ups.7.conv.1.running_var',\n",
       "              tensor([0.8499, 0.5683, 0.8060, 0.2764, 0.3058, 0.3981, 0.7013, 0.5320, 0.8219,\n",
       "                      0.5945, 0.9951, 0.5646, 0.3298, 0.6485, 0.7034, 0.4671])),\n",
       "             ('ups.7.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('ups.7.conv.3.weight',\n",
       "              tensor([[[[-0.0469,  0.0116, -0.0147],\n",
       "                        [-0.0747, -0.0407, -0.0421],\n",
       "                        [ 0.0214, -0.0911, -0.0110]],\n",
       "              \n",
       "                       [[ 0.0086, -0.0593,  0.0279],\n",
       "                        [-0.0780,  0.0449,  0.0068],\n",
       "                        [ 0.0131, -0.0019, -0.0567]],\n",
       "              \n",
       "                       [[ 0.0291, -0.0216,  0.0298],\n",
       "                        [-0.0476,  0.0195,  0.0078],\n",
       "                        [ 0.0011,  0.0158,  0.0181]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0457,  0.0536,  0.0573],\n",
       "                        [ 0.0075, -0.0262,  0.0513],\n",
       "                        [-0.0338,  0.0179, -0.0017]],\n",
       "              \n",
       "                       [[ 0.0117, -0.0546, -0.0493],\n",
       "                        [-0.0324, -0.0918,  0.0631],\n",
       "                        [-0.0523, -0.0540,  0.0405]],\n",
       "              \n",
       "                       [[ 0.0101,  0.0108,  0.0137],\n",
       "                        [-0.0581, -0.1027, -0.0209],\n",
       "                        [ 0.0749, -0.0646,  0.0430]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0167, -0.0263,  0.0023],\n",
       "                        [-0.0195, -0.0021, -0.0700],\n",
       "                        [-0.0552,  0.0865,  0.0442]],\n",
       "              \n",
       "                       [[ 0.0108, -0.0041, -0.0854],\n",
       "                        [ 0.0373,  0.0765, -0.0915],\n",
       "                        [-0.0668, -0.0217, -0.0113]],\n",
       "              \n",
       "                       [[ 0.0035,  0.0609, -0.0295],\n",
       "                        [-0.0089,  0.0895, -0.0380],\n",
       "                        [ 0.0119,  0.0688,  0.0044]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0298,  0.0513, -0.0291],\n",
       "                        [ 0.0119, -0.0164, -0.0866],\n",
       "                        [-0.0005, -0.0649, -0.0755]],\n",
       "              \n",
       "                       [[-0.0617,  0.0213,  0.0250],\n",
       "                        [ 0.0737,  0.0321,  0.0706],\n",
       "                        [ 0.0624, -0.0031,  0.0495]],\n",
       "              \n",
       "                       [[ 0.0306,  0.0125,  0.0594],\n",
       "                        [ 0.0594,  0.0746, -0.0022],\n",
       "                        [ 0.0630, -0.0346,  0.0110]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0477, -0.0743,  0.0728],\n",
       "                        [-0.0654, -0.0911, -0.0366],\n",
       "                        [ 0.0675, -0.0885, -0.0540]],\n",
       "              \n",
       "                       [[-0.1128, -0.0075,  0.0573],\n",
       "                        [-0.0489, -0.0818,  0.0364],\n",
       "                        [-0.0182,  0.0893,  0.0312]],\n",
       "              \n",
       "                       [[ 0.0212,  0.0297, -0.0304],\n",
       "                        [ 0.0325, -0.0194,  0.0578],\n",
       "                        [-0.0252, -0.0766, -0.0625]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0060,  0.0245,  0.0798],\n",
       "                        [-0.0355, -0.0294, -0.0003],\n",
       "                        [-0.0455,  0.0684,  0.0705]],\n",
       "              \n",
       "                       [[ 0.0424,  0.0276, -0.0359],\n",
       "                        [-0.0374, -0.0190,  0.0606],\n",
       "                        [ 0.0376,  0.0340,  0.0241]],\n",
       "              \n",
       "                       [[ 0.0384, -0.0927, -0.0068],\n",
       "                        [-0.0053, -0.0796, -0.0475],\n",
       "                        [-0.0206,  0.0370,  0.0155]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0294, -0.0572,  0.0567],\n",
       "                        [ 0.0524, -0.0471, -0.0879],\n",
       "                        [ 0.0468, -0.0844,  0.0492]],\n",
       "              \n",
       "                       [[-0.0674,  0.0198,  0.0248],\n",
       "                        [-0.0267,  0.0343, -0.0111],\n",
       "                        [-0.0387, -0.0445,  0.0683]],\n",
       "              \n",
       "                       [[-0.0189, -0.0421,  0.0068],\n",
       "                        [-0.0431, -0.0100, -0.0210],\n",
       "                        [-0.0132,  0.0622,  0.0322]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0826,  0.0901,  0.0028],\n",
       "                        [ 0.0110,  0.0971,  0.0331],\n",
       "                        [ 0.0134, -0.0189,  0.0234]],\n",
       "              \n",
       "                       [[-0.0728,  0.0455, -0.0406],\n",
       "                        [-0.0853, -0.0257,  0.0379],\n",
       "                        [ 0.0258, -0.0051, -0.0648]],\n",
       "              \n",
       "                       [[ 0.0026, -0.0714, -0.0682],\n",
       "                        [-0.0691,  0.0338, -0.0551],\n",
       "                        [ 0.0308, -0.0233,  0.0381]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0809,  0.0353,  0.0242],\n",
       "                        [-0.0520, -0.0634, -0.0099],\n",
       "                        [ 0.0358, -0.0308, -0.0390]],\n",
       "              \n",
       "                       [[-0.0305, -0.0701,  0.0895],\n",
       "                        [ 0.0357, -0.0737, -0.0477],\n",
       "                        [ 0.0564,  0.0427,  0.0358]],\n",
       "              \n",
       "                       [[ 0.0197, -0.0132,  0.0814],\n",
       "                        [-0.0047,  0.0009, -0.0381],\n",
       "                        [-0.0143, -0.0343,  0.0390]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0407,  0.0128,  0.0867],\n",
       "                        [ 0.0717,  0.0834, -0.0042],\n",
       "                        [ 0.0829,  0.0065, -0.0433]],\n",
       "              \n",
       "                       [[ 0.0522, -0.0186, -0.0474],\n",
       "                        [-0.0095,  0.0481, -0.0281],\n",
       "                        [-0.0307, -0.0615, -0.0500]],\n",
       "              \n",
       "                       [[ 0.0172, -0.0792,  0.0021],\n",
       "                        [-0.0184,  0.0163, -0.0500],\n",
       "                        [-0.0496, -0.0579,  0.0607]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0300,  0.0379,  0.0409],\n",
       "                        [ 0.0315, -0.1261,  0.0334],\n",
       "                        [ 0.0257,  0.0160, -0.0670]],\n",
       "              \n",
       "                       [[-0.0299,  0.0522,  0.0272],\n",
       "                        [-0.0339,  0.0270, -0.0052],\n",
       "                        [-0.0588,  0.0457,  0.0338]],\n",
       "              \n",
       "                       [[-0.0585,  0.0506, -0.0453],\n",
       "                        [-0.0675, -0.1027, -0.0016],\n",
       "                        [ 0.0064, -0.0307,  0.0196]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0624,  0.0101,  0.0511],\n",
       "                        [ 0.0247,  0.0597,  0.0515],\n",
       "                        [-0.0222, -0.0649,  0.0004]],\n",
       "              \n",
       "                       [[ 0.0302, -0.0239, -0.0672],\n",
       "                        [-0.0783, -0.0562, -0.0066],\n",
       "                        [ 0.0705,  0.0569, -0.0418]],\n",
       "              \n",
       "                       [[ 0.0641, -0.1123, -0.0080],\n",
       "                        [ 0.0251, -0.1018, -0.0437],\n",
       "                        [-0.0720, -0.0253, -0.0056]]]])),\n",
       "             ('ups.7.conv.4.weight',\n",
       "              tensor([1.3019, 1.2746, 1.3277, 1.2642, 1.3090, 1.2828, 1.2773, 1.2447, 1.2468,\n",
       "                      1.2441, 1.2604, 1.2933, 1.2214, 1.2673, 1.2416, 1.2488])),\n",
       "             ('ups.7.conv.4.bias',\n",
       "              tensor([0.2986, 0.2790, 0.3236, 0.2697, 0.3157, 0.2917, 0.2837, 0.2419, 0.2527,\n",
       "                      0.2441, 0.2734, 0.2894, 0.2220, 0.2663, 0.2446, 0.2511])),\n",
       "             ('ups.7.conv.4.running_mean',\n",
       "              tensor([-0.3050,  0.2782, -0.3174,  0.1270,  0.0847,  0.1312, -0.2580, -0.2928,\n",
       "                       0.0756, -0.2513,  0.1504,  0.1288, -0.2355, -0.2387, -0.3236, -0.3169])),\n",
       "             ('ups.7.conv.4.running_var',\n",
       "              tensor([0.1817, 0.4182, 0.2786, 0.2550, 0.2128, 0.2743, 0.2215, 0.1864, 0.1749,\n",
       "                      0.2159, 0.3706, 0.2355, 0.1785, 0.1517, 0.2152, 0.2896])),\n",
       "             ('ups.7.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('downs.0.conv.0.weight',\n",
       "              tensor([[[[ 0.1252,  0.1283, -0.0127],\n",
       "                        [-0.1369, -0.0079,  0.2321],\n",
       "                        [-0.0804,  0.0414, -0.0070]],\n",
       "              \n",
       "                       [[-0.2015,  0.1146,  0.1554],\n",
       "                        [ 0.0406,  0.0844,  0.1551],\n",
       "                        [-0.1196,  0.2045, -0.0636]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0591,  0.1993,  0.2148],\n",
       "                        [ 0.0801, -0.0199, -0.0950],\n",
       "                        [ 0.1312,  0.2206,  0.0207]],\n",
       "              \n",
       "                       [[ 0.2382,  0.0697,  0.0276],\n",
       "                        [ 0.0056, -0.1782, -0.1542],\n",
       "                        [ 0.0721, -0.0496,  0.0399]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2148,  0.0577, -0.1718],\n",
       "                        [-0.2373, -0.0478, -0.2253],\n",
       "                        [ 0.0966,  0.1251, -0.0235]],\n",
       "              \n",
       "                       [[ 0.1123, -0.0244,  0.1722],\n",
       "                        [-0.0589,  0.0797, -0.0887],\n",
       "                        [-0.1303,  0.2052,  0.0627]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2230,  0.0944,  0.0412],\n",
       "                        [ 0.0447,  0.0101,  0.0672],\n",
       "                        [ 0.0448, -0.1145,  0.1111]],\n",
       "              \n",
       "                       [[-0.1782, -0.1504, -0.1967],\n",
       "                        [-0.1158,  0.0857,  0.1971],\n",
       "                        [-0.1148,  0.1092,  0.1117]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0381,  0.0048,  0.1146],\n",
       "                        [ 0.1979,  0.1626,  0.1414],\n",
       "                        [ 0.0800, -0.1524,  0.0417]],\n",
       "              \n",
       "                       [[ 0.1229,  0.0422,  0.0190],\n",
       "                        [-0.0536, -0.0713,  0.1789],\n",
       "                        [-0.1859, -0.0352, -0.0701]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0376,  0.2347, -0.1894],\n",
       "                        [-0.0794,  0.1901, -0.0692],\n",
       "                        [ 0.0770, -0.1628,  0.1932]],\n",
       "              \n",
       "                       [[ 0.2078,  0.0689,  0.1387],\n",
       "                        [ 0.1302,  0.0013,  0.0266],\n",
       "                        [ 0.1259,  0.2383, -0.0762]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0879,  0.0041, -0.1467],\n",
       "                        [-0.1398,  0.1427,  0.2494],\n",
       "                        [ 0.1524, -0.1715,  0.2239]],\n",
       "              \n",
       "                       [[-0.1171,  0.1895,  0.1688],\n",
       "                        [ 0.0318, -0.0471, -0.2108],\n",
       "                        [ 0.1163,  0.1631, -0.2143]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2103, -0.0061,  0.0235],\n",
       "                        [-0.0619, -0.0726,  0.1644],\n",
       "                        [ 0.1755, -0.2328, -0.0658]],\n",
       "              \n",
       "                       [[-0.0470, -0.0700,  0.0908],\n",
       "                        [ 0.1267,  0.0726,  0.1975],\n",
       "                        [-0.0391,  0.1884,  0.0868]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0099, -0.0610, -0.1134],\n",
       "                        [ 0.1778,  0.1275, -0.0303],\n",
       "                        [ 0.1930,  0.2262,  0.1756]],\n",
       "              \n",
       "                       [[-0.0517, -0.2415,  0.0785],\n",
       "                        [ 0.0460, -0.0160, -0.0567],\n",
       "                        [-0.0260, -0.0642,  0.1177]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1173, -0.1679,  0.1820],\n",
       "                        [ 0.0489, -0.2145, -0.0235],\n",
       "                        [-0.1979, -0.2325,  0.1680]],\n",
       "              \n",
       "                       [[-0.2800, -0.2000,  0.0803],\n",
       "                        [-0.1760,  0.1415,  0.0233],\n",
       "                        [-0.1535,  0.0535,  0.0490]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1474, -0.0696,  0.0275],\n",
       "                        [-0.1560,  0.1484,  0.2174],\n",
       "                        [ 0.1565, -0.1736, -0.2300]],\n",
       "              \n",
       "                       [[-0.1640,  0.1482,  0.2218],\n",
       "                        [ 0.2347, -0.1261, -0.1097],\n",
       "                        [ 0.0790,  0.0524, -0.0927]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0276,  0.2108, -0.0212],\n",
       "                        [-0.2374,  0.0215,  0.0632],\n",
       "                        [-0.1253,  0.0057, -0.0843]],\n",
       "              \n",
       "                       [[ 0.0136,  0.2134,  0.1104],\n",
       "                        [-0.2560, -0.0283,  0.2053],\n",
       "                        [ 0.0908, -0.1745, -0.2398]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1895,  0.1688, -0.1104],\n",
       "                        [ 0.0239,  0.1226, -0.0520],\n",
       "                        [ 0.0697,  0.1210, -0.0310]],\n",
       "              \n",
       "                       [[-0.1428, -0.1035,  0.0883],\n",
       "                        [ 0.2208,  0.1143,  0.1089],\n",
       "                        [-0.0057, -0.1547,  0.0353]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2208, -0.1670, -0.0437],\n",
       "                        [ 0.0119, -0.0916, -0.0631],\n",
       "                        [ 0.0563,  0.2250,  0.1087]],\n",
       "              \n",
       "                       [[-0.0806, -0.1486,  0.0850],\n",
       "                        [ 0.1019, -0.1269, -0.2046],\n",
       "                        [-0.0770,  0.1655, -0.2159]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0909, -0.0100,  0.1242],\n",
       "                        [-0.0014,  0.0235, -0.1692],\n",
       "                        [-0.1632,  0.1911, -0.1975]],\n",
       "              \n",
       "                       [[ 0.2516, -0.1825, -0.0489],\n",
       "                        [-0.0479,  0.1071, -0.0472],\n",
       "                        [ 0.0415, -0.0735, -0.1071]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0551, -0.1322,  0.0049],\n",
       "                        [-0.1844,  0.1505, -0.0879],\n",
       "                        [ 0.0251, -0.1340,  0.0377]],\n",
       "              \n",
       "                       [[-0.2052, -0.2643,  0.0403],\n",
       "                        [ 0.0286, -0.0185, -0.0303],\n",
       "                        [-0.2096, -0.0272, -0.1948]]]])),\n",
       "             ('downs.0.conv.1.weight',\n",
       "              tensor([1.0039, 0.9962, 1.0045, 0.9893, 1.0098, 0.9953, 0.9977, 0.9920, 1.0064,\n",
       "                      1.0184, 0.9921, 1.0098, 0.9933, 0.9893, 0.9928, 1.0112])),\n",
       "             ('downs.0.conv.1.bias',\n",
       "              tensor([ 0.0139, -0.0004, -0.0052,  0.0021,  0.0084,  0.0007, -0.0120,  0.0136,\n",
       "                       0.0051,  0.0175, -0.0004,  0.0170,  0.0011,  0.0171,  0.0103, -0.0021])),\n",
       "             ('downs.0.conv.1.running_mean',\n",
       "              tensor([ 0.1372,  0.3918, -0.3103,  0.0373,  0.2690,  0.1140,  0.1943,  0.0619,\n",
       "                       0.3433, -0.1612,  0.0338, -0.0922,  0.2456,  0.1217, -0.0524, -0.1824])),\n",
       "             ('downs.0.conv.1.running_var',\n",
       "              tensor([0.0191, 0.1167, 0.0745, 0.0039, 0.0555, 0.0141, 0.0308, 0.0068, 0.0971,\n",
       "                      0.0287, 0.0039, 0.0119, 0.0506, 0.0143, 0.0052, 0.0299])),\n",
       "             ('downs.0.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('downs.0.conv.3.weight',\n",
       "              tensor([[[[ 0.0822, -0.0165, -0.0302],\n",
       "                        [ 0.0244, -0.0356, -0.0738],\n",
       "                        [ 0.0366, -0.0612, -0.0230]],\n",
       "              \n",
       "                       [[-0.0593, -0.0037,  0.0256],\n",
       "                        [-0.0314,  0.0497,  0.0662],\n",
       "                        [ 0.0804, -0.0036,  0.0685]],\n",
       "              \n",
       "                       [[ 0.0254,  0.0100,  0.0183],\n",
       "                        [-0.0642,  0.0894,  0.0012],\n",
       "                        [-0.0377, -0.0378,  0.0794]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0157, -0.0680, -0.0414],\n",
       "                        [ 0.0333,  0.0137, -0.0478],\n",
       "                        [-0.0334,  0.0639,  0.0676]],\n",
       "              \n",
       "                       [[-0.0297,  0.0554, -0.0603],\n",
       "                        [ 0.0015,  0.0283, -0.0430],\n",
       "                        [ 0.0297, -0.0535, -0.0195]],\n",
       "              \n",
       "                       [[-0.0462,  0.0806, -0.0122],\n",
       "                        [ 0.0629,  0.0012, -0.0087],\n",
       "                        [ 0.0176,  0.0175, -0.0689]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0782,  0.0868,  0.0733],\n",
       "                        [-0.0379,  0.0137, -0.0277],\n",
       "                        [-0.0086,  0.0585,  0.0205]],\n",
       "              \n",
       "                       [[-0.0066, -0.0242, -0.0725],\n",
       "                        [ 0.0023,  0.0514,  0.0548],\n",
       "                        [ 0.0533, -0.0096,  0.0396]],\n",
       "              \n",
       "                       [[ 0.0210,  0.0522,  0.0167],\n",
       "                        [ 0.0655, -0.0565,  0.0861],\n",
       "                        [-0.0328, -0.0616, -0.0641]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0867,  0.0307,  0.0443],\n",
       "                        [-0.0191, -0.0303, -0.0725],\n",
       "                        [-0.0816,  0.0301,  0.0595]],\n",
       "              \n",
       "                       [[ 0.0082, -0.0236, -0.0505],\n",
       "                        [-0.0192, -0.0648, -0.0354],\n",
       "                        [-0.0452,  0.0232, -0.0203]],\n",
       "              \n",
       "                       [[-0.0216, -0.0442,  0.0390],\n",
       "                        [ 0.0560,  0.0619,  0.0583],\n",
       "                        [ 0.0042, -0.0768,  0.0648]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0134,  0.0734,  0.0788],\n",
       "                        [ 0.0589,  0.1062,  0.0604],\n",
       "                        [-0.0648, -0.0334, -0.0216]],\n",
       "              \n",
       "                       [[-0.0670, -0.0650, -0.0641],\n",
       "                        [-0.0322, -0.0190,  0.0250],\n",
       "                        [-0.0303, -0.0233,  0.0629]],\n",
       "              \n",
       "                       [[-0.0725,  0.0432, -0.0475],\n",
       "                        [ 0.0014, -0.0653,  0.0857],\n",
       "                        [-0.0304,  0.0052,  0.0160]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0520,  0.0288,  0.0095],\n",
       "                        [-0.0592,  0.0504,  0.0173],\n",
       "                        [-0.0686, -0.0011,  0.0602]],\n",
       "              \n",
       "                       [[-0.0467,  0.0512,  0.0082],\n",
       "                        [ 0.0665, -0.0408,  0.0462],\n",
       "                        [-0.0277, -0.0229,  0.0367]],\n",
       "              \n",
       "                       [[-0.0403, -0.0024, -0.0841],\n",
       "                        [-0.0116, -0.0858, -0.0769],\n",
       "                        [-0.0103,  0.0803,  0.0019]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0191,  0.0191,  0.0606],\n",
       "                        [-0.0129,  0.0319, -0.0807],\n",
       "                        [-0.0123, -0.0563,  0.0184]],\n",
       "              \n",
       "                       [[ 0.0434,  0.0185, -0.0765],\n",
       "                        [-0.0312, -0.0048,  0.0009],\n",
       "                        [ 0.0967, -0.0258,  0.0126]],\n",
       "              \n",
       "                       [[-0.0711,  0.0263,  0.0776],\n",
       "                        [-0.0410,  0.0607, -0.0900],\n",
       "                        [ 0.0540, -0.0304, -0.0209]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0931, -0.0403,  0.0545],\n",
       "                        [ 0.0383,  0.0288,  0.0475],\n",
       "                        [-0.0375,  0.0724,  0.0512]],\n",
       "              \n",
       "                       [[-0.0123,  0.0500, -0.0883],\n",
       "                        [ 0.0420,  0.0364, -0.0760],\n",
       "                        [ 0.0609,  0.0577,  0.0251]],\n",
       "              \n",
       "                       [[ 0.0695,  0.0321,  0.0376],\n",
       "                        [-0.0181, -0.0918, -0.0365],\n",
       "                        [-0.0784,  0.0505, -0.0564]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0690,  0.0267, -0.0577],\n",
       "                        [-0.0735,  0.0193,  0.0227],\n",
       "                        [-0.0707,  0.0712,  0.0396]],\n",
       "              \n",
       "                       [[ 0.0714, -0.0262, -0.0020],\n",
       "                        [ 0.0572, -0.0056,  0.0507],\n",
       "                        [ 0.0582,  0.0336,  0.0523]],\n",
       "              \n",
       "                       [[-0.0855, -0.0124,  0.0919],\n",
       "                        [ 0.0581, -0.0746, -0.0696],\n",
       "                        [ 0.0198,  0.0249, -0.0527]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0680, -0.0596, -0.0136],\n",
       "                        [ 0.0541,  0.0521,  0.0205],\n",
       "                        [-0.0529,  0.0254,  0.0117]],\n",
       "              \n",
       "                       [[-0.0272,  0.0157, -0.0067],\n",
       "                        [ 0.0604,  0.0606, -0.0404],\n",
       "                        [-0.0939, -0.0614,  0.0964]],\n",
       "              \n",
       "                       [[-0.0713,  0.0364, -0.0760],\n",
       "                        [ 0.0480,  0.0380, -0.0208],\n",
       "                        [-0.0284, -0.0206, -0.0410]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0714, -0.0697,  0.0447],\n",
       "                        [ 0.0181,  0.0712,  0.0425],\n",
       "                        [-0.0161, -0.0707, -0.0454]],\n",
       "              \n",
       "                       [[-0.0745, -0.0396,  0.0583],\n",
       "                        [-0.0858,  0.0303, -0.0334],\n",
       "                        [ 0.0592,  0.0403, -0.0048]],\n",
       "              \n",
       "                       [[ 0.0427,  0.0556, -0.0261],\n",
       "                        [ 0.0267,  0.0837,  0.0089],\n",
       "                        [-0.0447,  0.0425,  0.0172]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0222, -0.0858,  0.0520],\n",
       "                        [-0.0643, -0.0290, -0.0875],\n",
       "                        [-0.0163,  0.0212, -0.0835]],\n",
       "              \n",
       "                       [[-0.0597,  0.0113, -0.0301],\n",
       "                        [-0.0340,  0.0743, -0.0461],\n",
       "                        [-0.0855, -0.0698, -0.0820]],\n",
       "              \n",
       "                       [[ 0.0403,  0.0056, -0.0746],\n",
       "                        [ 0.0853,  0.0109, -0.0734],\n",
       "                        [-0.0494, -0.0010, -0.0208]]]])),\n",
       "             ('downs.0.conv.4.weight',\n",
       "              tensor([0.9935, 0.9927, 0.9883, 0.9841, 1.0051, 0.9906, 0.9907, 1.0020, 0.9962,\n",
       "                      0.9917, 0.9821, 0.9851, 0.9786, 0.9856, 0.9937, 0.9891])),\n",
       "             ('downs.0.conv.4.bias',\n",
       "              tensor([ 0.0031,  0.0034, -0.0012,  0.0171,  0.0137, -0.0011,  0.0072,  0.0120,\n",
       "                       0.0219,  0.0085,  0.0028,  0.0079, -0.0011, -0.0036,  0.0009,  0.0176])),\n",
       "             ('downs.0.conv.4.running_mean',\n",
       "              tensor([ 0.1162,  0.3359,  0.0860, -0.1861,  0.0330,  0.2335,  0.1958, -0.4379,\n",
       "                      -0.7668, -0.2746, -0.1072,  0.0036,  0.1390,  0.3142,  0.2546, -0.2762])),\n",
       "             ('downs.0.conv.4.running_var',\n",
       "              tensor([0.1666, 0.5300, 0.2054, 0.2021, 0.4240, 0.1162, 0.3868, 0.4088, 0.1596,\n",
       "                      0.1854, 0.2823, 0.1043, 0.0891, 0.2552, 0.2068, 0.1813])),\n",
       "             ('downs.0.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('downs.1.conv.0.weight',\n",
       "              tensor([[[[-4.2338e-02, -6.8899e-02,  8.2530e-02],\n",
       "                        [ 8.0297e-02, -5.8053e-02,  4.2808e-02],\n",
       "                        [-7.0085e-02,  3.3263e-05, -6.1486e-02]],\n",
       "              \n",
       "                       [[ 4.8205e-02, -9.1414e-02, -1.6317e-02],\n",
       "                        [-6.6150e-02,  5.4637e-02, -1.2181e-02],\n",
       "                        [-7.7786e-02,  6.0842e-02,  5.9925e-02]],\n",
       "              \n",
       "                       [[ 6.7204e-02,  6.1258e-02,  1.0343e-02],\n",
       "                        [ 5.4294e-02, -1.9297e-02, -7.3729e-02],\n",
       "                        [ 4.8119e-02,  6.0448e-02, -4.0282e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.2636e-02, -3.2638e-02,  6.4439e-02],\n",
       "                        [ 5.0949e-02,  6.7220e-02, -7.2309e-02],\n",
       "                        [-6.2713e-02, -1.1199e-02,  5.9552e-02]],\n",
       "              \n",
       "                       [[-2.8348e-02, -3.8792e-02,  4.4611e-02],\n",
       "                        [ 1.2864e-02,  6.7323e-02,  5.4682e-02],\n",
       "                        [-7.5766e-02, -8.9107e-02, -7.2217e-02]],\n",
       "              \n",
       "                       [[ 9.4825e-04, -3.2105e-02, -4.7063e-02],\n",
       "                        [-8.1860e-02,  5.9211e-02,  1.5494e-02],\n",
       "                        [-4.5942e-02, -9.3190e-02, -5.9078e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.0152e-02, -8.4807e-02,  5.1435e-02],\n",
       "                        [ 2.9213e-02, -3.2774e-03,  1.0110e-02],\n",
       "                        [-2.2770e-02,  7.7096e-02, -1.7994e-02]],\n",
       "              \n",
       "                       [[ 6.0207e-02,  3.4714e-03,  2.9390e-02],\n",
       "                        [-1.7377e-02, -5.1847e-02,  5.0178e-02],\n",
       "                        [-6.2825e-02,  7.2237e-02, -5.7505e-02]],\n",
       "              \n",
       "                       [[ 6.9318e-03,  5.9795e-02, -6.8656e-02],\n",
       "                        [ 2.9212e-02, -2.0522e-02, -7.3066e-02],\n",
       "                        [-1.3299e-02, -3.2463e-02, -7.4952e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.4157e-02, -1.3311e-02,  5.7335e-02],\n",
       "                        [-2.3073e-02, -6.3800e-02,  6.3221e-02],\n",
       "                        [ 6.4851e-02, -3.6749e-02, -6.2585e-02]],\n",
       "              \n",
       "                       [[ 1.5147e-02,  5.6344e-02,  4.2555e-02],\n",
       "                        [-5.6402e-02,  2.2962e-02, -1.0738e-02],\n",
       "                        [-8.1838e-02,  2.8199e-02,  6.0222e-03]],\n",
       "              \n",
       "                       [[ 1.3880e-02, -3.9079e-02, -1.4059e-02],\n",
       "                        [-7.4398e-03,  6.5188e-02,  5.0635e-02],\n",
       "                        [-9.5808e-02,  4.1421e-02,  2.9815e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.6234e-02,  7.7387e-02, -4.1282e-02],\n",
       "                        [ 1.1206e-02,  5.4388e-03, -5.5164e-03],\n",
       "                        [ 3.1232e-02,  7.8608e-02,  9.7563e-02]],\n",
       "              \n",
       "                       [[ 3.3075e-02, -5.5220e-02, -7.2574e-03],\n",
       "                        [ 2.3452e-02, -2.3446e-02, -4.2041e-02],\n",
       "                        [-8.5877e-04,  4.5086e-02, -2.6179e-02]],\n",
       "              \n",
       "                       [[ 8.1238e-02, -4.9333e-02,  4.3029e-03],\n",
       "                        [ 5.2876e-02, -6.8071e-03,  1.4177e-02],\n",
       "                        [ 6.8694e-02,  1.5903e-02, -7.9234e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.6076e-02, -1.1000e-02,  5.5426e-02],\n",
       "                        [-2.1812e-02,  4.1030e-02, -3.6302e-03],\n",
       "                        [ 6.4308e-02, -2.3639e-02, -5.7932e-02]],\n",
       "              \n",
       "                       [[-4.6686e-02,  7.2159e-02,  7.4341e-02],\n",
       "                        [ 2.7588e-02, -4.0010e-02, -1.9088e-02],\n",
       "                        [-7.1317e-02,  2.9694e-02,  8.2149e-03]],\n",
       "              \n",
       "                       [[ 4.2491e-02,  4.9462e-02,  3.2788e-02],\n",
       "                        [ 4.0191e-02,  6.9256e-02,  7.1408e-02],\n",
       "                        [ 2.9928e-02, -6.5706e-02, -9.5817e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 5.2834e-02,  5.7856e-02,  3.9745e-02],\n",
       "                        [-1.1552e-02,  5.2819e-02,  7.6786e-02],\n",
       "                        [-6.8049e-02,  6.1863e-02,  3.7279e-02]],\n",
       "              \n",
       "                       [[-4.7995e-02, -4.5322e-02, -3.7164e-02],\n",
       "                        [ 6.4113e-02,  7.0441e-03,  5.5332e-03],\n",
       "                        [ 7.1308e-02, -8.6418e-02, -5.7674e-02]],\n",
       "              \n",
       "                       [[ 4.6385e-02,  7.0910e-02, -4.9788e-02],\n",
       "                        [-6.7643e-02, -5.5466e-02,  4.1015e-02],\n",
       "                        [ 5.1421e-02, -7.2592e-03,  7.8628e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.8301e-02, -1.7144e-02, -6.0661e-02],\n",
       "                        [-4.2532e-02,  3.8762e-03, -5.7096e-02],\n",
       "                        [-7.2650e-03,  2.5002e-02,  1.7830e-02]],\n",
       "              \n",
       "                       [[-4.2322e-02, -3.6419e-02, -5.7118e-02],\n",
       "                        [-5.9757e-02,  1.8481e-03, -5.8373e-02],\n",
       "                        [ 4.2634e-02,  8.4856e-04, -2.8843e-02]],\n",
       "              \n",
       "                       [[ 4.0843e-02,  6.8916e-03,  6.1433e-02],\n",
       "                        [ 1.5174e-02,  5.0296e-02,  7.1650e-02],\n",
       "                        [-4.6543e-02,  8.3959e-02, -3.2886e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.4597e-02, -6.6733e-02, -8.1534e-02],\n",
       "                        [-6.4454e-02, -4.8101e-02, -6.6689e-02],\n",
       "                        [ 2.0503e-02,  3.3373e-02,  5.1886e-02]],\n",
       "              \n",
       "                       [[ 1.8490e-02, -1.8429e-02, -5.6450e-02],\n",
       "                        [-2.0224e-02, -4.9825e-02, -7.1018e-02],\n",
       "                        [ 4.5045e-03, -1.8459e-02,  1.7164e-02]],\n",
       "              \n",
       "                       [[-2.6303e-02,  6.9127e-03,  7.2546e-02],\n",
       "                        [ 2.8826e-03, -6.6454e-03,  2.1690e-02],\n",
       "                        [-5.7644e-02,  4.9869e-02,  3.8942e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.8851e-02, -2.6432e-02,  7.9555e-02],\n",
       "                        [-7.3595e-02, -6.6057e-02,  8.4488e-02],\n",
       "                        [ 6.3238e-02, -2.0950e-02, -2.9418e-02]],\n",
       "              \n",
       "                       [[-6.7791e-02, -9.6674e-03,  8.4055e-02],\n",
       "                        [ 3.8329e-03,  7.7768e-02,  2.5937e-02],\n",
       "                        [-2.6865e-02,  3.2479e-02, -1.3947e-02]],\n",
       "              \n",
       "                       [[-3.0982e-02, -4.6021e-02,  4.4603e-02],\n",
       "                        [ 7.6276e-02, -7.6967e-02,  1.6331e-02],\n",
       "                        [ 9.2338e-03,  1.1625e-02, -7.0608e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.3735e-02,  2.1393e-02, -6.4991e-02],\n",
       "                        [-6.0588e-02, -4.3928e-02, -3.9350e-02],\n",
       "                        [ 2.8888e-02,  6.0081e-02, -4.3664e-03]],\n",
       "              \n",
       "                       [[ 6.7479e-02,  5.1602e-02, -4.8705e-02],\n",
       "                        [-5.5434e-02, -6.6999e-02, -8.2629e-03],\n",
       "                        [-4.8327e-02, -6.1842e-02, -6.8119e-02]],\n",
       "              \n",
       "                       [[-4.6475e-02,  4.4252e-02, -4.5564e-02],\n",
       "                        [ 5.4276e-02,  7.6552e-02,  4.3266e-02],\n",
       "                        [-2.9512e-02,  3.8534e-02,  1.6930e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-8.2329e-02, -4.5766e-02, -3.3230e-02],\n",
       "                        [ 8.4670e-02,  8.5200e-02, -5.3080e-02],\n",
       "                        [-5.6639e-02,  6.0070e-02,  1.0522e-02]],\n",
       "              \n",
       "                       [[-5.7639e-02,  4.1827e-02,  4.1375e-02],\n",
       "                        [-3.8627e-02,  3.1743e-03,  4.6524e-02],\n",
       "                        [-5.1036e-02, -7.6643e-02,  4.2479e-02]],\n",
       "              \n",
       "                       [[ 8.8019e-03, -7.6065e-02,  4.3321e-02],\n",
       "                        [-5.6665e-02, -7.4953e-02,  7.3011e-02],\n",
       "                        [-4.7142e-03, -1.3131e-02,  6.9145e-02]]]])),\n",
       "             ('downs.1.conv.1.weight',\n",
       "              tensor([1.0097, 1.0127, 0.9906, 0.9820, 1.0032, 0.9953, 0.9817, 0.9948, 0.9953,\n",
       "                      0.9961, 0.9906, 1.0165, 0.9965, 0.9997, 0.9942, 1.0203, 1.0072, 1.0188,\n",
       "                      0.9972, 0.9956, 1.0037, 0.9894, 0.9945, 1.0033, 1.0127, 0.9812, 0.9977,\n",
       "                      1.0164, 0.9870, 1.0084, 1.0017, 0.9964])),\n",
       "             ('downs.1.conv.1.bias',\n",
       "              tensor([ 0.0042,  0.0112,  0.0082,  0.0160,  0.0011,  0.0012, -0.0060, -0.0040,\n",
       "                      -0.0091,  0.0160,  0.0098,  0.0261,  0.0039,  0.0190, -0.0031,  0.0135,\n",
       "                       0.0107,  0.0397,  0.0031,  0.0102,  0.0007, -0.0069,  0.0039, -0.0025,\n",
       "                       0.0151, -0.0105,  0.0071,  0.0229,  0.0070,  0.0070,  0.0053,  0.0122])),\n",
       "             ('downs.1.conv.1.running_mean',\n",
       "              tensor([ 0.2353,  0.0341,  0.4318,  0.3023,  0.0148,  0.2846, -0.6174, -0.3873,\n",
       "                       0.0290, -0.0965,  0.1326, -0.2461,  0.0662,  0.0022, -0.7022, -0.3484,\n",
       "                      -0.0428, -0.2241, -0.1353,  0.6629,  0.1498, -0.0338, -0.5293, -0.2242,\n",
       "                      -0.0130,  0.1287, -0.1421, -0.0971, -0.0276,  0.4201, -0.2556, -0.5031])),\n",
       "             ('downs.1.conv.1.running_var',\n",
       "              tensor([0.2004, 0.1413, 0.2740, 0.0914, 0.0871, 0.0592, 0.2123, 0.0856, 0.0958,\n",
       "                      0.1081, 0.1228, 0.1435, 0.1483, 0.1306, 0.1732, 0.2669, 0.1668, 0.0658,\n",
       "                      0.2467, 0.1765, 0.0958, 0.0810, 0.0890, 0.2471, 0.1711, 0.1398, 0.1794,\n",
       "                      0.1207, 0.0692, 0.3472, 0.1113, 0.0801])),\n",
       "             ('downs.1.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('downs.1.conv.3.weight',\n",
       "              tensor([[[[ 0.0297, -0.0431,  0.0333],\n",
       "                        [-0.0554, -0.0005,  0.0561],\n",
       "                        [-0.0137,  0.0044,  0.0065]],\n",
       "              \n",
       "                       [[ 0.0403, -0.0050, -0.0310],\n",
       "                        [ 0.0440,  0.0005, -0.0390],\n",
       "                        [-0.0303, -0.0212,  0.0433]],\n",
       "              \n",
       "                       [[ 0.0112,  0.0258, -0.0247],\n",
       "                        [-0.0215, -0.0202, -0.0119],\n",
       "                        [ 0.0014,  0.0282, -0.0015]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0440,  0.0624, -0.0516],\n",
       "                        [-0.0465,  0.0029,  0.0202],\n",
       "                        [ 0.0311,  0.0326,  0.0201]],\n",
       "              \n",
       "                       [[ 0.0022, -0.0178,  0.0068],\n",
       "                        [ 0.0314, -0.0578,  0.0333],\n",
       "                        [-0.0441,  0.0208, -0.0451]],\n",
       "              \n",
       "                       [[-0.0560,  0.0404,  0.0191],\n",
       "                        [ 0.0303, -0.0212, -0.0204],\n",
       "                        [-0.0387, -0.0556, -0.0215]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0140, -0.0640,  0.0103],\n",
       "                        [-0.0528, -0.0584, -0.0086],\n",
       "                        [-0.0539, -0.0213,  0.0144]],\n",
       "              \n",
       "                       [[-0.0029, -0.0131, -0.0423],\n",
       "                        [ 0.0393, -0.0087, -0.0565],\n",
       "                        [-0.0129,  0.0607,  0.0484]],\n",
       "              \n",
       "                       [[ 0.0387,  0.0146, -0.0433],\n",
       "                        [-0.0159, -0.0384,  0.0557],\n",
       "                        [ 0.0430,  0.0393,  0.0329]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0412, -0.0484, -0.0452],\n",
       "                        [-0.0344, -0.0216,  0.0021],\n",
       "                        [-0.0436,  0.0158, -0.0282]],\n",
       "              \n",
       "                       [[-0.0032,  0.0320, -0.0148],\n",
       "                        [ 0.0392,  0.0407, -0.0307],\n",
       "                        [-0.0296,  0.0558,  0.0478]],\n",
       "              \n",
       "                       [[-0.0143, -0.0531,  0.0498],\n",
       "                        [ 0.0436, -0.0462,  0.0111],\n",
       "                        [-0.0023, -0.0511,  0.0023]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0018, -0.0525,  0.0634],\n",
       "                        [ 0.0263, -0.0484, -0.0036],\n",
       "                        [ 0.0165, -0.0394,  0.0335]],\n",
       "              \n",
       "                       [[ 0.0057,  0.0438, -0.0041],\n",
       "                        [ 0.0438, -0.0549,  0.0341],\n",
       "                        [-0.0262,  0.0568, -0.0436]],\n",
       "              \n",
       "                       [[-0.0291,  0.0159, -0.0408],\n",
       "                        [ 0.0357,  0.0194, -0.0556],\n",
       "                        [ 0.0146,  0.0286, -0.0392]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0242,  0.0082, -0.0067],\n",
       "                        [ 0.0162,  0.0670,  0.0351],\n",
       "                        [ 0.0628,  0.0195,  0.0324]],\n",
       "              \n",
       "                       [[ 0.0443, -0.0399, -0.0411],\n",
       "                        [ 0.0173,  0.0193,  0.0370],\n",
       "                        [-0.0233,  0.0259, -0.0580]],\n",
       "              \n",
       "                       [[-0.0306, -0.0371, -0.0563],\n",
       "                        [-0.0434, -0.0277,  0.0377],\n",
       "                        [ 0.0112, -0.0496, -0.0641]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0115, -0.0181, -0.0094],\n",
       "                        [-0.0003, -0.0211,  0.0272],\n",
       "                        [ 0.0583, -0.0426, -0.0072]],\n",
       "              \n",
       "                       [[-0.0294, -0.0205,  0.0177],\n",
       "                        [-0.0076,  0.0035, -0.0240],\n",
       "                        [ 0.0470,  0.0585, -0.0306]],\n",
       "              \n",
       "                       [[-0.0264, -0.0098,  0.0180],\n",
       "                        [ 0.0174,  0.0121, -0.0459],\n",
       "                        [-0.0436, -0.0497,  0.0675]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0142, -0.0519,  0.0169],\n",
       "                        [-0.0335,  0.0045,  0.0151],\n",
       "                        [ 0.0546,  0.0169,  0.0194]],\n",
       "              \n",
       "                       [[-0.0351, -0.0240,  0.0166],\n",
       "                        [ 0.0284,  0.0504, -0.0057],\n",
       "                        [-0.0002,  0.0248,  0.0388]],\n",
       "              \n",
       "                       [[ 0.0058,  0.0114, -0.0118],\n",
       "                        [-0.0035,  0.0207, -0.0137],\n",
       "                        [ 0.0428, -0.0607, -0.0551]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0435,  0.0411,  0.0074],\n",
       "                        [-0.0174, -0.0390, -0.0272],\n",
       "                        [-0.0370, -0.0601, -0.0492]],\n",
       "              \n",
       "                       [[ 0.0464,  0.0342,  0.0200],\n",
       "                        [-0.0067, -0.0356,  0.0447],\n",
       "                        [-0.0288, -0.0540,  0.0121]],\n",
       "              \n",
       "                       [[-0.0321, -0.0052,  0.0116],\n",
       "                        [-0.0008,  0.0499, -0.0316],\n",
       "                        [ 0.0060, -0.0574,  0.0041]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0373,  0.0374,  0.0309],\n",
       "                        [-0.0314, -0.0211, -0.0531],\n",
       "                        [ 0.0236,  0.0237, -0.0178]],\n",
       "              \n",
       "                       [[-0.0102, -0.0741, -0.0502],\n",
       "                        [ 0.0412, -0.0813, -0.0257],\n",
       "                        [ 0.0358,  0.0442, -0.0781]],\n",
       "              \n",
       "                       [[-0.0284,  0.0077,  0.0197],\n",
       "                        [ 0.0057,  0.0605, -0.0475],\n",
       "                        [-0.0061, -0.0296,  0.0180]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0326, -0.0050, -0.0726],\n",
       "                        [ 0.0197, -0.0435, -0.0486],\n",
       "                        [-0.0523,  0.0481, -0.0275]],\n",
       "              \n",
       "                       [[ 0.0227,  0.0387, -0.0107],\n",
       "                        [ 0.0017,  0.0277,  0.0110],\n",
       "                        [ 0.0436,  0.0435,  0.0329]],\n",
       "              \n",
       "                       [[-0.0233, -0.0391,  0.0480],\n",
       "                        [ 0.0339,  0.0217, -0.0028],\n",
       "                        [-0.0227, -0.0242, -0.0583]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0020, -0.0189,  0.0550],\n",
       "                        [ 0.0553,  0.0356,  0.0628],\n",
       "                        [ 0.0434, -0.0208,  0.0437]],\n",
       "              \n",
       "                       [[-0.0102,  0.0075, -0.0455],\n",
       "                        [-0.0184,  0.0342, -0.0210],\n",
       "                        [-0.0185, -0.0225, -0.0223]],\n",
       "              \n",
       "                       [[-0.0374,  0.0265,  0.0437],\n",
       "                        [ 0.0623,  0.0453, -0.0526],\n",
       "                        [ 0.0099,  0.0411,  0.0078]]]])),\n",
       "             ('downs.1.conv.4.weight',\n",
       "              tensor([0.9904, 0.9770, 0.9981, 0.9979, 0.9901, 1.0115, 1.0037, 1.0044, 0.9908,\n",
       "                      0.9973, 1.0016, 1.0019, 0.9946, 0.9859, 0.9908, 0.9818, 0.9900, 0.9895,\n",
       "                      0.9897, 0.9851, 0.9826, 0.9749, 0.9964, 1.0048, 0.9979, 0.9939, 0.9998,\n",
       "                      0.9920, 0.9753, 0.9919, 0.9956, 0.9860])),\n",
       "             ('downs.1.conv.4.bias',\n",
       "              tensor([-0.0045, -0.0133,  0.0097,  0.0013,  0.0058,  0.0141,  0.0228, -0.0104,\n",
       "                      -0.0077,  0.0126,  0.0015,  0.0092,  0.0112, -0.0020, -0.0083, -0.0052,\n",
       "                      -0.0035, -0.0027,  0.0077,  0.0110, -0.0061,  0.0087, -0.0010,  0.0020,\n",
       "                      -0.0039, -0.0030,  0.0108,  0.0142,  0.0003,  0.0094, -0.0024, -0.0059])),\n",
       "             ('downs.1.conv.4.running_mean',\n",
       "              tensor([-0.0475, -0.1872, -0.2487, -0.1906, -0.1606, -0.0674,  0.1967, -0.1732,\n",
       "                       0.3205, -0.2522,  0.2217, -0.0154, -0.0137, -0.2399, -0.2356, -0.4789,\n",
       "                      -0.1607,  0.1389, -0.0491,  0.0435, -0.2612,  0.0282, -0.1499, -0.0961,\n",
       "                      -0.1953, -0.1114, -0.1572, -0.3267, -0.0068, -0.0357, -0.5358,  0.2034])),\n",
       "             ('downs.1.conv.4.running_var',\n",
       "              tensor([0.1401, 0.1606, 0.2926, 0.2834, 0.2995, 0.1896, 0.1444, 0.2184, 0.1324,\n",
       "                      0.1976, 0.2040, 0.2175, 0.1245, 0.1185, 0.1747, 0.1778, 0.1274, 0.1349,\n",
       "                      0.1984, 0.1452, 0.1804, 0.2087, 0.2648, 0.2898, 0.2672, 0.2150, 0.1799,\n",
       "                      0.1253, 0.1040, 0.1389, 0.1731, 0.1663])),\n",
       "             ('downs.1.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('downs.2.conv.0.weight',\n",
       "              tensor([[[[-0.0613,  0.0313, -0.0392],\n",
       "                        [ 0.0424, -0.0589, -0.0454],\n",
       "                        [-0.0366, -0.0064, -0.0186]],\n",
       "              \n",
       "                       [[-0.0548,  0.0262, -0.0238],\n",
       "                        [-0.0025, -0.0310,  0.0092],\n",
       "                        [ 0.0175,  0.0032,  0.0092]],\n",
       "              \n",
       "                       [[-0.0429, -0.0336,  0.0481],\n",
       "                        [-0.0475,  0.0612, -0.0520],\n",
       "                        [ 0.0174,  0.0393,  0.0562]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0062,  0.0402,  0.0455],\n",
       "                        [ 0.0194,  0.0674, -0.0038],\n",
       "                        [-0.0363, -0.0533,  0.0224]],\n",
       "              \n",
       "                       [[ 0.0044, -0.0476,  0.0039],\n",
       "                        [-0.0015,  0.0094, -0.0068],\n",
       "                        [ 0.0672,  0.0018, -0.0181]],\n",
       "              \n",
       "                       [[-0.0434,  0.0413,  0.0745],\n",
       "                        [ 0.0112, -0.0452, -0.0258],\n",
       "                        [ 0.0478, -0.0085,  0.0041]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0006, -0.0216,  0.0645],\n",
       "                        [ 0.0420,  0.0271,  0.0138],\n",
       "                        [-0.0196, -0.0146, -0.0092]],\n",
       "              \n",
       "                       [[-0.0312, -0.0423, -0.0158],\n",
       "                        [-0.0324,  0.0190, -0.0426],\n",
       "                        [-0.0273, -0.0344,  0.0059]],\n",
       "              \n",
       "                       [[-0.0419,  0.0327,  0.0545],\n",
       "                        [ 0.0218,  0.0269,  0.0604],\n",
       "                        [-0.0595,  0.0184, -0.0144]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0602, -0.0418,  0.0325],\n",
       "                        [-0.0481, -0.0306, -0.0624],\n",
       "                        [ 0.0077, -0.0345,  0.0530]],\n",
       "              \n",
       "                       [[ 0.0140, -0.0499,  0.0224],\n",
       "                        [-0.0582,  0.0544,  0.0315],\n",
       "                        [-0.0181, -0.0372,  0.0491]],\n",
       "              \n",
       "                       [[-0.0406,  0.0137,  0.0398],\n",
       "                        [ 0.0224, -0.0361, -0.0407],\n",
       "                        [ 0.0041, -0.0031,  0.0459]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0281, -0.0257, -0.0136],\n",
       "                        [ 0.0572,  0.0553,  0.0343],\n",
       "                        [-0.0517, -0.0272,  0.0594]],\n",
       "              \n",
       "                       [[ 0.0083, -0.0030,  0.0135],\n",
       "                        [-0.0149, -0.0247,  0.0203],\n",
       "                        [ 0.0214, -0.0391, -0.0362]],\n",
       "              \n",
       "                       [[-0.0410,  0.0417,  0.0671],\n",
       "                        [-0.0484, -0.0168,  0.0253],\n",
       "                        [ 0.0262, -0.0488,  0.0461]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0365, -0.0614, -0.0563],\n",
       "                        [-0.0191,  0.0403, -0.0471],\n",
       "                        [ 0.0524,  0.0421,  0.0035]],\n",
       "              \n",
       "                       [[ 0.0334, -0.0033,  0.0515],\n",
       "                        [ 0.0518,  0.0075, -0.0272],\n",
       "                        [-0.0239,  0.0422,  0.0449]],\n",
       "              \n",
       "                       [[-0.0532, -0.0429,  0.0029],\n",
       "                        [-0.0455,  0.0079, -0.0443],\n",
       "                        [ 0.0150,  0.0296,  0.0034]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0494, -0.0315, -0.0112],\n",
       "                        [ 0.0149, -0.0496, -0.0169],\n",
       "                        [ 0.0201, -0.0044,  0.0045]],\n",
       "              \n",
       "                       [[ 0.0013, -0.0421, -0.0300],\n",
       "                        [-0.0566, -0.0064, -0.0323],\n",
       "                        [ 0.0117,  0.0400, -0.0544]],\n",
       "              \n",
       "                       [[ 0.0027,  0.0340, -0.0149],\n",
       "                        [ 0.0483, -0.0632, -0.0263],\n",
       "                        [ 0.0280, -0.0224,  0.0270]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0051, -0.0422,  0.0258],\n",
       "                        [ 0.0367,  0.0262, -0.0011],\n",
       "                        [ 0.0181, -0.0486, -0.0087]],\n",
       "              \n",
       "                       [[ 0.0461, -0.0236, -0.0350],\n",
       "                        [ 0.0098, -0.0303,  0.0174],\n",
       "                        [ 0.0347, -0.0426, -0.0092]],\n",
       "              \n",
       "                       [[ 0.0098,  0.0260, -0.0116],\n",
       "                        [-0.0602,  0.0411,  0.0358],\n",
       "                        [ 0.0314,  0.0353,  0.0296]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0161, -0.0171, -0.0199],\n",
       "                        [-0.0287, -0.0200, -0.0540],\n",
       "                        [ 0.0343, -0.0163,  0.0362]],\n",
       "              \n",
       "                       [[ 0.0422,  0.0381, -0.0321],\n",
       "                        [ 0.0497, -0.0515, -0.0108],\n",
       "                        [ 0.0045, -0.0167, -0.0305]],\n",
       "              \n",
       "                       [[-0.0345, -0.0071, -0.0458],\n",
       "                        [ 0.0452, -0.0337,  0.0027],\n",
       "                        [ 0.0512,  0.0111,  0.0189]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0182, -0.0214,  0.0209],\n",
       "                        [-0.0553,  0.0369,  0.0277],\n",
       "                        [-0.0046, -0.0274, -0.0144]],\n",
       "              \n",
       "                       [[-0.0346,  0.0360,  0.0481],\n",
       "                        [ 0.0432, -0.0207, -0.0452],\n",
       "                        [ 0.0199,  0.0350, -0.0016]],\n",
       "              \n",
       "                       [[ 0.0470, -0.0380,  0.0041],\n",
       "                        [ 0.0491, -0.0012,  0.0446],\n",
       "                        [-0.0049, -0.0245, -0.0524]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0029,  0.0330, -0.0277],\n",
       "                        [-0.0554,  0.0177, -0.0233],\n",
       "                        [-0.0340,  0.0256, -0.0304]],\n",
       "              \n",
       "                       [[-0.0246, -0.0376, -0.0328],\n",
       "                        [-0.0129,  0.0501, -0.0129],\n",
       "                        [-0.0390, -0.0300, -0.0171]],\n",
       "              \n",
       "                       [[ 0.0265, -0.0115, -0.0341],\n",
       "                        [-0.0577,  0.0462,  0.0225],\n",
       "                        [ 0.0262, -0.0021,  0.0388]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0329, -0.0040, -0.0317],\n",
       "                        [-0.0015, -0.0070, -0.0266],\n",
       "                        [ 0.0324,  0.0248,  0.0466]],\n",
       "              \n",
       "                       [[-0.0447, -0.0267,  0.0542],\n",
       "                        [ 0.0353,  0.0506, -0.0622],\n",
       "                        [ 0.0311, -0.0316, -0.0618]],\n",
       "              \n",
       "                       [[-0.0369,  0.0089,  0.0344],\n",
       "                        [ 0.0206,  0.0468, -0.0096],\n",
       "                        [-0.0200,  0.0332,  0.0357]]]])),\n",
       "             ('downs.2.conv.1.weight',\n",
       "              tensor([1.0079, 0.9991, 0.9972, 1.0046, 1.0061, 1.0019, 1.0030, 0.9935, 0.9912,\n",
       "                      1.0002, 0.9900, 0.9928, 1.0006, 0.9981, 0.9932, 1.0035, 1.0170, 0.9984,\n",
       "                      1.0042, 1.0050, 1.0023, 0.9898, 0.9900, 1.0033, 0.9993, 1.0005, 0.9958,\n",
       "                      0.9983, 1.0076, 0.9930, 0.9939, 1.0048, 1.0077, 0.9984, 1.0205, 0.9847,\n",
       "                      1.0081, 1.0001, 1.0051, 1.0002, 1.0004, 1.0043, 0.9970, 1.0048, 0.9975,\n",
       "                      0.9959, 1.0068, 0.9916, 1.0023, 1.0119, 1.0025, 1.0097, 0.9893, 0.9949,\n",
       "                      1.0003, 0.9945, 0.9974, 0.9970, 0.9946, 1.0053, 1.0064, 1.0000, 0.9854,\n",
       "                      1.0042])),\n",
       "             ('downs.2.conv.1.bias',\n",
       "              tensor([-0.0110, -0.0107, -0.0022, -0.0147,  0.0008, -0.0021,  0.0106,  0.0049,\n",
       "                      -0.0054, -0.0088, -0.0126, -0.0014, -0.0014, -0.0229, -0.0074, -0.0015,\n",
       "                       0.0064, -0.0075,  0.0122,  0.0080, -0.0103, -0.0056, -0.0239,  0.0026,\n",
       "                       0.0125,  0.0009, -0.0029, -0.0028, -0.0189, -0.0133,  0.0062, -0.0004,\n",
       "                       0.0119, -0.0065,  0.0146, -0.0197, -0.0023, -0.0007, -0.0054, -0.0043,\n",
       "                      -0.0052, -0.0105,  0.0055, -0.0013, -0.0036, -0.0079, -0.0108, -0.0283,\n",
       "                       0.0027,  0.0172, -0.0045, -0.0007, -0.0046, -0.0067, -0.0074, -0.0175,\n",
       "                      -0.0180, -0.0023,  0.0114,  0.0054, -0.0008, -0.0075, -0.0161,  0.0044])),\n",
       "             ('downs.2.conv.1.running_mean',\n",
       "              tensor([-0.1306,  0.5674, -0.1578, -0.3717,  0.3707, -0.3653, -0.0994, -0.5186,\n",
       "                      -0.0323, -0.4441, -0.1452, -0.1184, -0.0782,  0.0860, -0.0549, -0.5322,\n",
       "                      -0.2435, -0.1600, -0.1901, -0.2700,  0.2464,  0.8555, -0.7776, -0.5112,\n",
       "                      -0.0190, -0.2222,  0.7315,  0.0279, -0.5299, -0.0983, -0.5546, -0.2062,\n",
       "                      -0.6500,  0.6386, -0.0897, -0.1186, -0.4601,  0.6906,  0.2698, -0.1558,\n",
       "                      -0.5308,  0.1103,  0.2378,  0.0230, -0.4295,  0.1796, -0.1291,  0.4569,\n",
       "                       0.2895, -0.2110,  0.2986,  0.2666, -0.0190, -0.5157,  0.0456, -0.1885,\n",
       "                      -0.2190, -0.1638, -0.0060, -0.6155,  0.1525, -0.3834, -0.1222,  0.0433])),\n",
       "             ('downs.2.conv.1.running_var',\n",
       "              tensor([0.1944, 0.1830, 0.1776, 0.1231, 0.1524, 0.2311, 0.1513, 0.2637, 0.2343,\n",
       "                      0.1319, 0.2016, 0.1950, 0.1429, 0.2170, 0.2872, 0.2391, 0.1135, 0.1393,\n",
       "                      0.1216, 0.1867, 0.2198, 0.2584, 0.2626, 0.1645, 0.2194, 0.2147, 0.1523,\n",
       "                      0.1685, 0.2087, 0.2090, 0.2274, 0.1062, 0.2481, 0.1839, 0.2594, 0.1932,\n",
       "                      0.1931, 0.1591, 0.1119, 0.1662, 0.1449, 0.2331, 0.2596, 0.1636, 0.1910,\n",
       "                      0.2072, 0.1391, 0.3125, 0.1395, 0.1133, 0.1744, 0.2044, 0.2179, 0.1801,\n",
       "                      0.2801, 0.2097, 0.2043, 0.1451, 0.1038, 0.1549, 0.2447, 0.3753, 0.2123,\n",
       "                      0.1249])),\n",
       "             ('downs.2.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('downs.2.conv.3.weight',\n",
       "              tensor([[[[-4.9435e-03,  2.2938e-02, -4.7293e-02],\n",
       "                        [-1.6046e-02, -6.1790e-03, -2.0344e-02],\n",
       "                        [-1.4539e-02, -1.7484e-02, -2.8811e-02]],\n",
       "              \n",
       "                       [[-1.0193e-02, -3.2170e-02, -4.0454e-03],\n",
       "                        [ 1.4424e-02, -2.0477e-02,  3.2812e-02],\n",
       "                        [ 2.5987e-02,  6.8675e-03,  1.1661e-02]],\n",
       "              \n",
       "                       [[-1.5721e-02,  2.2122e-02, -4.7336e-02],\n",
       "                        [-5.0782e-04,  9.9087e-03,  3.9288e-02],\n",
       "                        [-6.6262e-04, -3.4308e-02,  2.2575e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.8113e-03,  4.7622e-02, -8.3738e-03],\n",
       "                        [-1.4496e-02,  2.0346e-03, -1.5931e-02],\n",
       "                        [ 4.0675e-02, -1.6942e-02,  6.4143e-03]],\n",
       "              \n",
       "                       [[ 2.6249e-02,  2.5705e-02, -3.1222e-02],\n",
       "                        [ 2.9235e-02,  1.2338e-02, -8.0558e-03],\n",
       "                        [ 4.6809e-03,  2.8425e-03, -1.3860e-02]],\n",
       "              \n",
       "                       [[-4.7348e-02, -1.0991e-02, -5.1123e-02],\n",
       "                        [-2.2427e-02, -1.9202e-02, -1.9219e-03],\n",
       "                        [-1.0505e-02, -2.3975e-03, -9.6032e-04]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0253e-04, -1.1719e-02,  3.8154e-04],\n",
       "                        [ 3.0142e-02, -2.3898e-03,  1.7353e-02],\n",
       "                        [-1.0959e-02,  5.5683e-03,  1.6655e-02]],\n",
       "              \n",
       "                       [[-2.2469e-02,  4.3699e-02, -1.4388e-02],\n",
       "                        [-3.8817e-02, -1.5612e-02,  1.9366e-02],\n",
       "                        [-3.0879e-03, -6.1117e-03, -2.9715e-02]],\n",
       "              \n",
       "                       [[-6.4812e-03, -4.7838e-03,  1.1885e-03],\n",
       "                        [-3.5114e-02, -3.1148e-02,  4.9574e-03],\n",
       "                        [-3.0879e-02, -3.1548e-02,  1.1407e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.9843e-02, -1.5162e-02,  2.4660e-02],\n",
       "                        [ 2.2641e-02, -3.5632e-02,  2.4744e-02],\n",
       "                        [-2.0538e-02,  3.3978e-02,  8.2593e-03]],\n",
       "              \n",
       "                       [[ 1.3074e-02,  2.8381e-02,  3.2155e-02],\n",
       "                        [-2.3903e-02, -1.9559e-02, -4.1904e-02],\n",
       "                        [ 4.5988e-02,  1.8510e-02,  1.1252e-02]],\n",
       "              \n",
       "                       [[-7.8408e-03,  1.2346e-03, -2.9235e-02],\n",
       "                        [-1.6143e-02,  1.2245e-02, -1.2929e-02],\n",
       "                        [ 1.2182e-02, -2.6111e-02, -3.4070e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.8213e-02,  2.0666e-02, -6.9441e-03],\n",
       "                        [-1.3433e-02,  3.5273e-02,  2.3227e-02],\n",
       "                        [ 9.1047e-03, -4.3405e-02,  7.7976e-03]],\n",
       "              \n",
       "                       [[ 1.9633e-03, -3.9839e-02,  3.8207e-02],\n",
       "                        [-1.9921e-03, -1.5995e-02,  3.2319e-02],\n",
       "                        [ 2.7644e-02, -6.0726e-03, -2.5065e-02]],\n",
       "              \n",
       "                       [[-3.2356e-02, -2.4076e-02,  3.8070e-02],\n",
       "                        [ 4.3181e-02,  1.1012e-02,  1.0002e-02],\n",
       "                        [-1.5444e-02, -2.7909e-02,  2.3728e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.1345e-02, -3.3506e-02,  1.5014e-02],\n",
       "                        [ 1.9410e-03,  1.0839e-02,  1.5556e-02],\n",
       "                        [ 1.6259e-02,  3.9187e-02, -2.5585e-02]],\n",
       "              \n",
       "                       [[ 2.1477e-02, -1.2767e-02, -1.0605e-02],\n",
       "                        [-3.4221e-03, -2.6950e-02,  4.0133e-02],\n",
       "                        [ 1.1899e-02,  2.1193e-02, -4.3931e-03]],\n",
       "              \n",
       "                       [[ 7.5245e-03,  2.8421e-03, -9.4405e-03],\n",
       "                        [ 6.0438e-05, -7.9633e-03,  7.2042e-03],\n",
       "                        [ 2.0181e-03,  1.3083e-02,  3.0483e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-2.0309e-03,  1.1927e-02, -4.0884e-02],\n",
       "                        [-1.1680e-02,  2.5104e-02, -4.6924e-02],\n",
       "                        [-3.4865e-02,  9.7244e-05,  4.0910e-02]],\n",
       "              \n",
       "                       [[-2.8547e-02, -2.3464e-02, -3.9909e-02],\n",
       "                        [-3.3251e-03, -5.1304e-02, -2.2916e-02],\n",
       "                        [ 4.2387e-02,  1.8811e-02, -1.2406e-02]],\n",
       "              \n",
       "                       [[-3.4658e-02,  5.6672e-03, -6.5749e-03],\n",
       "                        [-4.9934e-04, -1.8855e-02,  4.0049e-03],\n",
       "                        [-2.9297e-02,  2.4562e-04, -3.0533e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3839e-02,  2.4356e-02, -1.5712e-02],\n",
       "                        [-1.2497e-02, -3.2344e-02,  2.5138e-02],\n",
       "                        [-2.1894e-02, -4.8602e-02,  7.0834e-03]],\n",
       "              \n",
       "                       [[-1.6828e-02,  3.5223e-03, -7.0437e-03],\n",
       "                        [ 3.4635e-02,  1.8078e-02, -3.6319e-02],\n",
       "                        [ 9.0346e-03, -2.7496e-02, -3.0699e-03]],\n",
       "              \n",
       "                       [[ 2.1004e-03,  4.7039e-03, -2.1236e-02],\n",
       "                        [-2.6678e-02, -2.7271e-04,  1.6637e-02],\n",
       "                        [-5.3676e-02,  1.3011e-02, -4.4391e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8963e-02, -1.6020e-02, -1.2300e-02],\n",
       "                        [-2.8846e-02, -1.5598e-02, -2.6470e-02],\n",
       "                        [-2.1505e-02, -2.6339e-02,  3.5555e-02]],\n",
       "              \n",
       "                       [[-5.8063e-03,  2.3920e-02, -3.0627e-02],\n",
       "                        [-2.2577e-02, -1.4009e-02,  2.2394e-02],\n",
       "                        [-8.2860e-03, -3.8042e-02, -7.7824e-03]],\n",
       "              \n",
       "                       [[ 1.9116e-02,  2.8974e-02, -2.4391e-02],\n",
       "                        [-1.8503e-02,  3.3473e-02,  4.2752e-02],\n",
       "                        [ 2.0461e-02,  3.1280e-02,  3.6236e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.5031e-02, -4.0659e-02,  4.3957e-02],\n",
       "                        [ 2.4558e-02,  3.5515e-02,  2.8430e-02],\n",
       "                        [ 1.2548e-02,  8.0598e-03, -3.0672e-02]],\n",
       "              \n",
       "                       [[ 2.6058e-02,  2.9994e-02,  1.4225e-02],\n",
       "                        [-4.9744e-02,  1.6843e-02,  2.8500e-02],\n",
       "                        [ 1.9979e-02, -1.0201e-02,  7.2402e-03]],\n",
       "              \n",
       "                       [[-3.2178e-02, -2.6680e-02, -2.4564e-02],\n",
       "                        [-4.4376e-02,  2.2491e-02, -2.3920e-02],\n",
       "                        [ 2.2066e-02, -2.8689e-02, -1.2776e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5818e-02, -3.0345e-03,  3.1474e-02],\n",
       "                        [-2.1349e-02,  9.2015e-03, -2.9335e-02],\n",
       "                        [-2.8072e-02, -2.9118e-02, -1.0970e-02]],\n",
       "              \n",
       "                       [[-2.5959e-03,  4.9765e-02, -2.1860e-02],\n",
       "                        [-4.7001e-02, -4.0448e-02,  3.2161e-02],\n",
       "                        [ 4.0140e-02, -3.2009e-02, -2.2632e-02]],\n",
       "              \n",
       "                       [[ 2.2019e-02, -4.0389e-02,  1.2590e-02],\n",
       "                        [ 1.3884e-02, -2.8702e-02, -3.5064e-02],\n",
       "                        [ 3.2514e-02, -2.6340e-02, -2.8124e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.0628e-02, -1.7336e-02,  2.4087e-02],\n",
       "                        [ 1.1240e-02, -1.7580e-02,  1.7285e-02],\n",
       "                        [-3.7656e-02, -1.5124e-03,  3.1406e-03]],\n",
       "              \n",
       "                       [[-3.6441e-03, -1.1079e-02,  2.8090e-02],\n",
       "                        [ 8.2555e-03, -6.5026e-03,  5.6661e-02],\n",
       "                        [-1.9962e-02,  2.0448e-06, -2.2956e-02]],\n",
       "              \n",
       "                       [[-3.1870e-02, -2.7254e-02, -3.9565e-02],\n",
       "                        [-3.5345e-02,  3.9171e-02,  6.0218e-03],\n",
       "                        [-2.3114e-03,  3.0498e-02,  3.8230e-02]]]])),\n",
       "             ('downs.2.conv.4.weight',\n",
       "              tensor([0.9748, 0.9814, 0.9792, 0.9699, 0.9692, 0.9840, 0.9771, 0.9788, 0.9871,\n",
       "                      0.9960, 0.9695, 0.9769, 0.9841, 0.9831, 0.9883, 0.9714, 0.9826, 0.9710,\n",
       "                      0.9833, 0.9785, 0.9811, 0.9773, 0.9742, 0.9686, 0.9662, 0.9706, 0.9800,\n",
       "                      0.9831, 0.9966, 0.9667, 0.9738, 0.9756, 0.9660, 0.9775, 0.9764, 0.9748,\n",
       "                      0.9825, 0.9741, 0.9796, 0.9789, 0.9904, 0.9799, 0.9882, 0.9774, 0.9712,\n",
       "                      0.9774, 0.9819, 0.9846, 0.9753, 0.9743, 0.9752, 0.9720, 0.9706, 0.9802,\n",
       "                      0.9839, 0.9911, 0.9859, 0.9741, 0.9733, 0.9765, 0.9836, 0.9765, 0.9884,\n",
       "                      0.9721])),\n",
       "             ('downs.2.conv.4.bias',\n",
       "              tensor([-0.0405, -0.0156, -0.0215, -0.0358, -0.0305, -0.0278, -0.0341, -0.0269,\n",
       "                      -0.0221, -0.0153, -0.0286, -0.0256, -0.0084, -0.0373, -0.0323, -0.0348,\n",
       "                      -0.0286, -0.0310, -0.0234, -0.0254, -0.0269, -0.0279, -0.0325, -0.0343,\n",
       "                      -0.0363, -0.0395, -0.0263, -0.0298, -0.0065, -0.0274, -0.0284, -0.0444,\n",
       "                      -0.0375, -0.0232, -0.0168, -0.0437, -0.0177, -0.0357, -0.0372, -0.0321,\n",
       "                       0.0021, -0.0271, -0.0206, -0.0225, -0.0324, -0.0288, -0.0360, -0.0321,\n",
       "                      -0.0333, -0.0306, -0.0331, -0.0147, -0.0327, -0.0402, -0.0165, -0.0356,\n",
       "                      -0.0275, -0.0219, -0.0276, -0.0383, -0.0278, -0.0265, -0.0247, -0.0331])),\n",
       "             ('downs.2.conv.4.running_mean',\n",
       "              tensor([-0.2188,  0.0528,  0.3530,  0.3990, -0.2420, -0.3343,  0.1599, -0.4539,\n",
       "                      -0.0179, -0.3866,  0.2273, -0.3627,  0.0513, -0.1742,  0.0770, -0.1683,\n",
       "                      -0.3240, -0.0451, -0.2131, -0.0949, -0.3390,  0.3128, -0.1739,  0.2294,\n",
       "                      -0.4894,  0.2795, -0.1545, -0.4487,  0.2621, -0.0710, -0.3284, -0.1668,\n",
       "                       0.0316, -0.3019, -0.2830,  0.0288,  0.0169,  0.0461,  0.1385,  0.0385,\n",
       "                      -0.1445,  0.0162, -0.1781, -0.1620, -0.3254, -0.2412,  0.0258, -0.1015,\n",
       "                       0.0015, -0.0160, -0.1076, -0.0313,  0.0320,  0.1739,  0.0678, -0.1244,\n",
       "                       0.2124,  0.0751, -0.1867,  0.0851,  0.0241, -0.2956, -0.4371, -0.0239])),\n",
       "             ('downs.2.conv.4.running_var',\n",
       "              tensor([0.2128, 0.1551, 0.1765, 0.1640, 0.1721, 0.1632, 0.1455, 0.1378, 0.1386,\n",
       "                      0.1338, 0.1713, 0.1565, 0.1422, 0.2470, 0.1397, 0.1758, 0.1798, 0.2212,\n",
       "                      0.1347, 0.1446, 0.1284, 0.1501, 0.1254, 0.1598, 0.1741, 0.1485, 0.1825,\n",
       "                      0.1482, 0.2112, 0.1902, 0.1202, 0.1460, 0.1844, 0.1058, 0.1554, 0.1752,\n",
       "                      0.1496, 0.1635, 0.1442, 0.1177, 0.1024, 0.1190, 0.2007, 0.2083, 0.2366,\n",
       "                      0.1235, 0.1540, 0.1718, 0.1711, 0.2123, 0.1810, 0.1509, 0.1666, 0.1317,\n",
       "                      0.1550, 0.1299, 0.1073, 0.1521, 0.1342, 0.1143, 0.1606, 0.1050, 0.1365,\n",
       "                      0.1584])),\n",
       "             ('downs.2.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('downs.3.conv.0.weight',\n",
       "              tensor([[[[-0.0078, -0.0056,  0.0151],\n",
       "                        [-0.0340, -0.0364, -0.0071],\n",
       "                        [-0.0117,  0.0163, -0.0204]],\n",
       "              \n",
       "                       [[-0.0119,  0.0306,  0.0252],\n",
       "                        [ 0.0116,  0.0107, -0.0104],\n",
       "                        [ 0.0054,  0.0178,  0.0100]],\n",
       "              \n",
       "                       [[-0.0224, -0.0531, -0.0038],\n",
       "                        [ 0.0218,  0.0381,  0.0367],\n",
       "                        [ 0.0107,  0.0276,  0.0411]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0237, -0.0251, -0.0254],\n",
       "                        [ 0.0379, -0.0038,  0.0002],\n",
       "                        [ 0.0241, -0.0223,  0.0146]],\n",
       "              \n",
       "                       [[ 0.0060, -0.0115, -0.0109],\n",
       "                        [-0.0246,  0.0016, -0.0240],\n",
       "                        [ 0.0325, -0.0232,  0.0301]],\n",
       "              \n",
       "                       [[ 0.0003,  0.0251, -0.0396],\n",
       "                        [-0.0421,  0.0111,  0.0255],\n",
       "                        [ 0.0279,  0.0146, -0.0115]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0031, -0.0360,  0.0010],\n",
       "                        [-0.0344,  0.0417, -0.0261],\n",
       "                        [-0.0100,  0.0274, -0.0224]],\n",
       "              \n",
       "                       [[ 0.0383, -0.0349,  0.0228],\n",
       "                        [-0.0147, -0.0070,  0.0198],\n",
       "                        [-0.0083, -0.0176, -0.0223]],\n",
       "              \n",
       "                       [[ 0.0099,  0.0271,  0.0152],\n",
       "                        [-0.0374,  0.0479, -0.0370],\n",
       "                        [-0.0381, -0.0272,  0.0077]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0482, -0.0277,  0.0003],\n",
       "                        [-0.0152, -0.0094, -0.0142],\n",
       "                        [-0.0131,  0.0003, -0.0245]],\n",
       "              \n",
       "                       [[-0.0021, -0.0302,  0.0223],\n",
       "                        [ 0.0307,  0.0313, -0.0394],\n",
       "                        [ 0.0078,  0.0011,  0.0591]],\n",
       "              \n",
       "                       [[ 0.0372, -0.0256, -0.0146],\n",
       "                        [-0.0071, -0.0382,  0.0195],\n",
       "                        [-0.0133,  0.0461, -0.0358]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0164,  0.0171, -0.0049],\n",
       "                        [ 0.0116,  0.0406, -0.0332],\n",
       "                        [ 0.0403, -0.0307, -0.0008]],\n",
       "              \n",
       "                       [[-0.0098,  0.0463, -0.0079],\n",
       "                        [-0.0048, -0.0394, -0.0206],\n",
       "                        [ 0.0291,  0.0154, -0.0291]],\n",
       "              \n",
       "                       [[ 0.0408, -0.0014, -0.0353],\n",
       "                        [ 0.0025,  0.0181, -0.0223],\n",
       "                        [-0.0197, -0.0197,  0.0436]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0261, -0.0261,  0.0229],\n",
       "                        [-0.0305,  0.0097,  0.0147],\n",
       "                        [ 0.0329,  0.0292,  0.0388]],\n",
       "              \n",
       "                       [[-0.0072,  0.0250, -0.0194],\n",
       "                        [ 0.0194,  0.0008,  0.0058],\n",
       "                        [ 0.0224, -0.0378,  0.0360]],\n",
       "              \n",
       "                       [[-0.0279,  0.0269,  0.0359],\n",
       "                        [ 0.0030, -0.0175, -0.0023],\n",
       "                        [ 0.0198,  0.0446,  0.0108]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0265, -0.0114, -0.0353],\n",
       "                        [-0.0105, -0.0405,  0.0219],\n",
       "                        [-0.0255,  0.0238,  0.0038]],\n",
       "              \n",
       "                       [[ 0.0394,  0.0283, -0.0208],\n",
       "                        [ 0.0308, -0.0109,  0.0347],\n",
       "                        [-0.0187,  0.0335,  0.0240]],\n",
       "              \n",
       "                       [[-0.0463, -0.0231, -0.0078],\n",
       "                        [ 0.0049, -0.0184, -0.0143],\n",
       "                        [-0.0241,  0.0214, -0.0087]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0108, -0.0151, -0.0081],\n",
       "                        [-0.0156, -0.0121, -0.0282],\n",
       "                        [-0.0015,  0.0415,  0.0353]],\n",
       "              \n",
       "                       [[-0.0124, -0.0064, -0.0222],\n",
       "                        [ 0.0331, -0.0385, -0.0290],\n",
       "                        [-0.0312, -0.0340,  0.0039]],\n",
       "              \n",
       "                       [[ 0.0174, -0.0106, -0.0248],\n",
       "                        [-0.0091,  0.0229,  0.0414],\n",
       "                        [ 0.0226, -0.0051,  0.0347]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0266,  0.0215,  0.0165],\n",
       "                        [-0.0308,  0.0442,  0.0357],\n",
       "                        [-0.0027, -0.0144,  0.0287]],\n",
       "              \n",
       "                       [[-0.0327, -0.0199,  0.0040],\n",
       "                        [ 0.0220,  0.0024, -0.0090],\n",
       "                        [ 0.0468, -0.0054,  0.0366]],\n",
       "              \n",
       "                       [[ 0.0225, -0.0067,  0.0174],\n",
       "                        [-0.0449, -0.0225,  0.0416],\n",
       "                        [-0.0146,  0.0209,  0.0382]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0110, -0.0380, -0.0328],\n",
       "                        [-0.0145,  0.0219,  0.0171],\n",
       "                        [ 0.0205, -0.0364,  0.0133]],\n",
       "              \n",
       "                       [[-0.0238,  0.0155, -0.0400],\n",
       "                        [-0.0309,  0.0099,  0.0037],\n",
       "                        [-0.0266,  0.0020, -0.0202]],\n",
       "              \n",
       "                       [[ 0.0169, -0.0532,  0.0156],\n",
       "                        [-0.0186,  0.0314, -0.0497],\n",
       "                        [-0.0303, -0.0170, -0.0092]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0030, -0.0064,  0.0204],\n",
       "                        [ 0.0098,  0.0132, -0.0121],\n",
       "                        [ 0.0273,  0.0380, -0.0219]],\n",
       "              \n",
       "                       [[-0.0225,  0.0258, -0.0184],\n",
       "                        [-0.0051, -0.0307, -0.0376],\n",
       "                        [-0.0106,  0.0082, -0.0269]],\n",
       "              \n",
       "                       [[-0.0307, -0.0187, -0.0285],\n",
       "                        [-0.0276, -0.0390, -0.0281],\n",
       "                        [-0.0274,  0.0077,  0.0110]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0375, -0.0215,  0.0136],\n",
       "                        [ 0.0149,  0.0024, -0.0311],\n",
       "                        [-0.0210,  0.0349, -0.0190]],\n",
       "              \n",
       "                       [[ 0.0343,  0.0005,  0.0011],\n",
       "                        [ 0.0020, -0.0260,  0.0375],\n",
       "                        [-0.0315,  0.0272,  0.0143]],\n",
       "              \n",
       "                       [[-0.0043, -0.0118,  0.0221],\n",
       "                        [ 0.0216, -0.0299,  0.0011],\n",
       "                        [-0.0244,  0.0355, -0.0022]]]])),\n",
       "             ('downs.3.conv.1.weight',\n",
       "              tensor([0.9902, 1.0005, 1.0041, 1.0066, 0.9926, 1.0049, 0.9977, 0.9934, 1.0027,\n",
       "                      1.0021, 1.0092, 0.9833, 1.0152, 0.9868, 0.9986, 0.9883, 0.9897, 0.9957,\n",
       "                      1.0105, 0.9994, 0.9922, 1.0058, 1.0013, 1.0030, 1.0057, 1.0079, 0.9949,\n",
       "                      1.0055, 1.0041, 0.9986, 1.0026, 0.9931, 1.0086, 0.9892, 1.0060, 0.9997,\n",
       "                      0.9993, 1.0026, 1.0005, 1.0145, 1.0183, 1.0140, 1.0107, 0.9755, 0.9973,\n",
       "                      0.9899, 0.9897, 0.9972, 1.0091, 1.0066, 1.0043, 1.0016, 0.9999, 1.0118,\n",
       "                      0.9823, 0.9912, 0.9729, 0.9928, 1.0113, 1.0081, 0.9984, 0.9979, 1.0047,\n",
       "                      1.0101, 0.9980, 0.9908, 1.0055, 1.0130, 1.0122, 0.9919, 1.0097, 0.9879,\n",
       "                      0.9941, 1.0055, 1.0054, 1.0001, 0.9917, 0.9900, 1.0059, 0.9973, 1.0065,\n",
       "                      1.0012, 1.0046, 1.0055, 1.0002, 1.0014, 0.9943, 0.9949, 1.0062, 0.9978,\n",
       "                      1.0098, 0.9846, 0.9957, 0.9935, 1.0004, 0.9962, 1.0126, 1.0023, 1.0115,\n",
       "                      1.0042, 0.9924, 1.0048, 0.9983, 0.9920, 1.0110, 0.9948, 0.9952, 1.0002,\n",
       "                      1.0126, 1.0146, 0.9950, 0.9876, 0.9957, 0.9965, 0.9977, 0.9805, 1.0032,\n",
       "                      1.0093, 1.0041, 1.0033, 1.0057, 1.0075, 1.0055, 0.9996, 1.0058, 0.9925,\n",
       "                      1.0022, 1.0107])),\n",
       "             ('downs.3.conv.1.bias',\n",
       "              tensor([-1.7815e-02, -2.7340e-02, -2.9445e-03, -1.6593e-02, -1.4869e-02,\n",
       "                      -2.8822e-02, -1.5935e-02, -2.3053e-02, -2.7479e-02, -1.7119e-02,\n",
       "                      -1.2928e-02, -2.6180e-02, -1.7004e-02, -9.0412e-03, -1.9286e-02,\n",
       "                      -1.4237e-02, -2.5296e-02, -1.4347e-02, -8.8474e-03, -3.0558e-02,\n",
       "                      -1.1769e-02, -1.5087e-02, -9.4298e-03, -1.0359e-02,  1.2629e-03,\n",
       "                      -1.8181e-02, -1.1021e-02, -2.6194e-02, -1.3523e-02, -3.4123e-02,\n",
       "                      -1.2365e-02, -1.1876e-02, -8.8743e-03, -2.0463e-02, -8.1379e-03,\n",
       "                      -2.4038e-02, -3.1168e-02, -1.4850e-02, -8.5398e-03, -1.8797e-02,\n",
       "                      -7.1131e-05, -1.8081e-02, -4.6809e-04, -3.2337e-02, -8.8592e-03,\n",
       "                      -1.9206e-02, -3.8269e-02, -2.7999e-02, -6.9813e-03, -1.8771e-02,\n",
       "                      -2.5286e-02,  2.9582e-04, -1.6868e-02, -1.1554e-02, -3.1250e-02,\n",
       "                      -1.9684e-02, -3.7035e-02, -3.8235e-02, -1.8320e-02, -5.2549e-03,\n",
       "                      -2.8145e-02, -1.3724e-02, -1.4504e-02,  9.2273e-03, -1.1818e-02,\n",
       "                      -1.7076e-02, -1.7485e-02,  3.2889e-03, -1.3531e-02, -3.0432e-02,\n",
       "                      -1.6061e-02, -2.3914e-02, -1.9343e-02, -1.8575e-02, -2.1747e-02,\n",
       "                      -1.8709e-02, -2.2507e-02, -2.7106e-02,  6.2891e-03, -4.6273e-03,\n",
       "                      -1.8386e-02, -8.6859e-03, -1.7705e-02, -9.5289e-03, -1.9445e-02,\n",
       "                      -1.8984e-02, -3.5890e-02, -2.3145e-02, -1.5558e-02, -7.7989e-03,\n",
       "                      -2.4049e-02, -2.3996e-02, -1.5591e-02, -7.4684e-03, -8.5180e-03,\n",
       "                      -2.2414e-02, -6.5018e-03, -3.1921e-02,  4.6051e-03, -1.5139e-02,\n",
       "                      -2.1915e-02, -2.4552e-02, -3.9970e-03, -2.3427e-02, -1.9287e-02,\n",
       "                      -2.8457e-02, -2.2129e-02, -3.4051e-03, -1.0432e-02, -6.9585e-03,\n",
       "                      -2.8989e-02, -2.7647e-02, -8.8134e-03, -2.0584e-02, -1.5315e-02,\n",
       "                      -4.2028e-02, -1.6968e-02, -1.0397e-02, -3.1309e-02, -2.4316e-02,\n",
       "                      -2.4317e-02, -1.1751e-02, -2.2652e-03, -1.0803e-02, -1.5338e-02,\n",
       "                      -1.6013e-02, -6.4908e-03, -5.1957e-03])),\n",
       "             ('downs.3.conv.1.running_mean',\n",
       "              tensor([ 1.9715e-01, -3.3290e-01, -3.7254e-01,  2.1299e-01, -2.1815e-01,\n",
       "                      -1.0803e+00,  1.6020e-01, -6.8654e-02, -3.6198e-01, -1.3465e-01,\n",
       "                      -4.1867e-02, -8.1171e-02, -6.7788e-02,  2.6172e-02, -8.6039e-01,\n",
       "                      -4.4378e-01,  1.0568e-01, -8.3743e-02,  3.1376e-01, -3.2190e-03,\n",
       "                      -6.2403e-01, -1.6885e-01,  1.4199e-01, -3.7185e-02,  1.0917e-01,\n",
       "                       2.9565e-01, -4.4082e-01,  1.4745e-01,  1.4421e-02, -2.9530e-01,\n",
       "                       3.1770e-01,  2.9604e-01,  4.8865e-01, -1.9902e-01, -1.7087e-01,\n",
       "                       3.6927e-01,  1.4904e-01, -3.6561e-01, -1.4642e-02,  1.2191e-01,\n",
       "                       2.4302e-01, -2.4129e-01,  4.3967e-02,  1.3581e-01,  1.7887e-03,\n",
       "                       4.3873e-01,  4.4073e-01,  4.5538e-01, -1.6603e-01, -7.3988e-01,\n",
       "                      -9.1239e-02,  2.0514e-01,  2.8536e-01, -5.5410e-02, -1.1051e+00,\n",
       "                       9.2338e-02,  7.1029e-01, -1.0013e-01, -2.9459e-01, -1.0843e-01,\n",
       "                       3.6299e-01,  1.2689e-01,  6.1026e-02, -1.2270e-01, -4.6009e-02,\n",
       "                       9.5160e-02,  3.9763e-04, -2.7771e-01, -2.3931e-01,  5.0355e-02,\n",
       "                       4.5008e-01, -2.3846e-01,  1.3790e-01, -1.9025e-01,  1.3666e-01,\n",
       "                      -2.5558e-01,  4.6812e-01,  2.2250e-01,  9.8323e-02,  2.9097e-02,\n",
       "                      -4.5846e-01,  2.0814e-01, -5.7575e-01,  3.7120e-01, -2.2419e-01,\n",
       "                       6.5767e-01,  6.0089e-01, -1.6498e-02, -4.7312e-01,  2.0039e-01,\n",
       "                       3.4838e-01, -4.6052e-02,  4.6920e-01, -1.7747e-01,  3.3883e-01,\n",
       "                      -1.1296e+00, -1.3683e-01, -6.0613e-01, -1.8287e-01, -2.4186e-01,\n",
       "                      -4.8068e-03,  2.0339e-01, -5.7654e-01, -1.3573e-01, -2.4914e-01,\n",
       "                      -1.4882e-01,  2.6803e-02, -3.1953e-01, -1.8288e-01, -1.0425e-01,\n",
       "                       4.0581e-01,  5.2090e-01, -1.0340e-02, -7.0862e-01, -2.7604e-01,\n",
       "                       3.9686e-01,  1.1373e-02, -6.6591e-01,  9.2320e-03, -5.6695e-01,\n",
       "                      -1.5816e-02, -7.8612e-02, -2.1292e-01,  6.2461e-01,  4.8900e-01,\n",
       "                       6.9032e-02,  2.8349e-01, -2.2894e-01])),\n",
       "             ('downs.3.conv.1.running_var',\n",
       "              tensor([0.1269, 0.1816, 0.1438, 0.1637, 0.1496, 0.2418, 0.2014, 0.1399, 0.1493,\n",
       "                      0.1756, 0.2030, 0.2613, 0.1547, 0.1720, 0.2236, 0.3195, 0.1762, 0.1718,\n",
       "                      0.1334, 0.1605, 0.1960, 0.2112, 0.1698, 0.1441, 0.1515, 0.2252, 0.1774,\n",
       "                      0.1707, 0.1637, 0.1824, 0.1726, 0.2182, 0.1539, 0.2075, 0.1345, 0.2699,\n",
       "                      0.1915, 0.1722, 0.1232, 0.2256, 0.1397, 0.1494, 0.1179, 0.2480, 0.1358,\n",
       "                      0.2203, 0.1792, 0.2317, 0.1142, 0.1458, 0.2027, 0.1384, 0.1964, 0.1382,\n",
       "                      0.3005, 0.1983, 0.3708, 0.2544, 0.1597, 0.1221, 0.1784, 0.1479, 0.1291,\n",
       "                      0.1300, 0.1614, 0.1330, 0.1638, 0.1655, 0.2352, 0.1724, 0.1459, 0.1848,\n",
       "                      0.1665, 0.2079, 0.2105, 0.1764, 0.1780, 0.2355, 0.1479, 0.1286, 0.1623,\n",
       "                      0.1293, 0.2517, 0.1524, 0.1600, 0.1766, 0.1608, 0.1638, 0.1472, 0.1304,\n",
       "                      0.1367, 0.1680, 0.1622, 0.1703, 0.1403, 0.2158, 0.1320, 0.1480, 0.1362,\n",
       "                      0.1875, 0.1792, 0.1252, 0.1819, 0.2006, 0.1699, 0.1667, 0.2105, 0.1334,\n",
       "                      0.1802, 0.1390, 0.1826, 0.2108, 0.1887, 0.2389, 0.1905, 0.1776, 0.1196,\n",
       "                      0.1959, 0.1456, 0.1800, 0.1615, 0.1768, 0.1963, 0.1428, 0.1746, 0.1886,\n",
       "                      0.1375, 0.1602])),\n",
       "             ('downs.3.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('downs.3.conv.3.weight',\n",
       "              tensor([[[[-2.1208e-02,  1.7766e-02,  1.8116e-02],\n",
       "                        [ 2.3981e-03,  1.8503e-02, -1.5506e-02],\n",
       "                        [ 2.3306e-02, -1.8209e-02,  1.7316e-02]],\n",
       "              \n",
       "                       [[ 2.6865e-02,  3.8276e-02, -2.5429e-02],\n",
       "                        [-2.2924e-02, -6.2529e-03, -3.1572e-02],\n",
       "                        [-4.0137e-03,  2.8540e-02,  8.1035e-03]],\n",
       "              \n",
       "                       [[ 1.0697e-02,  9.6681e-03,  1.1574e-02],\n",
       "                        [ 1.4477e-02, -2.4612e-02,  1.2642e-02],\n",
       "                        [-1.3877e-02,  1.8550e-02, -2.9856e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2569e-02, -2.5447e-02,  1.4834e-02],\n",
       "                        [ 6.2936e-03,  2.4468e-02,  1.4070e-02],\n",
       "                        [-2.1725e-02, -8.2018e-03, -1.5704e-02]],\n",
       "              \n",
       "                       [[ 3.5496e-03,  2.4344e-02, -5.2362e-03],\n",
       "                        [-4.8171e-03,  1.5791e-02, -2.0930e-02],\n",
       "                        [ 7.2680e-03,  1.9836e-02,  3.7906e-03]],\n",
       "              \n",
       "                       [[-3.1503e-03, -2.8167e-02, -1.2543e-02],\n",
       "                        [ 1.9599e-02, -1.7601e-02,  7.6051e-03],\n",
       "                        [ 1.7668e-02,  2.3519e-02, -1.6188e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1012e-02,  1.8737e-02, -2.1931e-02],\n",
       "                        [ 3.5931e-02, -2.2959e-02, -1.7466e-02],\n",
       "                        [-1.1584e-02, -1.2707e-03,  1.1421e-02]],\n",
       "              \n",
       "                       [[-6.7333e-03, -2.0143e-02, -2.0533e-02],\n",
       "                        [ 6.7396e-03,  8.3105e-03,  4.6775e-03],\n",
       "                        [ 7.8939e-03, -2.0136e-02, -1.6686e-02]],\n",
       "              \n",
       "                       [[-2.8178e-02, -1.2284e-02,  1.2479e-02],\n",
       "                        [ 1.3924e-02, -1.4611e-03,  1.2842e-02],\n",
       "                        [-5.4961e-03, -2.2611e-02, -3.2913e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4461e-02, -2.9070e-02, -2.0552e-02],\n",
       "                        [ 2.2656e-02, -2.0650e-02,  2.2794e-02],\n",
       "                        [ 5.1547e-03, -1.7680e-02,  1.3689e-02]],\n",
       "              \n",
       "                       [[ 7.2027e-03, -9.6362e-03, -5.3935e-03],\n",
       "                        [-1.1245e-03,  1.5338e-02,  2.7456e-02],\n",
       "                        [ 2.9947e-02, -1.3181e-02,  2.5977e-02]],\n",
       "              \n",
       "                       [[ 5.4322e-04, -8.7405e-03, -2.3297e-02],\n",
       "                        [-3.3462e-03,  6.9675e-03, -2.4971e-02],\n",
       "                        [-1.6491e-03, -2.7832e-02, -1.8909e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.4881e-03, -7.1272e-03, -2.4075e-02],\n",
       "                        [-2.5913e-03, -7.8820e-03,  2.4786e-02],\n",
       "                        [-2.4835e-03, -3.0176e-02, -1.1044e-02]],\n",
       "              \n",
       "                       [[-6.9194e-03,  1.8430e-02, -3.4361e-02],\n",
       "                        [ 5.4004e-03, -1.1334e-02, -4.5032e-03],\n",
       "                        [-1.8444e-02,  2.1291e-02, -9.6460e-03]],\n",
       "              \n",
       "                       [[ 1.1042e-02,  1.0642e-02,  2.6899e-02],\n",
       "                        [ 9.7656e-03, -5.2221e-03,  1.8806e-02],\n",
       "                        [-1.8710e-02,  2.1698e-02,  2.2561e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.7731e-02, -2.1890e-02,  2.2699e-02],\n",
       "                        [-1.9337e-02, -3.1143e-03, -1.9771e-02],\n",
       "                        [ 3.3203e-02,  6.3759e-03, -1.4825e-02]],\n",
       "              \n",
       "                       [[ 1.5279e-02, -3.9854e-02,  6.0796e-03],\n",
       "                        [-5.0938e-03,  9.9467e-03, -2.0198e-02],\n",
       "                        [-2.6032e-02,  2.5538e-02,  5.0141e-02]],\n",
       "              \n",
       "                       [[-8.7294e-03,  2.3816e-02, -4.3292e-03],\n",
       "                        [-4.3915e-02, -9.7407e-03,  1.1360e-02],\n",
       "                        [-1.1565e-02,  1.5014e-02, -1.6967e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.4662e-05, -2.9362e-02,  1.1977e-02],\n",
       "                        [ 1.2900e-02,  1.1076e-02, -3.1274e-03],\n",
       "                        [ 7.3488e-03, -1.6615e-02,  3.3353e-02]],\n",
       "              \n",
       "                       [[-1.0616e-02,  2.7201e-02,  2.9356e-02],\n",
       "                        [-1.9366e-02,  1.3793e-02,  1.1654e-02],\n",
       "                        [ 4.6636e-03, -2.5352e-02,  6.4205e-04]],\n",
       "              \n",
       "                       [[-8.5688e-04, -3.6602e-03,  1.5042e-02],\n",
       "                        [ 1.4156e-02,  1.3232e-02, -6.3585e-03],\n",
       "                        [ 3.4021e-02, -1.5109e-03,  2.5719e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.0709e-02, -1.6941e-03, -4.3436e-03],\n",
       "                        [-6.0138e-03,  4.0142e-02, -2.4405e-02],\n",
       "                        [ 2.4288e-02,  4.1221e-03, -3.0404e-02]],\n",
       "              \n",
       "                       [[-2.5191e-02,  1.3129e-02,  3.6989e-02],\n",
       "                        [ 2.4855e-03,  5.4882e-03, -2.1021e-02],\n",
       "                        [-9.0522e-03,  2.5816e-03,  7.6460e-03]],\n",
       "              \n",
       "                       [[-2.9964e-02,  8.6864e-03,  2.2238e-02],\n",
       "                        [-1.9025e-02, -2.8900e-02, -7.4186e-03],\n",
       "                        [-1.9811e-02, -1.2686e-02,  2.8668e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4899e-02,  2.0918e-02,  4.7208e-03],\n",
       "                        [-1.5676e-02, -1.0870e-02,  2.3633e-02],\n",
       "                        [-6.1068e-03,  1.1613e-02, -3.4745e-02]],\n",
       "              \n",
       "                       [[ 2.3315e-02, -9.5702e-03,  3.0520e-02],\n",
       "                        [-4.2150e-03, -2.4222e-02, -9.0900e-03],\n",
       "                        [ 2.1694e-02, -3.6416e-02, -2.3965e-02]],\n",
       "              \n",
       "                       [[-1.3824e-02,  2.4608e-02,  2.9890e-02],\n",
       "                        [-1.0622e-02, -8.8212e-03, -6.5162e-03],\n",
       "                        [ 8.3086e-03,  1.6423e-02, -1.6016e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-9.5236e-03,  1.2918e-02, -1.8639e-02],\n",
       "                        [ 1.5810e-02, -6.0710e-03, -3.2064e-02],\n",
       "                        [-1.5407e-02,  2.1940e-03, -2.0428e-02]],\n",
       "              \n",
       "                       [[-3.6460e-03,  1.2887e-02, -1.0011e-02],\n",
       "                        [-1.2404e-03,  1.1066e-02, -5.3254e-03],\n",
       "                        [-8.1456e-03,  1.3675e-02, -5.1008e-03]],\n",
       "              \n",
       "                       [[ 7.0562e-04,  1.6697e-02, -3.3612e-04],\n",
       "                        [ 2.8534e-02, -2.8552e-02, -5.8826e-03],\n",
       "                        [-2.9380e-02,  1.3725e-02, -1.4849e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.2775e-02, -2.0570e-02, -3.0348e-02],\n",
       "                        [ 2.2028e-03, -8.9507e-05, -1.0189e-03],\n",
       "                        [ 1.1429e-02, -7.6257e-03,  2.0096e-02]],\n",
       "              \n",
       "                       [[-1.9835e-02,  2.5905e-02, -1.2439e-02],\n",
       "                        [ 9.7406e-03,  3.0806e-02,  4.7935e-04],\n",
       "                        [-3.2292e-02, -6.6899e-03,  2.1427e-02]],\n",
       "              \n",
       "                       [[ 1.2746e-02,  2.7394e-03,  9.3948e-03],\n",
       "                        [-1.5201e-03,  2.2783e-02,  2.3070e-02],\n",
       "                        [ 1.7341e-02, -1.8223e-03, -1.3316e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.5630e-03,  7.4951e-03, -1.6462e-02],\n",
       "                        [-6.7708e-03,  3.1778e-03,  5.4777e-03],\n",
       "                        [ 1.3872e-02, -1.5098e-02, -1.3844e-02]],\n",
       "              \n",
       "                       [[ 1.8875e-02, -1.1291e-02,  2.0860e-02],\n",
       "                        [-6.7479e-03,  2.4058e-02,  5.7489e-04],\n",
       "                        [-2.0976e-02, -3.5828e-04, -8.4276e-03]],\n",
       "              \n",
       "                       [[-1.6145e-02, -3.4273e-02, -3.7132e-02],\n",
       "                        [ 1.7464e-03,  1.9456e-02, -2.6904e-02],\n",
       "                        [ 2.3277e-02,  6.2801e-03,  8.4730e-03]]]])),\n",
       "             ('downs.3.conv.4.weight',\n",
       "              tensor([0.9820, 0.9824, 0.9934, 0.9828, 0.9933, 0.9879, 0.9864, 0.9832, 0.9655,\n",
       "                      0.9807, 0.9814, 0.9928, 0.9987, 0.9928, 1.0033, 0.9778, 0.9861, 0.9829,\n",
       "                      0.9854, 0.9892, 0.9884, 1.0012, 0.9975, 0.9943, 0.9892, 0.9796, 0.9856,\n",
       "                      0.9920, 0.9903, 0.9740, 0.9932, 0.9999, 0.9809, 0.9941, 0.9925, 0.9952,\n",
       "                      0.9891, 0.9944, 1.0149, 0.9768, 0.9826, 0.9918, 0.9857, 0.9652, 0.9849,\n",
       "                      1.0012, 0.9895, 0.9993, 0.9873, 0.9815, 0.9881, 0.9874, 0.9728, 0.9829,\n",
       "                      0.9850, 0.9902, 0.9940, 0.9703, 0.9831, 0.9883, 0.9936, 0.9993, 1.0008,\n",
       "                      0.9978, 0.9932, 1.0003, 0.9841, 0.9659, 0.9890, 0.9785, 0.9869, 0.9856,\n",
       "                      0.9897, 0.9957, 0.9806, 0.9892, 0.9892, 0.9925, 0.9965, 0.9876, 0.9929,\n",
       "                      0.9875, 0.9877, 0.9836, 0.9951, 0.9925, 0.9822, 0.9926, 0.9822, 0.9689,\n",
       "                      0.9889, 0.9900, 0.9887, 0.9723, 1.0123, 0.9825, 0.9818, 0.9929, 0.9885,\n",
       "                      0.9695, 0.9885, 0.9900, 0.9941, 1.0016, 0.9879, 0.9849, 1.0050, 0.9886,\n",
       "                      0.9902, 0.9938, 0.9889, 0.9790, 0.9897, 0.9890, 0.9831, 0.9834, 0.9916,\n",
       "                      0.9874, 0.9788, 0.9872, 0.9986, 0.9928, 0.9745, 0.9878, 0.9807, 0.9953,\n",
       "                      0.9909, 0.9750])),\n",
       "             ('downs.3.conv.4.bias',\n",
       "              tensor([-0.0452, -0.0415, -0.0381, -0.0373, -0.0395, -0.0512, -0.0415, -0.0361,\n",
       "                      -0.0514, -0.0530, -0.0519, -0.0466, -0.0248, -0.0363, -0.0476, -0.0509,\n",
       "                      -0.0351, -0.0371, -0.0315, -0.0448, -0.0357, -0.0308, -0.0277, -0.0519,\n",
       "                      -0.0403, -0.0492, -0.0335, -0.0394, -0.0347, -0.0432, -0.0300, -0.0412,\n",
       "                      -0.0347, -0.0413, -0.0395, -0.0354, -0.0365, -0.0330, -0.0384, -0.0482,\n",
       "                      -0.0422, -0.0459, -0.0521, -0.0519, -0.0365, -0.0449, -0.0350, -0.0289,\n",
       "                      -0.0378, -0.0459, -0.0428, -0.0357, -0.0481, -0.0396, -0.0426, -0.0208,\n",
       "                      -0.0290, -0.0439, -0.0576, -0.0529, -0.0416, -0.0376, -0.0292, -0.0425,\n",
       "                      -0.0401, -0.0395, -0.0446, -0.0563, -0.0319, -0.0432, -0.0405, -0.0405,\n",
       "                      -0.0353, -0.0235, -0.0505, -0.0347, -0.0436, -0.0461, -0.0424, -0.0500,\n",
       "                      -0.0384, -0.0479, -0.0550, -0.0436, -0.0487, -0.0382, -0.0423, -0.0321,\n",
       "                      -0.0301, -0.0502, -0.0415, -0.0433, -0.0400, -0.0529, -0.0375, -0.0469,\n",
       "                      -0.0637, -0.0423, -0.0366, -0.0481, -0.0344, -0.0426, -0.0364, -0.0192,\n",
       "                      -0.0421, -0.0594, -0.0402, -0.0383, -0.0405, -0.0385, -0.0357, -0.0430,\n",
       "                      -0.0472, -0.0448, -0.0467, -0.0460, -0.0398, -0.0409, -0.0434, -0.0222,\n",
       "                      -0.0205, -0.0474, -0.0515, -0.0393, -0.0398, -0.0463, -0.0326, -0.0480])),\n",
       "             ('downs.3.conv.4.running_mean',\n",
       "              tensor([-1.5588e-01,  7.2955e-02, -1.4531e-01, -2.1890e-01, -5.8685e-02,\n",
       "                      -2.7759e-01, -2.3078e-01,  6.3241e-02,  1.0191e-01,  8.0446e-02,\n",
       "                      -2.7570e-02, -2.3621e-01, -1.6566e-01, -4.8661e-01, -2.9939e-01,\n",
       "                      -2.0626e-01, -1.6166e-01, -4.1673e-02, -9.0533e-02, -1.4679e-01,\n",
       "                      -2.3230e-01, -1.8856e-01, -2.3442e-01, -3.8764e-01, -8.1949e-02,\n",
       "                      -2.2087e-01,  1.6529e-01,  1.2537e-01, -1.2095e-01, -8.4414e-02,\n",
       "                      -1.6818e-01, -2.7875e-01,  3.1592e-01, -3.1132e-01, -3.6915e-01,\n",
       "                      -5.7896e-02, -2.5731e-01,  3.6665e-01, -1.2577e-02,  8.8534e-02,\n",
       "                      -3.6551e-01, -1.9824e-01,  1.1198e-02, -2.4070e-02, -2.5422e-01,\n",
       "                      -1.2976e-01, -5.4584e-02, -1.3691e-01, -6.6150e-02, -1.8972e-01,\n",
       "                      -1.3748e-01, -7.2483e-02, -3.6026e-02,  7.5640e-03, -1.1698e-01,\n",
       "                       6.0793e-02, -3.1259e-01, -1.9414e-02,  5.5890e-02, -1.4898e-01,\n",
       "                      -2.5874e-01, -3.7960e-02, -2.4302e-01, -3.2322e-01, -1.0111e-01,\n",
       "                      -1.9299e-01,  1.0731e-01, -1.9059e-02, -5.7545e-02, -1.7110e-01,\n",
       "                      -9.7087e-02,  2.6841e-01, -1.4148e-01, -3.3329e-02, -1.5426e-01,\n",
       "                      -1.6996e-01,  1.5149e-01,  1.6661e-01, -1.6149e-01, -2.8128e-01,\n",
       "                      -3.0834e-01, -2.3106e-01, -4.9837e-01, -9.7572e-02, -1.2999e-01,\n",
       "                       8.0861e-02, -1.0310e-01, -3.7002e-02, -1.8179e-01, -5.1171e-02,\n",
       "                      -1.2432e-01, -3.9874e-01, -7.6143e-02, -1.4266e-01, -1.1549e-01,\n",
       "                       3.6494e-04, -3.2075e-01, -1.1631e-01, -2.7742e-01,  4.1164e-02,\n",
       "                       6.2596e-02, -1.4246e-01, -3.6039e-01, -1.6422e-01, -1.8030e-01,\n",
       "                      -2.7276e-01, -2.6530e-01, -1.3842e-02, -1.6375e-01, -9.2341e-02,\n",
       "                      -1.7899e-01, -1.4179e-01, -1.6549e-01, -5.7284e-02, -2.2519e-01,\n",
       "                       4.1595e-02, -1.8418e-01, -2.7658e-01, -1.4586e-01, -2.4973e-01,\n",
       "                       7.7766e-02,  3.7740e-02, -7.4270e-02,  7.5711e-02, -1.8679e-01,\n",
       "                      -1.5811e-01, -3.5561e-01, -2.0799e-01])),\n",
       "             ('downs.3.conv.4.running_var',\n",
       "              tensor([0.1291, 0.1219, 0.1067, 0.1157, 0.1189, 0.1185, 0.1247, 0.1239, 0.1543,\n",
       "                      0.1424, 0.1260, 0.1273, 0.1187, 0.1603, 0.1499, 0.1211, 0.1357, 0.1423,\n",
       "                      0.1033, 0.1532, 0.1452, 0.0995, 0.1303, 0.1530, 0.1486, 0.1193, 0.1006,\n",
       "                      0.1252, 0.1316, 0.1195, 0.1137, 0.1390, 0.1150, 0.1512, 0.1290, 0.1338,\n",
       "                      0.1090, 0.1135, 0.1112, 0.1213, 0.1495, 0.1544, 0.1292, 0.1293, 0.1125,\n",
       "                      0.1232, 0.1152, 0.1126, 0.1390, 0.0991, 0.1247, 0.1131, 0.1185, 0.1072,\n",
       "                      0.1184, 0.1459, 0.1318, 0.1510, 0.1247, 0.1203, 0.1217, 0.1140, 0.1488,\n",
       "                      0.1657, 0.1260, 0.1089, 0.1096, 0.1508, 0.1220, 0.1493, 0.1170, 0.1383,\n",
       "                      0.1244, 0.1150, 0.1119, 0.1310, 0.1385, 0.1281, 0.1248, 0.1267, 0.1815,\n",
       "                      0.1364, 0.1215, 0.1101, 0.1400, 0.1077, 0.1384, 0.1171, 0.1285, 0.1225,\n",
       "                      0.1108, 0.1425, 0.1164, 0.1490, 0.1186, 0.1389, 0.1928, 0.1330, 0.1323,\n",
       "                      0.1222, 0.1604, 0.1141, 0.1208, 0.1226, 0.1275, 0.1163, 0.1313, 0.1285,\n",
       "                      0.1295, 0.1288, 0.1150, 0.1142, 0.1359, 0.1224, 0.1272, 0.1091, 0.1281,\n",
       "                      0.1749, 0.1615, 0.1226, 0.1227, 0.1178, 0.1235, 0.1269, 0.1286, 0.1656,\n",
       "                      0.1645, 0.1517])),\n",
       "             ('downs.3.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('bottleneck.conv.0.weight',\n",
       "              tensor([[[[-9.5885e-03,  1.5461e-02, -1.0172e-02],\n",
       "                        [ 2.2857e-02,  3.1693e-02,  2.0003e-02],\n",
       "                        [-2.2373e-02, -8.9569e-03,  2.5320e-02]],\n",
       "              \n",
       "                       [[ 1.7480e-02,  5.4659e-03,  1.3719e-02],\n",
       "                        [ 1.4697e-02, -3.6303e-02, -2.0509e-02],\n",
       "                        [ 1.7049e-02,  9.9232e-05, -8.9877e-03]],\n",
       "              \n",
       "                       [[-1.3594e-02,  1.5344e-02, -2.9879e-02],\n",
       "                        [ 2.4319e-02, -1.8261e-02, -5.1437e-03],\n",
       "                        [-2.0246e-02, -4.7807e-03, -6.5829e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.3579e-03,  2.2755e-03,  1.2971e-02],\n",
       "                        [-2.0421e-02, -1.4829e-02, -1.7392e-02],\n",
       "                        [-9.9004e-03, -3.7899e-02, -3.3328e-02]],\n",
       "              \n",
       "                       [[ 8.7983e-03, -2.4286e-02, -2.1351e-02],\n",
       "                        [-2.3783e-02,  4.5797e-04,  1.7941e-02],\n",
       "                        [ 2.8188e-03, -2.0824e-02,  3.3973e-02]],\n",
       "              \n",
       "                       [[-2.1476e-02,  2.3991e-02,  2.6905e-02],\n",
       "                        [-1.5406e-02,  3.7294e-02,  6.6142e-03],\n",
       "                        [ 1.7255e-02, -4.6603e-03,  7.9206e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.9170e-03, -4.1222e-03, -2.4068e-02],\n",
       "                        [ 4.0840e-02, -2.3859e-02,  9.8421e-04],\n",
       "                        [ 2.4715e-02,  2.9812e-02,  2.8991e-02]],\n",
       "              \n",
       "                       [[-3.0973e-03,  2.5587e-02, -3.3125e-02],\n",
       "                        [ 2.6830e-02, -3.7917e-02,  5.6146e-03],\n",
       "                        [ 1.4405e-02, -2.8523e-02, -3.8566e-03]],\n",
       "              \n",
       "                       [[ 1.6531e-02,  1.6387e-02, -1.4163e-02],\n",
       "                        [ 5.5248e-03,  3.7912e-04,  3.0616e-02],\n",
       "                        [ 2.5700e-02, -8.3409e-03, -1.4807e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.5866e-03,  2.8312e-02,  1.3635e-02],\n",
       "                        [ 6.9024e-03,  1.6627e-02,  2.1398e-02],\n",
       "                        [-4.6045e-03, -3.1541e-02, -1.7203e-02]],\n",
       "              \n",
       "                       [[-1.1962e-02,  3.1088e-02,  2.5709e-02],\n",
       "                        [ 1.5665e-02,  2.2754e-03,  7.3685e-03],\n",
       "                        [-2.3605e-02,  3.5570e-03,  3.7769e-02]],\n",
       "              \n",
       "                       [[ 1.6082e-02, -1.9799e-02,  2.4119e-02],\n",
       "                        [ 1.7044e-02,  2.1003e-02,  5.5236e-03],\n",
       "                        [-2.5813e-02, -2.6135e-02, -1.5524e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.7950e-02, -1.6598e-02, -3.0290e-04],\n",
       "                        [-1.3768e-02,  2.4372e-02,  4.7927e-02],\n",
       "                        [-7.9508e-03,  2.6722e-04, -1.9391e-02]],\n",
       "              \n",
       "                       [[-3.1055e-02, -1.3267e-02,  1.1219e-02],\n",
       "                        [ 2.3521e-03,  2.2629e-03, -1.8659e-03],\n",
       "                        [ 7.1419e-03, -1.9725e-02,  6.4650e-03]],\n",
       "              \n",
       "                       [[ 1.3872e-02,  2.5559e-02, -5.4354e-03],\n",
       "                        [-4.5457e-03,  3.0258e-02, -6.9516e-03],\n",
       "                        [ 2.1240e-02,  2.9205e-02,  2.1797e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.1030e-02, -1.3504e-02,  2.3079e-02],\n",
       "                        [ 1.0192e-02,  1.9250e-02,  3.6547e-03],\n",
       "                        [-1.1754e-02,  2.1895e-03,  4.4462e-03]],\n",
       "              \n",
       "                       [[ 3.8674e-03,  3.2775e-02, -3.0293e-02],\n",
       "                        [ 2.3331e-02,  3.5627e-03,  2.3918e-02],\n",
       "                        [-4.6381e-03, -2.4431e-02,  2.0473e-02]],\n",
       "              \n",
       "                       [[ 2.9885e-03,  7.6077e-03,  2.0854e-02],\n",
       "                        [ 4.3798e-03,  8.5661e-05, -2.2985e-02],\n",
       "                        [-1.6853e-02, -1.0125e-02, -7.5500e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.1620e-02,  1.0587e-02,  2.1923e-02],\n",
       "                        [ 2.0386e-02,  2.6401e-02, -1.4457e-02],\n",
       "                        [ 1.0354e-02, -8.5801e-03, -2.8145e-02]],\n",
       "              \n",
       "                       [[ 1.8794e-02,  7.7287e-03, -2.9394e-02],\n",
       "                        [ 2.1496e-02,  2.5501e-02,  2.1375e-02],\n",
       "                        [-2.3957e-02, -1.9711e-02,  1.3147e-02]],\n",
       "              \n",
       "                       [[-6.3746e-04,  1.5492e-02,  2.1830e-02],\n",
       "                        [-1.9139e-02,  1.7069e-02, -1.2892e-02],\n",
       "                        [ 1.1824e-02,  3.5154e-02,  2.7801e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.8488e-02,  6.1703e-03, -1.6585e-02],\n",
       "                        [ 7.6319e-04, -1.8341e-02, -5.9199e-03],\n",
       "                        [ 6.3450e-04,  2.6814e-03,  6.5320e-03]],\n",
       "              \n",
       "                       [[-1.1860e-02,  1.1240e-02, -6.4611e-03],\n",
       "                        [-1.5307e-02, -6.0025e-04, -1.2802e-03],\n",
       "                        [ 7.2320e-03,  8.8671e-03, -1.9584e-02]],\n",
       "              \n",
       "                       [[-3.2269e-02, -2.0995e-02, -2.8736e-03],\n",
       "                        [-2.1482e-02, -1.2945e-02,  3.1113e-03],\n",
       "                        [-2.4280e-02,  3.3173e-03,  1.5638e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4183e-03, -2.6936e-02,  1.0091e-02],\n",
       "                        [ 1.5008e-02, -2.7525e-02,  3.1329e-02],\n",
       "                        [ 1.4481e-02,  2.8402e-02, -2.7286e-03]],\n",
       "              \n",
       "                       [[ 8.6212e-03, -3.1921e-02, -9.0049e-03],\n",
       "                        [ 1.8597e-02,  1.2854e-02, -3.4340e-02],\n",
       "                        [ 1.7383e-02, -3.7472e-03,  3.5738e-02]],\n",
       "              \n",
       "                       [[ 8.3832e-03,  1.0437e-02, -6.3407e-03],\n",
       "                        [-3.4115e-02,  1.2274e-03, -1.6051e-02],\n",
       "                        [-3.1315e-02,  2.1622e-02, -1.0645e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2325e-02,  1.8638e-02, -5.9622e-03],\n",
       "                        [-9.9818e-03,  1.8639e-02,  2.8176e-02],\n",
       "                        [-9.0526e-04,  2.4675e-02, -1.8747e-02]],\n",
       "              \n",
       "                       [[ 7.8134e-04,  2.0139e-02,  2.4706e-02],\n",
       "                        [-8.0719e-03,  2.3021e-03,  8.3614e-04],\n",
       "                        [-1.3618e-02, -3.3383e-02,  2.1179e-02]],\n",
       "              \n",
       "                       [[-2.4279e-02, -1.3534e-02, -2.8755e-02],\n",
       "                        [-1.5237e-02,  1.5181e-02, -2.8535e-02],\n",
       "                        [-2.7683e-03, -2.7682e-02,  2.6906e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.1383e-03,  8.0161e-03, -2.2299e-02],\n",
       "                        [-1.7319e-02, -7.6918e-03, -2.7377e-02],\n",
       "                        [-8.3769e-03, -2.3782e-02,  1.0333e-02]],\n",
       "              \n",
       "                       [[-2.9473e-02,  2.5725e-02,  3.5517e-05],\n",
       "                        [-2.2256e-03, -1.8762e-02, -8.9458e-03],\n",
       "                        [-2.9251e-03,  3.5193e-03,  7.6563e-03]],\n",
       "              \n",
       "                       [[ 9.1815e-03,  1.5716e-02,  1.1270e-02],\n",
       "                        [-2.0842e-02, -1.6945e-02,  2.0381e-02],\n",
       "                        [ 3.2740e-02,  2.9106e-02,  9.1910e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.5954e-02,  3.2334e-02,  8.8390e-03],\n",
       "                        [-7.0858e-03,  9.9687e-03, -1.2201e-02],\n",
       "                        [ 4.1829e-03,  2.0580e-02, -1.6277e-02]],\n",
       "              \n",
       "                       [[ 2.4891e-02,  6.8552e-03,  1.6217e-02],\n",
       "                        [-1.2263e-02,  2.4116e-02,  1.4463e-02],\n",
       "                        [ 1.7864e-02, -1.4166e-05, -2.1641e-02]],\n",
       "              \n",
       "                       [[ 8.3873e-03,  4.6395e-03,  4.1007e-03],\n",
       "                        [ 2.3581e-03,  8.0070e-03, -1.8205e-02],\n",
       "                        [-6.3556e-03,  2.4269e-02, -2.9673e-02]]]])),\n",
       "             ('bottleneck.conv.1.weight',\n",
       "              tensor([0.9836, 1.0029, 0.9951, 0.9820, 1.0252, 0.9834, 1.0045, 0.9897, 0.9971,\n",
       "                      1.0086, 0.9982, 0.9549, 0.9990, 1.0042, 1.0057, 0.9948, 0.9846, 0.9898,\n",
       "                      1.0002, 0.9977, 1.0034, 1.0120, 1.0038, 1.0008, 1.0045, 0.9918, 0.9936,\n",
       "                      1.0153, 1.0290, 0.9914, 1.0086, 1.0104, 0.9969, 1.0024, 0.9876, 0.9929,\n",
       "                      0.9839, 0.9672, 1.0023, 0.9981, 1.0135, 1.0175, 0.9931, 0.9983, 0.9903,\n",
       "                      0.9957, 0.9979, 0.9954, 1.0074, 1.0179, 0.9703, 1.0171, 0.9917, 1.0049,\n",
       "                      0.9835, 0.9993, 1.0075, 0.9901, 1.0107, 1.0095, 0.9921, 1.0027, 0.9914,\n",
       "                      1.0069, 1.0033, 1.0027, 1.0106, 1.0080, 0.9947, 0.9912, 0.9946, 0.9942,\n",
       "                      0.9991, 0.9947, 1.0020, 0.9871, 0.9742, 0.9881, 0.9778, 1.0037, 1.0171,\n",
       "                      0.9873, 1.0113, 0.9933, 0.9402, 1.0087, 1.0067, 1.0035, 0.9842, 0.9967,\n",
       "                      0.9944, 1.0067, 0.9872, 1.0037, 1.0036, 0.9976, 0.9890, 1.0068, 1.0081,\n",
       "                      1.0007, 0.9999, 0.9971, 0.9874, 0.9739, 0.9960, 1.0258, 1.0015, 0.9952,\n",
       "                      1.0043, 1.0139, 0.9903, 1.0107, 0.9951, 1.0036, 0.9986, 1.0047, 1.0152,\n",
       "                      1.0110, 1.0115, 1.0101, 0.9866, 1.0021, 1.0043, 0.9848, 1.0039, 1.0114,\n",
       "                      1.0108, 0.9997, 0.9951, 1.0037, 1.0067, 0.9994, 0.9931, 0.9980, 1.0047,\n",
       "                      1.0127, 0.9854, 0.9845, 0.9959, 0.9992, 0.9933, 0.9991, 1.0146, 0.9902,\n",
       "                      1.0028, 1.0127, 0.9848, 0.9998, 1.0186, 1.0203, 1.0123, 0.9889, 1.0025,\n",
       "                      1.0046, 0.9987, 1.0138, 0.9907, 1.0045, 0.9919, 0.9932, 0.9949, 1.0158,\n",
       "                      0.9871, 0.9979, 0.9939, 0.9938, 0.9745, 0.9809, 1.0124, 1.0080, 1.0103,\n",
       "                      0.9773, 1.0170, 0.9994, 0.9965, 1.0194, 0.9965, 0.9881, 0.9904, 0.9894,\n",
       "                      1.0028, 0.9932, 0.9962, 1.0032, 1.0068, 1.0036, 0.9656, 0.9966, 0.9775,\n",
       "                      1.0040, 0.9813, 1.0042, 0.9786, 1.0077, 1.0035, 1.0145, 0.9845, 1.0039,\n",
       "                      1.0156, 0.9923, 0.9935, 0.9938, 0.9986, 1.0069, 0.9944, 1.0022, 1.0067,\n",
       "                      0.9878, 0.9903, 0.9950, 0.9884, 0.9826, 1.0099, 1.0047, 0.9891, 1.0247,\n",
       "                      1.0064, 1.0060, 1.0080, 1.0034, 1.0222, 1.0181, 0.9961, 1.0077, 0.9961,\n",
       "                      1.0095, 0.9877, 0.9914, 0.9882, 0.9675, 1.0067, 0.9769, 0.9862, 1.0057,\n",
       "                      1.0076, 0.9963, 0.9814, 1.0194, 0.9855, 1.0006, 0.9937, 0.9950, 1.0006,\n",
       "                      1.0041, 0.9977, 1.0166, 1.0081, 0.9985, 0.9781, 0.9972, 1.0110, 1.0031,\n",
       "                      1.0174, 1.0087, 1.0163, 1.0021])),\n",
       "             ('bottleneck.conv.1.bias',\n",
       "              tensor([-0.0306, -0.0235, -0.0367, -0.0434, -0.0232, -0.0363, -0.0380, -0.0508,\n",
       "                      -0.0373, -0.0233, -0.0456, -0.0517, -0.0257, -0.0284, -0.0324, -0.0385,\n",
       "                      -0.0330, -0.0299, -0.0301, -0.0322, -0.0340, -0.0230, -0.0245, -0.0328,\n",
       "                      -0.0354, -0.0378, -0.0314, -0.0279, -0.0153, -0.0347, -0.0376, -0.0174,\n",
       "                      -0.0395, -0.0242, -0.0495, -0.0320, -0.0333, -0.0445, -0.0394, -0.0252,\n",
       "                      -0.0418, -0.0174, -0.0326, -0.0335, -0.0393, -0.0281, -0.0353, -0.0476,\n",
       "                      -0.0351, -0.0325, -0.0432, -0.0372, -0.0351, -0.0269, -0.0524, -0.0347,\n",
       "                      -0.0309, -0.0322, -0.0240, -0.0265, -0.0361, -0.0344, -0.0497, -0.0255,\n",
       "                      -0.0276, -0.0410, -0.0386, -0.0209, -0.0319, -0.0355, -0.0337, -0.0196,\n",
       "                      -0.0252, -0.0403, -0.0259, -0.0415, -0.0416, -0.0582, -0.0349, -0.0209,\n",
       "                      -0.0408, -0.0259, -0.0355, -0.0338, -0.0686, -0.0352, -0.0237, -0.0285,\n",
       "                      -0.0401, -0.0452, -0.0368, -0.0309, -0.0406, -0.0179, -0.0311, -0.0304,\n",
       "                      -0.0464, -0.0257, -0.0293, -0.0315, -0.0245, -0.0358, -0.0436, -0.0472,\n",
       "                      -0.0429, -0.0200, -0.0340, -0.0372, -0.0281, -0.0156, -0.0307, -0.0240,\n",
       "                      -0.0418, -0.0378, -0.0415, -0.0274, -0.0183, -0.0123, -0.0236, -0.0207,\n",
       "                      -0.0362, -0.0429, -0.0290, -0.0440, -0.0216, -0.0350, -0.0193, -0.0307,\n",
       "                      -0.0293, -0.0267, -0.0133, -0.0266, -0.0330, -0.0412, -0.0255, -0.0219,\n",
       "                      -0.0325, -0.0374, -0.0383, -0.0353, -0.0318, -0.0350, -0.0302, -0.0456,\n",
       "                      -0.0303, -0.0260, -0.0448, -0.0250, -0.0242, -0.0263, -0.0354, -0.0389,\n",
       "                      -0.0175, -0.0262, -0.0366, -0.0274, -0.0432, -0.0176, -0.0411, -0.0330,\n",
       "                      -0.0445, -0.0235, -0.0329, -0.0285, -0.0341, -0.0159, -0.0384, -0.0550,\n",
       "                      -0.0266, -0.0192, -0.0094, -0.0367, -0.0214, -0.0230, -0.0379, -0.0288,\n",
       "                      -0.0400, -0.0420, -0.0358, -0.0398, -0.0394, -0.0323, -0.0290, -0.0335,\n",
       "                      -0.0290, -0.0436, -0.0474, -0.0276, -0.0537, -0.0385, -0.0430, -0.0282,\n",
       "                      -0.0523, -0.0321, -0.0422, -0.0129, -0.0500, -0.0529, -0.0176, -0.0418,\n",
       "                      -0.0366, -0.0348, -0.0306, -0.0334, -0.0344, -0.0361, -0.0303, -0.0342,\n",
       "                      -0.0348, -0.0370, -0.0383, -0.0350, -0.0257, -0.0348, -0.0408, -0.0296,\n",
       "                      -0.0284, -0.0290, -0.0355, -0.0241, -0.0185, -0.0213, -0.0425, -0.0263,\n",
       "                      -0.0327, -0.0305, -0.0333, -0.0313, -0.0345, -0.0411, -0.0267, -0.0407,\n",
       "                      -0.0507, -0.0395, -0.0349, -0.0215, -0.0397, -0.0208, -0.0432, -0.0365,\n",
       "                      -0.0362, -0.0304, -0.0275, -0.0248, -0.0375, -0.0146, -0.0288, -0.0393,\n",
       "                      -0.0398, -0.0401, -0.0196, -0.0277, -0.0342, -0.0271, -0.0213, -0.0290])),\n",
       "             ('bottleneck.conv.1.running_mean',\n",
       "              tensor([-9.6589e-02, -5.3685e-01,  1.9067e-02, -4.7715e-02, -1.2329e-02,\n",
       "                       1.6593e-01,  1.5783e-01, -2.9810e-02, -1.8350e-01,  8.2659e-02,\n",
       "                       1.5187e-01,  2.0704e-01,  4.2753e-01,  2.7954e-01, -2.8406e-01,\n",
       "                      -7.0755e-02,  1.8492e-01,  1.5284e-01,  1.9036e-01,  1.3256e-01,\n",
       "                      -5.5353e-02,  9.7305e-02, -5.5181e-01, -1.9042e-01, -3.6002e-01,\n",
       "                       5.2589e-02, -1.4332e-01, -4.0276e-02,  4.1628e-01,  2.0501e-01,\n",
       "                       1.5017e-01,  8.4956e-02,  1.5903e-01,  1.5268e-01, -3.2196e-01,\n",
       "                      -2.6418e-01, -3.2574e-01, -7.2734e-01,  3.2626e-02,  3.1316e-01,\n",
       "                      -1.3144e-01, -3.9488e-01,  2.0103e-01,  4.4335e-01,  3.2129e-02,\n",
       "                       1.3168e-01,  7.6689e-01,  7.1370e-01, -3.8518e-01,  2.2632e-01,\n",
       "                       9.5514e-02, -1.0100e-01,  1.4581e-01,  2.3282e-01, -5.7348e-02,\n",
       "                       4.6830e-02,  2.7368e-02, -1.5055e-01,  4.4444e-01,  1.6213e-01,\n",
       "                      -1.3185e-02, -4.9972e-01, -1.1515e-01, -1.5962e-02,  1.4156e-01,\n",
       "                      -2.2018e-01, -1.9991e-01,  1.3249e-01, -1.0192e-01, -1.5929e-01,\n",
       "                       2.2871e-01, -8.1796e-02,  1.4096e-01,  4.7131e-02,  1.3149e-01,\n",
       "                       1.5004e-01,  2.6861e-02,  3.2461e-02, -1.6737e-01, -2.3840e-01,\n",
       "                      -1.8733e-01, -1.0178e-01,  1.5978e-01, -3.0307e-01, -3.8080e-01,\n",
       "                      -4.8947e-01, -9.1345e-02, -3.6528e-01, -1.0577e-02,  1.2101e-01,\n",
       "                       3.8833e-01,  5.1811e-01, -3.5345e-02,  3.2695e-01,  4.6211e-01,\n",
       "                      -2.2307e-01, -2.4153e-01, -1.4362e-02,  1.2982e-02, -1.0696e-01,\n",
       "                       3.7193e-01,  5.3778e-01, -3.2346e-01, -5.9630e-01, -3.6316e-01,\n",
       "                       2.6403e-01,  2.7642e-01, -1.3887e-01,  5.6351e-02,  2.8847e-02,\n",
       "                      -1.8646e-01, -5.4255e-02, -4.3380e-02, -2.8037e-01,  6.8869e-01,\n",
       "                      -4.1688e-01, -3.6561e-02,  5.3723e-01,  3.1920e-01, -1.4578e-02,\n",
       "                      -4.0073e-01,  2.5219e-01,  1.8797e-01, -2.8434e-01,  1.5258e-01,\n",
       "                       3.2115e-01,  1.2099e-01, -2.8137e-01, -1.2419e-01, -2.2880e-01,\n",
       "                       5.1074e-02, -2.2236e-01, -1.0006e-01,  6.7460e-04, -5.4357e-02,\n",
       "                      -1.7970e-01, -1.4539e-01, -4.8575e-01,  3.8273e-01, -2.7469e-01,\n",
       "                       2.6631e-01, -7.7294e-01,  5.8678e-01,  4.7012e-02, -2.0311e-01,\n",
       "                      -1.6415e-01, -2.6553e-01, -4.5907e-01, -1.3463e-01, -3.0899e-01,\n",
       "                       6.1516e-02,  6.6288e-01, -2.7470e-01,  6.7839e-02,  1.8503e-01,\n",
       "                       3.4597e-01, -4.1958e-01, -1.1097e-01, -3.9550e-01,  1.6733e-01,\n",
       "                      -3.8105e-02,  5.1915e-02, -3.8323e-01, -7.4088e-02,  1.7799e-02,\n",
       "                      -7.0034e-02,  2.8863e-02,  1.6773e-01, -1.7039e-02,  3.9756e-01,\n",
       "                      -2.5831e-01,  2.4896e-01, -2.7529e-01, -2.1617e-01, -4.6345e-01,\n",
       "                      -6.6084e-01,  1.2630e-02,  1.9043e-02, -2.7647e-01,  3.2852e-01,\n",
       "                       1.1122e-02,  9.6774e-02,  5.6280e-03, -5.2469e-02, -5.0371e-03,\n",
       "                      -4.3113e-01, -2.6951e-01, -5.8951e-01, -9.5297e-02, -2.7920e-01,\n",
       "                       9.0510e-02,  3.3050e-01,  9.0166e-01, -2.7663e-01, -2.9062e-01,\n",
       "                       3.7061e-01,  1.8606e-02, -1.2167e-01, -1.8585e-01,  6.2390e-01,\n",
       "                      -3.2832e-01,  1.6036e-01, -2.8514e-01,  1.5112e-03, -9.8644e-04,\n",
       "                      -1.2522e-01, -3.2369e-01,  1.9606e-01,  7.2815e-01, -2.0479e-01,\n",
       "                       1.5233e-02,  1.1622e-01, -2.9098e-01,  2.7893e-01, -5.2138e-01,\n",
       "                      -1.1033e-01,  2.4709e-01,  1.7770e-01,  2.3521e-01, -2.4464e-01,\n",
       "                       2.4552e-01,  2.6763e-02,  1.2125e-01,  9.2735e-03,  5.8761e-02,\n",
       "                      -1.4094e-01, -5.0134e-01,  3.7800e-01,  3.4909e-01, -1.4786e-01,\n",
       "                       6.1021e-01, -5.1714e-02, -9.3131e-02,  1.3396e-01,  1.8911e-01,\n",
       "                       4.0462e-01, -6.1746e-01, -3.8559e-01, -5.0171e-02, -4.7407e-01,\n",
       "                       4.2441e-01,  1.5435e-01,  4.5038e-01, -4.7376e-02,  1.1856e-03,\n",
       "                      -1.9048e-02,  3.0514e-01, -7.8897e-02, -3.6848e-02, -1.8510e-01,\n",
       "                       4.4954e-01, -2.2980e-02,  2.2020e-01, -9.0222e-02,  6.7059e-01,\n",
       "                       5.5084e-01])),\n",
       "             ('bottleneck.conv.1.running_var',\n",
       "              tensor([0.0909, 0.0909, 0.1015, 0.1075, 0.1029, 0.0998, 0.0894, 0.1286, 0.0950,\n",
       "                      0.0904, 0.0893, 0.1299, 0.0914, 0.0944, 0.0956, 0.1039, 0.0801, 0.0944,\n",
       "                      0.0930, 0.0910, 0.0809, 0.0875, 0.1008, 0.1018, 0.0906, 0.0918, 0.0820,\n",
       "                      0.0993, 0.0890, 0.0888, 0.0817, 0.0951, 0.0880, 0.0858, 0.1111, 0.1055,\n",
       "                      0.0909, 0.1176, 0.1038, 0.0996, 0.0884, 0.1071, 0.0853, 0.1112, 0.0970,\n",
       "                      0.1001, 0.1022, 0.1029, 0.0884, 0.0937, 0.1038, 0.0825, 0.0974, 0.1039,\n",
       "                      0.0888, 0.0910, 0.0994, 0.0824, 0.0834, 0.0823, 0.1004, 0.0902, 0.1096,\n",
       "                      0.0989, 0.0848, 0.0992, 0.0835, 0.0894, 0.0931, 0.1056, 0.1043, 0.0932,\n",
       "                      0.1077, 0.0810, 0.0892, 0.0912, 0.1385, 0.0838, 0.1392, 0.1059, 0.0785,\n",
       "                      0.0912, 0.0792, 0.1346, 0.2297, 0.0919, 0.0944, 0.0928, 0.1075, 0.0898,\n",
       "                      0.1058, 0.0918, 0.0979, 0.0898, 0.1154, 0.0779, 0.0967, 0.0849, 0.0987,\n",
       "                      0.0868, 0.0857, 0.0892, 0.0910, 0.0937, 0.0932, 0.0980, 0.1001, 0.0942,\n",
       "                      0.0855, 0.0870, 0.0923, 0.0933, 0.1030, 0.0991, 0.0998, 0.0939, 0.0866,\n",
       "                      0.0936, 0.0859, 0.0920, 0.1093, 0.0939, 0.0837, 0.0870, 0.0833, 0.0862,\n",
       "                      0.0865, 0.0796, 0.0880, 0.0899, 0.0886, 0.0955, 0.0848, 0.1082, 0.0946,\n",
       "                      0.0851, 0.0980, 0.0947, 0.1165, 0.1024, 0.1021, 0.1038, 0.0854, 0.0922,\n",
       "                      0.0981, 0.0864, 0.1014, 0.0913, 0.0985, 0.1024, 0.0838, 0.1150, 0.0906,\n",
       "                      0.0874, 0.0916, 0.0930, 0.0839, 0.0902, 0.1022, 0.0961, 0.0908, 0.0816,\n",
       "                      0.1156, 0.0870, 0.0969, 0.0827, 0.1126, 0.0933, 0.0848, 0.0876, 0.0866,\n",
       "                      0.1002, 0.0730, 0.1137, 0.0909, 0.0920, 0.0985, 0.0950, 0.0817, 0.0900,\n",
       "                      0.1031, 0.1034, 0.0970, 0.0845, 0.0876, 0.0887, 0.1052, 0.1075, 0.1025,\n",
       "                      0.1046, 0.1000, 0.0885, 0.0960, 0.0860, 0.0876, 0.0964, 0.1031, 0.0856,\n",
       "                      0.1034, 0.0947, 0.1001, 0.1228, 0.0836, 0.1011, 0.0882, 0.1000, 0.0903,\n",
       "                      0.1089, 0.1115, 0.1105, 0.0916, 0.0888, 0.0818, 0.0942, 0.0960, 0.0885,\n",
       "                      0.0894, 0.0892, 0.0876, 0.1004, 0.0859, 0.0930, 0.0988, 0.1047, 0.1014,\n",
       "                      0.0932, 0.1126, 0.0894, 0.0794, 0.1739, 0.0953, 0.1124, 0.0973, 0.0941,\n",
       "                      0.1003, 0.0918, 0.0965, 0.1017, 0.0928, 0.0872, 0.0938, 0.1238, 0.0914,\n",
       "                      0.0797, 0.0977, 0.0833, 0.1042, 0.0951, 0.1138, 0.0990, 0.0963, 0.0976,\n",
       "                      0.0800, 0.1060, 0.0914, 0.0833])),\n",
       "             ('bottleneck.conv.1.num_batches_tracked', tensor(4002)),\n",
       "             ('bottleneck.conv.3.weight',\n",
       "              tensor([[[[ 0.0116, -0.0197, -0.0053],\n",
       "                        [ 0.0227, -0.0268, -0.0133],\n",
       "                        [-0.0121,  0.0059, -0.0052]],\n",
       "              \n",
       "                       [[ 0.0121, -0.0135,  0.0113],\n",
       "                        [-0.0012,  0.0024, -0.0123],\n",
       "                        [-0.0102, -0.0190,  0.0023]],\n",
       "              \n",
       "                       [[ 0.0007, -0.0318,  0.0014],\n",
       "                        [ 0.0190, -0.0121, -0.0078],\n",
       "                        [ 0.0047, -0.0013,  0.0093]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0065,  0.0155,  0.0083],\n",
       "                        [-0.0070,  0.0083,  0.0020],\n",
       "                        [ 0.0052,  0.0043,  0.0155]],\n",
       "              \n",
       "                       [[ 0.0127, -0.0004,  0.0121],\n",
       "                        [ 0.0219,  0.0144,  0.0060],\n",
       "                        [ 0.0103,  0.0166, -0.0250]],\n",
       "              \n",
       "                       [[ 0.0398, -0.0190, -0.0343],\n",
       "                        [-0.0037, -0.0016,  0.0077],\n",
       "                        [ 0.0118, -0.0011, -0.0051]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0031,  0.0059,  0.0250],\n",
       "                        [ 0.0004,  0.0029, -0.0103],\n",
       "                        [-0.0054,  0.0167,  0.0119]],\n",
       "              \n",
       "                       [[ 0.0153, -0.0049, -0.0104],\n",
       "                        [ 0.0210, -0.0184, -0.0108],\n",
       "                        [-0.0057, -0.0110, -0.0126]],\n",
       "              \n",
       "                       [[ 0.0073, -0.0318,  0.0372],\n",
       "                        [ 0.0077, -0.0099,  0.0017],\n",
       "                        [ 0.0045,  0.0002,  0.0260]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0009, -0.0369, -0.0260],\n",
       "                        [-0.0067,  0.0095, -0.0047],\n",
       "                        [ 0.0352,  0.0231, -0.0012]],\n",
       "              \n",
       "                       [[-0.0033,  0.0012,  0.0193],\n",
       "                        [ 0.0147,  0.0153,  0.0316],\n",
       "                        [-0.0128, -0.0098,  0.0100]],\n",
       "              \n",
       "                       [[-0.0029, -0.0099, -0.0088],\n",
       "                        [ 0.0172,  0.0075, -0.0095],\n",
       "                        [ 0.0062,  0.0150, -0.0073]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0226, -0.0283,  0.0036],\n",
       "                        [-0.0169, -0.0158,  0.0206],\n",
       "                        [ 0.0169, -0.0128,  0.0011]],\n",
       "              \n",
       "                       [[-0.0045,  0.0199,  0.0262],\n",
       "                        [ 0.0035, -0.0224,  0.0156],\n",
       "                        [ 0.0164,  0.0042, -0.0023]],\n",
       "              \n",
       "                       [[ 0.0114, -0.0094, -0.0136],\n",
       "                        [-0.0258,  0.0300, -0.0027],\n",
       "                        [ 0.0123, -0.0041, -0.0222]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0245, -0.0260,  0.0171],\n",
       "                        [-0.0037,  0.0095,  0.0019],\n",
       "                        [ 0.0194,  0.0009,  0.0159]],\n",
       "              \n",
       "                       [[ 0.0036,  0.0072,  0.0256],\n",
       "                        [-0.0190, -0.0012, -0.0204],\n",
       "                        [ 0.0176,  0.0109,  0.0088]],\n",
       "              \n",
       "                       [[-0.0025, -0.0397,  0.0198],\n",
       "                        [-0.0176,  0.0124, -0.0072],\n",
       "                        [-0.0152,  0.0230,  0.0106]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0023, -0.0146, -0.0110],\n",
       "                        [ 0.0139, -0.0247,  0.0079],\n",
       "                        [ 0.0199, -0.0040,  0.0021]],\n",
       "              \n",
       "                       [[-0.0132,  0.0078, -0.0007],\n",
       "                        [-0.0145,  0.0021, -0.0150],\n",
       "                        [ 0.0073, -0.0141, -0.0068]],\n",
       "              \n",
       "                       [[-0.0151, -0.0107,  0.0127],\n",
       "                        [ 0.0116, -0.0035, -0.0061],\n",
       "                        [-0.0063, -0.0099, -0.0070]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0209, -0.0127,  0.0027],\n",
       "                        [-0.0035,  0.0112,  0.0118],\n",
       "                        [-0.0127,  0.0275, -0.0017]],\n",
       "              \n",
       "                       [[-0.0153, -0.0111, -0.0204],\n",
       "                        [ 0.0267, -0.0133,  0.0084],\n",
       "                        [ 0.0179,  0.0116,  0.0068]],\n",
       "              \n",
       "                       [[-0.0122, -0.0277, -0.0056],\n",
       "                        [ 0.0009,  0.0002, -0.0315],\n",
       "                        [ 0.0188,  0.0037, -0.0216]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0122, -0.0230, -0.0025],\n",
       "                        [-0.0099,  0.0143, -0.0053],\n",
       "                        [-0.0134,  0.0019, -0.0216]],\n",
       "              \n",
       "                       [[-0.0163,  0.0201,  0.0141],\n",
       "                        [ 0.0050, -0.0015, -0.0020],\n",
       "                        [ 0.0057,  0.0026,  0.0286]],\n",
       "              \n",
       "                       [[-0.0111,  0.0038, -0.0249],\n",
       "                        [ 0.0141, -0.0145, -0.0127],\n",
       "                        [ 0.0149, -0.0052,  0.0053]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0050, -0.0102, -0.0040],\n",
       "                        [-0.0144,  0.0127,  0.0118],\n",
       "                        [ 0.0076,  0.0011, -0.0099]],\n",
       "              \n",
       "                       [[ 0.0032, -0.0054, -0.0100],\n",
       "                        [-0.0019, -0.0128, -0.0230],\n",
       "                        [-0.0234, -0.0003, -0.0293]],\n",
       "              \n",
       "                       [[-0.0228, -0.0101,  0.0088],\n",
       "                        [-0.0170,  0.0103, -0.0130],\n",
       "                        [ 0.0147,  0.0090,  0.0016]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0031, -0.0150, -0.0033],\n",
       "                        [-0.0323, -0.0031,  0.0171],\n",
       "                        [ 0.0044,  0.0189, -0.0077]],\n",
       "              \n",
       "                       [[ 0.0281, -0.0086,  0.0016],\n",
       "                        [ 0.0194, -0.0033, -0.0016],\n",
       "                        [-0.0012,  0.0111, -0.0276]],\n",
       "              \n",
       "                       [[ 0.0020,  0.0098,  0.0071],\n",
       "                        [ 0.0214, -0.0143,  0.0206],\n",
       "                        [-0.0258,  0.0007, -0.0213]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0307,  0.0018,  0.0238],\n",
       "                        [ 0.0210,  0.0049, -0.0079],\n",
       "                        [-0.0098, -0.0044,  0.0069]],\n",
       "              \n",
       "                       [[ 0.0067, -0.0196,  0.0234],\n",
       "                        [ 0.0020, -0.0168, -0.0160],\n",
       "                        [ 0.0008,  0.0062, -0.0011]],\n",
       "              \n",
       "                       [[ 0.0076,  0.0061, -0.0032],\n",
       "                        [ 0.0153, -0.0118, -0.0240],\n",
       "                        [ 0.0148, -0.0023, -0.0165]]]])),\n",
       "             ('bottleneck.conv.4.weight',\n",
       "              tensor([1.0189, 1.0245, 1.0304, 1.0150, 1.0236, 1.0268, 1.0239, 1.0356, 1.0264,\n",
       "                      1.0083, 1.0243, 1.0280, 1.0194, 1.0288, 1.0329, 1.0318, 1.0349, 1.0278,\n",
       "                      1.0098, 1.0283, 1.0272, 1.0268, 1.0192, 1.0276, 1.0217, 1.0409, 1.0264,\n",
       "                      1.0220, 1.0227, 1.0366, 1.0286, 1.0356, 1.0369, 1.0152, 1.0227, 1.0208,\n",
       "                      1.0108, 1.0097, 1.0332, 1.0168, 1.0154, 1.0352, 1.0257, 1.0284, 1.0260,\n",
       "                      1.0222, 1.0242, 1.0240, 1.0293, 1.0230, 1.0297, 1.0259, 1.0306, 1.0315,\n",
       "                      1.0233, 1.0233, 1.0303, 1.0137, 1.0213, 1.0225, 1.0227, 1.0376, 1.0219,\n",
       "                      1.0400, 1.0193, 1.0067, 1.0251, 1.0219, 1.0416, 1.0245, 1.0227, 1.0075,\n",
       "                      1.0194, 1.0108, 1.0121, 1.0228, 1.0193, 1.0167, 1.0224, 1.0122, 1.0122,\n",
       "                      1.0098, 1.0435, 1.0161, 1.0265, 1.0111, 1.0252, 1.0258, 1.0243, 1.0096,\n",
       "                      1.0381, 1.0142, 1.0208, 1.0206, 1.0293, 1.0258, 1.0134, 1.0212, 1.0324,\n",
       "                      1.0151, 1.0295, 1.0360, 1.0210, 1.0122, 1.0232, 1.0108, 1.0171, 1.0193,\n",
       "                      1.0217, 1.0289, 1.0328, 1.0313, 1.0125, 1.0329, 1.0246, 1.0423, 1.0268,\n",
       "                      1.0301, 1.0178, 1.0177, 1.0284, 1.0421, 1.0104, 1.0310, 1.0261, 1.0380,\n",
       "                      1.0298, 1.0156, 1.0231, 1.0281, 1.0349, 1.0310, 1.0204, 1.0349, 1.0357,\n",
       "                      1.0228, 1.0255, 1.0286, 1.0316, 1.0312, 1.0294, 1.0198, 1.0165, 1.0379,\n",
       "                      1.0091, 1.0236, 1.0277, 1.0348, 1.0213, 1.0333, 1.0170, 1.0180, 1.0427,\n",
       "                      1.0095, 1.0255, 1.0307, 1.0279, 1.0274, 1.0290, 1.0293, 1.0290, 1.0263,\n",
       "                      1.0120, 1.0309, 1.0332, 1.0249, 1.0134, 1.0210, 1.0187, 1.0153, 1.0224,\n",
       "                      1.0145, 1.0274, 1.0233, 1.0363, 1.0227, 1.0371, 1.0379, 1.0211, 1.0092,\n",
       "                      1.0179, 1.0309, 1.0290, 1.0359, 1.0316, 1.0281, 1.0232, 1.0356, 1.0199,\n",
       "                      1.0152, 1.0302, 1.0237, 1.0272, 1.0433, 1.0298, 1.0198, 1.0253, 1.0220,\n",
       "                      1.0294, 1.0323, 1.0196, 1.0361, 1.0272, 1.0369, 1.0219, 1.0178, 1.0132,\n",
       "                      1.0336, 1.0018, 0.9997, 1.0311, 1.0187, 1.0183, 1.0292, 1.0178, 1.0254,\n",
       "                      1.0332, 1.0284, 1.0191, 1.0086, 1.0339, 1.0208, 1.0160, 1.0403, 1.0189,\n",
       "                      1.0267, 1.0162, 1.0209, 1.0232, 1.0335, 1.0386, 1.0188, 1.0269, 1.0307,\n",
       "                      1.0209, 1.0251, 1.0295, 1.0355, 1.0220, 1.0250, 1.0321, 1.0191, 1.0147,\n",
       "                      1.0238, 1.0253, 1.0191, 1.0179, 1.0279, 1.0161, 1.0334, 1.0289, 1.0095,\n",
       "                      1.0203, 1.0163, 1.0295, 1.0290])),\n",
       "             ('bottleneck.conv.4.bias',\n",
       "              tensor([-9.2634e-03, -1.8998e-02, -4.5509e-03, -1.4882e-02, -1.2179e-02,\n",
       "                      -2.6923e-02, -1.0995e-02, -2.9928e-03, -4.2452e-03, -1.1906e-02,\n",
       "                      -9.2910e-03, -8.7184e-03, -2.8029e-02, -1.5125e-02, -3.4688e-03,\n",
       "                      -2.2588e-03, -6.1785e-03, -3.5526e-04, -9.3014e-03, -1.4980e-02,\n",
       "                      -1.1975e-02, -1.6634e-02, -1.6196e-02, -2.1210e-02, -2.9865e-02,\n",
       "                       1.5546e-02, -1.3638e-02, -1.1030e-02, -1.8461e-02,  6.1472e-03,\n",
       "                      -1.3957e-02, -5.5181e-03, -1.0403e-03, -1.5377e-02, -2.9990e-03,\n",
       "                      -1.1634e-02, -1.8682e-02, -1.9009e-02, -3.6924e-04, -1.2292e-02,\n",
       "                      -1.6831e-02, -1.5491e-02, -2.3651e-02, -5.0959e-03, -1.2551e-02,\n",
       "                      -1.9805e-02, -4.1500e-03, -7.2703e-05, -3.6171e-03, -1.3218e-02,\n",
       "                      -7.6438e-03, -7.5628e-03, -3.9152e-04,  2.4903e-03, -1.3631e-02,\n",
       "                      -1.7047e-02,  5.1552e-04, -1.2154e-02, -3.0855e-02, -2.7380e-02,\n",
       "                      -7.4362e-03, -2.3879e-03, -2.0512e-02, -4.8372e-03, -1.8078e-02,\n",
       "                      -2.6622e-02, -2.4348e-02, -9.9002e-03,  3.1314e-03, -1.2416e-02,\n",
       "                      -2.3604e-02, -4.2079e-03, -1.3655e-02, -1.8393e-02, -1.4068e-02,\n",
       "                      -1.2167e-02, -2.8196e-02, -1.6773e-02, -8.4647e-03, -5.4284e-03,\n",
       "                      -1.8495e-02, -1.8745e-02,  6.9980e-03, -1.3750e-02, -1.4386e-02,\n",
       "                      -1.6285e-02, -6.9818e-04, -1.9161e-02, -1.0858e-02, -2.9483e-02,\n",
       "                      -9.6987e-04, -1.5639e-02, -1.9438e-02, -1.1975e-02,  4.8229e-03,\n",
       "                      -1.0418e-02, -4.5373e-03, -1.6559e-02, -1.2488e-02, -9.6061e-03,\n",
       "                      -9.1001e-03,  3.1706e-03, -1.5834e-02, -1.1975e-02, -2.2998e-02,\n",
       "                      -2.4543e-02, -8.7046e-03, -5.5851e-03, -1.0143e-02, -8.8123e-03,\n",
       "                      -1.0965e-02,  3.3015e-03, -5.7674e-03, -1.2732e-02, -8.4661e-03,\n",
       "                       7.7461e-03,  4.9442e-04, -2.0516e-03, -9.9020e-03, -3.5928e-03,\n",
       "                      -1.3119e-02,  4.6367e-03, -2.8545e-02, -1.5720e-02, -1.9429e-02,\n",
       "                       1.3023e-03, -1.5555e-02, -1.6791e-02, -2.2231e-03, -2.4142e-03,\n",
       "                      -5.8043e-03, -1.2398e-02, -4.3177e-03, -6.9032e-03,  3.3426e-03,\n",
       "                      -7.2255e-03, -4.0462e-03, -1.7994e-02, -5.9275e-03, -1.4552e-02,\n",
       "                      -1.5852e-02, -8.7443e-03, -8.2044e-03, -8.5392e-03, -2.2638e-02,\n",
       "                      -9.3640e-03, -7.4800e-03, -1.4734e-02, -1.1011e-02, -6.5415e-03,\n",
       "                      -5.2658e-04, -2.5592e-02, -9.6976e-03, -1.0423e-02, -2.1468e-02,\n",
       "                       1.7975e-04, -3.2666e-03, -3.0810e-02, -8.3110e-03, -1.1423e-02,\n",
       "                      -5.1660e-03, -3.7606e-03, -1.7449e-02,  1.6990e-03, -2.7688e-02,\n",
       "                      -9.2120e-04, -1.2795e-02, -1.8365e-02, -7.6134e-03, -1.5506e-02,\n",
       "                      -3.9319e-03, -3.3455e-02, -8.9094e-03, -1.3582e-02, -1.8651e-02,\n",
       "                      -1.0863e-02, -3.3053e-03, -1.6217e-02, -1.2276e-02, -1.2099e-02,\n",
       "                      -8.7539e-03, -6.0986e-03, -1.8652e-03, -1.3216e-02, -1.5486e-02,\n",
       "                      -7.7243e-03, -1.7511e-02, -5.3186e-03, -7.6814e-03, -1.4288e-02,\n",
       "                      -6.5550e-03, -4.7799e-04, -5.7703e-03,  6.7433e-04, -1.6408e-02,\n",
       "                      -1.6399e-02,  2.4744e-03, -1.6077e-02, -6.9693e-03,  4.9729e-03,\n",
       "                      -1.8025e-02, -5.7917e-03, -9.5267e-03, -3.2500e-03, -1.9145e-02,\n",
       "                      -1.0605e-02, -1.6197e-02, -5.9124e-03, -2.1819e-02, -1.0964e-02,\n",
       "                      -6.5885e-03, -1.5128e-02, -8.6076e-03, -1.3923e-02, -2.2198e-02,\n",
       "                      -1.2346e-02, -3.2543e-03, -1.3564e-02, -1.8789e-02, -9.9169e-03,\n",
       "                      -1.4497e-02, -9.0762e-04, -3.3300e-02, -6.8416e-03,  2.6202e-03,\n",
       "                      -2.3318e-02, -9.5979e-03, -1.4233e-02, -1.1616e-02, -1.2635e-02,\n",
       "                      -4.0146e-03, -7.5639e-03, -1.3253e-02, -9.5103e-03, -1.1497e-02,\n",
       "                      -3.8718e-03, -2.2725e-03,  7.3744e-03, -1.9553e-02, -1.2763e-02,\n",
       "                      -1.3788e-02, -2.8759e-02, -1.3740e-02, -1.6428e-02, -1.3582e-02,\n",
       "                      -1.2596e-02, -9.7396e-03, -9.4723e-03, -2.4904e-02, -2.1636e-02,\n",
       "                      -1.4099e-02, -6.9699e-03, -1.1562e-02, -2.6441e-02, -5.4668e-03,\n",
       "                      -5.0579e-03])),\n",
       "             ('bottleneck.conv.4.running_mean',\n",
       "              tensor([ 0.0376,  0.1166, -0.2211,  0.0866,  0.0946,  0.2999,  0.1307,  0.1484,\n",
       "                       0.0339, -0.0602, -0.0606,  0.0025,  0.1708, -0.2346,  0.1132,  0.3140,\n",
       "                       0.0013,  0.1665,  0.0778,  0.0255, -0.1042,  0.2454,  0.0737,  0.2110,\n",
       "                       0.1410, -0.0099,  0.3197, -0.2413,  0.2800, -0.0801,  0.2406, -0.1335,\n",
       "                      -0.3631, -0.1745,  0.0109,  0.2405,  0.1416,  0.1654, -0.0425, -0.0045,\n",
       "                      -0.1298,  0.1108, -0.0839, -0.0513,  0.3188, -0.2528,  0.1410, -0.1711,\n",
       "                       0.2225,  0.0511,  0.3190, -0.0892, -0.3977,  0.0385, -0.1113, -0.4144,\n",
       "                       0.0769, -0.0043,  0.0510, -0.0238, -0.2159, -0.1841,  0.0680,  0.0842,\n",
       "                       0.1191,  0.1501, -0.1847,  0.0347,  0.0583, -0.0905,  0.0898,  0.2484,\n",
       "                      -0.0871, -0.0533, -0.1245,  0.0419, -0.0274, -0.1646,  0.4160, -0.0522,\n",
       "                       0.1629, -0.1899, -0.0852, -0.2006, -0.1812,  0.0068,  0.3845, -0.1397,\n",
       "                       0.1876, -0.0297, -0.1244, -0.2568,  0.0078,  0.1562, -0.1747, -0.2005,\n",
       "                       0.1710,  0.1029,  0.2142,  0.1510, -0.0397,  0.0782,  0.1801,  0.1174,\n",
       "                      -0.1600, -0.0654, -0.3047, -0.1280, -0.0966, -0.0616,  0.0281,  0.0330,\n",
       "                       0.1007, -0.0078,  0.1315,  0.4188,  0.1436,  0.0998, -0.1010,  0.0339,\n",
       "                       0.0008,  0.1596,  0.3106,  0.1920,  0.2928, -0.1726, -0.0580,  0.1542,\n",
       "                       0.2926, -0.1706, -0.0637, -0.0050, -0.0527,  0.2311,  0.0298, -0.0260,\n",
       "                      -0.0035,  0.1287,  0.2361,  0.2058,  0.0036, -0.0630, -0.1255, -0.0550,\n",
       "                       0.2701,  0.0400, -0.1218, -0.0836, -0.1520, -0.0670, -0.1148,  0.2199,\n",
       "                      -0.1854,  0.0340, -0.3113,  0.2114, -0.3731, -0.1952, -0.1087,  0.1562,\n",
       "                       0.2908,  0.0092, -0.0405,  0.0836, -0.2344,  0.1700,  0.0722,  0.1154,\n",
       "                      -0.1343,  0.1335,  0.0982,  0.3509,  0.2009, -0.1717, -0.0664, -0.1078,\n",
       "                      -0.0048,  0.0212,  0.1956,  0.0210, -0.0888,  0.1551,  0.0542, -0.0147,\n",
       "                       0.0028, -0.0950,  0.2050, -0.1198,  0.1698, -0.1004, -0.1792,  0.2829,\n",
       "                       0.0233, -0.1376,  0.2080, -0.0694,  0.0656,  0.2951, -0.1952,  0.3415,\n",
       "                       0.6338,  0.0317, -0.0612, -0.2860, -0.1074,  0.2499,  0.1108,  0.0202,\n",
       "                       0.2389, -0.2571,  0.2368, -0.1552, -0.0621, -0.0179,  0.2707,  0.1571,\n",
       "                       0.0714,  0.3311,  0.1391, -0.1369, -0.0451, -0.1700,  0.3067,  0.0432,\n",
       "                       0.1712, -0.2341, -0.0153,  0.0152,  0.1074,  0.1627,  0.2488,  0.0473,\n",
       "                       0.0099, -0.1191, -0.2451,  0.3294,  0.0740, -0.0201, -0.2817, -0.2506,\n",
       "                       0.0536,  0.3628, -0.1685, -0.2068, -0.0520,  0.2828,  0.0898,  0.0229,\n",
       "                      -0.2032, -0.0332,  0.1638, -0.0316, -0.1217,  0.1280,  0.1319, -0.0741])),\n",
       "             ('bottleneck.conv.4.running_var',\n",
       "              tensor([0.1157, 0.0938, 0.0936, 0.1121, 0.1196, 0.1036, 0.1174, 0.1062, 0.1138,\n",
       "                      0.1411, 0.0940, 0.1057, 0.0985, 0.1002, 0.1020, 0.1132, 0.1113, 0.1038,\n",
       "                      0.1110, 0.1228, 0.1035, 0.1030, 0.1098, 0.1016, 0.1021, 0.1051, 0.1114,\n",
       "                      0.1063, 0.0897, 0.1144, 0.1092, 0.1014, 0.1089, 0.1114, 0.1049, 0.0918,\n",
       "                      0.0953, 0.1095, 0.1144, 0.1072, 0.1006, 0.1131, 0.0970, 0.0973, 0.1128,\n",
       "                      0.1039, 0.1055, 0.1189, 0.1005, 0.1292, 0.0954, 0.1122, 0.1223, 0.1185,\n",
       "                      0.1122, 0.1341, 0.1083, 0.1133, 0.1127, 0.1091, 0.1139, 0.1004, 0.0983,\n",
       "                      0.1080, 0.1124, 0.1084, 0.1075, 0.1125, 0.1023, 0.1101, 0.1083, 0.1218,\n",
       "                      0.1197, 0.1475, 0.1070, 0.1183, 0.1139, 0.1086, 0.1109, 0.1156, 0.1210,\n",
       "                      0.1138, 0.1054, 0.1068, 0.1077, 0.1144, 0.1165, 0.1035, 0.1105, 0.1098,\n",
       "                      0.1086, 0.0991, 0.1062, 0.1235, 0.1087, 0.0959, 0.1023, 0.0863, 0.1225,\n",
       "                      0.1227, 0.1105, 0.1185, 0.1006, 0.1118, 0.1152, 0.1012, 0.1042, 0.1068,\n",
       "                      0.1122, 0.1184, 0.1167, 0.1093, 0.1114, 0.1088, 0.0966, 0.1033, 0.1354,\n",
       "                      0.1129, 0.0980, 0.1232, 0.1026, 0.1015, 0.1190, 0.1036, 0.1018, 0.1043,\n",
       "                      0.1185, 0.0952, 0.1186, 0.1104, 0.1029, 0.1192, 0.1076, 0.1004, 0.1108,\n",
       "                      0.1258, 0.1161, 0.1135, 0.1184, 0.1120, 0.1083, 0.1208, 0.1092, 0.1100,\n",
       "                      0.1115, 0.1070, 0.1182, 0.1110, 0.1047, 0.1036, 0.1182, 0.0964, 0.1047,\n",
       "                      0.1449, 0.1320, 0.0975, 0.1081, 0.1202, 0.1063, 0.1054, 0.1087, 0.1010,\n",
       "                      0.1176, 0.1075, 0.1026, 0.1058, 0.1046, 0.1119, 0.1033, 0.1052, 0.1126,\n",
       "                      0.1195, 0.0989, 0.1043, 0.1013, 0.1132, 0.0872, 0.1103, 0.1080, 0.1201,\n",
       "                      0.1065, 0.0964, 0.1120, 0.1007, 0.1078, 0.1166, 0.1135, 0.1147, 0.1083,\n",
       "                      0.1236, 0.1014, 0.1022, 0.1073, 0.1261, 0.1012, 0.1094, 0.1116, 0.1122,\n",
       "                      0.1039, 0.0987, 0.1196, 0.1144, 0.0994, 0.1051, 0.1062, 0.1113, 0.1066,\n",
       "                      0.1083, 0.1423, 0.1568, 0.1258, 0.1034, 0.1073, 0.1031, 0.1127, 0.1071,\n",
       "                      0.1116, 0.0997, 0.1016, 0.1283, 0.1069, 0.1079, 0.1064, 0.1115, 0.1362,\n",
       "                      0.0946, 0.1166, 0.1088, 0.1176, 0.1094, 0.1072, 0.1037, 0.0998, 0.1081,\n",
       "                      0.1116, 0.0974, 0.1038, 0.1026, 0.1189, 0.1050, 0.1104, 0.1101, 0.1136,\n",
       "                      0.1091, 0.1157, 0.1017, 0.1156, 0.1109, 0.1331, 0.1088, 0.1190, 0.1204,\n",
       "                      0.1170, 0.1191, 0.0976, 0.1003])),\n",
       "             ('bottleneck.conv.4.num_batches_tracked', tensor(4002)),\n",
       "             ('final_conv.weight',\n",
       "              tensor([[[[ 0.2680]],\n",
       "              \n",
       "                       [[-0.3399]],\n",
       "              \n",
       "                       [[ 0.2580]],\n",
       "              \n",
       "                       [[-0.3909]],\n",
       "              \n",
       "                       [[-0.3117]],\n",
       "              \n",
       "                       [[-0.3312]],\n",
       "              \n",
       "                       [[ 0.2997]],\n",
       "              \n",
       "                       [[ 0.4031]],\n",
       "              \n",
       "                       [[-0.4717]],\n",
       "              \n",
       "                       [[ 0.3959]],\n",
       "              \n",
       "                       [[-0.4162]],\n",
       "              \n",
       "                       [[-0.3254]],\n",
       "              \n",
       "                       [[ 0.4506]],\n",
       "              \n",
       "                       [[ 0.3283]],\n",
       "              \n",
       "                       [[ 0.4092]],\n",
       "              \n",
       "                       [[ 0.3424]]]])),\n",
       "             ('final_conv.bias', tensor([-0.2573]))])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80feb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e68db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ee19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = torch.load(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f50a804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_load = model_load(test_ex[0])\n",
    "tr_load = model_load(train_ex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1c196fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            preds = model(x)\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct +=  (preds ==y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum())\n",
    "    \n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "    )\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2120695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 12474689/12800000 with acc 97.46\n",
      "Dice score: 0.9741436839103699\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf11e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3093269/3200000 with acc 96.66\n",
      "Dice score: 0.9660056233406067\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cda14b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6318b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
